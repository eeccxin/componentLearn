MySQL 45讲

# 0 课程介绍

> 极客时间  MySQL 实战 45 讲 https://time.geekbang.org/column/intro/100020801

## 你将获得

- 前腾讯云数据库负责人独家经验分享；
- MySQL 核心技术详解与原理说明；
- 36 个 MySQL 常见痛点问题解析；
- 完整的 MySQL 学习路径。

## 课程介绍

MySQL 使用和面试中遇到的问题，很多人会通过搜索别人的经验来解决 ，零散不成体系。实际上只要理解了 MySQL 的底层工作原理，就能很快地直戳问题的本质。

本专栏一共 **45** 讲，**全集**更新完毕，通过探讨 MySQL 实战中最常见的 **36 个** 痛点问题，串起各个零散的知识点，配合 **100+** 手绘详解图，由线到面带你构建 MySQL 系统的学习路径。

在专栏中，丁奇结合自己十多年来，通过 MySQL 源码找答案的经历，梳理了 **MySQL 的主线知识**，比如**事务、索引、锁**等；并基于这条主线，带你缕清**概念、机制、原理、案例分析**以及本质，让你真正能掌握 **MySQL 核心技术与底层原理。**

值得一提的是，每篇文章中都附有**实践案例**，给你从理论到实战的系统性指导，让你少走弯路，彻底搞懂 MySQL。

本课程共包括两大模块。

**模块一，基础篇**。为你深入浅出地讲述 MySQL 核心知识，涵盖 MySQL 基础架构、日志系统、事务隔离、锁等内容。

**模块二，实践篇**。将从一个个关键的数据库问题出发，分析数据库原理，并给出实践指导。每个问题，都不只是简单地给出答案，而是从为什么要这么想、到底该怎样做出发，让你能够知其所以然，都将能够解决你平时工作中的一个疑惑点。

## 课程目录

<img src="MYSQL实战45讲.assets/b736f37014d28199c2457a67ed669bc2.jpg" alt="img" style="zoom:33%;" />





# 00 安装mysql

> 详见：https://www.runoob.com/mysql/mysql-install.html

```bash
# 下载
wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm
rpm -ivh mysql-community-release-el7-5.noarch.rpm
yum update
yum install mysql-server
```

权限设置：

```
chown -R mysql:mysql /var/lib/mysql/
```

初始化 MySQL：

```
mysqld --initialize
```

启动 MySQL：

```
systemctl start mysqld
```

查看 MySQL 运行状态：

```
systemctl status mysqld
```



查看初始密码：

```
grep password /var/log/mysql/mysqld.log 
2024-05-14T11:48:54.670531Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: 3d:Aw9u?-Tp!
```



登录后修改密码：

```bash
mysql -u root -p 3d:Aw9u?-Tp!
#修改密码
alter user 'root'@'localhost' identified by 'xxxxxx密码'
```



# 01 | 基础架构：一条SQL查询语句是如何执行的？

> https://juejin.cn/post/7130258357184626724 这篇文章更详细

下面是MySQL的基本架构示意图，从中你可以清楚地看到SQL语句在MySQL的各个功能模块中的执行过程。

<img src="MYSQL实战45讲.assets/image-20240428175938023.png" alt="image-20240428175938023" style="zoom:50%;" />

<center>MySQL的逻辑架构图</center>

​		大体来说，MySQL可以分为Server层和存储引擎层两部分。

-  Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。
- 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从**MySQL 5.5.5**版本开始成为了默认存储引擎。

 以1条SQL语句，带你走一遍整个执行流程，依次看下每个组件的作用。       

```sql
mysql> select * from T where ID=10；
```

![未命名绘图.png](MYSQL实战45讲.assets/73b4a36d35c3439694d0a29c20b880d9tplv-k3u1fbpfcp-zoom-in-crop-mark1512000-17143044539905.webp)

## 连接器

第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：

```sql
mysql -h $ip -P $port -u $user -p
```

连接命令中的mysql是客户端工具，用来跟服务端建立连接。在完成经典的TCP握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。

- 如果用户名或密码不对，你就会收到一个"Access denied for user"的错误，然后客户端程序结束执行。
- 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限

这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。**修改完成后，只有再新建的连接才会使用新的权限设置**。  

连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在**showprocesslist命令**中看到它。文本中这个图是showprocesslist的结果，其中的Command列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。

​                                                       <img src="MYSQL实战45讲.assets/image-20240428193357383.png" alt="image-20240428193357383" style="zoom:50%;" />

> 空闲连接会一直占用着吗？

客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数**wait_timeout（等待超时）**控制的，默认值是8小时。

```sql
mysql> show variables like 'wait_timeout';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| wait_timeout  | 28800 |
+---------------+-------+
1 row in set (0.00 sec)
```

当然，我们自己也可以手动断开空闲的连接，使用的是 kill connection + id 的命令。

```sql
mysql> kill connection +6;
Query OK, 0 rows affected (0.00 sec)
```

如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connectionto MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。

数据库里面，**长连接**是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。**短连接**则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。

建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是**尽量使用长连接**。但是全部使用长连接后，你可能会发现，有些时候MySQL占用**内存涨得特别快**，这是因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（**OOM**），从现象看就是MySQL异常重启了。

> 怎么解决长连接长期占用内存这个问题呢？你可以考虑以下两种方案。

1. 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。

2. **客户端主动重置连接**。MySQL 5.7 版本实现了 `mysql_reset_connection()` 函数的接口，注意这是接口函数不是命令，那么当客户端执行了一个很大的操作后，在代码里调用 mysql_reset_connection 函数来重置连接，达到释放内存的效果。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

   

> MySQL 的连接数有限制吗？

MySQL 服务支持的最大连接数由 **max_connections** 参数控制，比如我的 MySQL 服务默认是 151 个,超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。

```sql
mysql> show variables like 'max_connections';
+-----------------+-------+
| Variable_name   | Value |
+-----------------+-------+
| max_connections | 151   |
+-----------------+-------+
1 row in set (0.00 sec)
```



## 查询缓存

连接建立完成后，你就可以执行select语句了。执行逻辑就会来到第二步：查询缓存。

MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。

如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。

> 但是大多数情况下我会建议你不要使用查询缓存，为什么呢？

因为查询缓存往往弊大于利。**查询缓存的失效非常频繁**，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。

好在MySQL也提供了这种“按需使用”的方式。你可以将参数**query_cache_type设置成DEMAND**，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样：

```sql
mysql> select SQL_CACHE * from T where ID=10;
```

需要注意的是，MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有这个功能了。



## 分析器

如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL需要知道你要做什么，因此需要对SQL语句做解析。

1、词法分析

分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。

MySQL从你输入的"select"这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名T”，把字符串“ID”识别成“列ID”。



2、语法分析

做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。

如果你的语句不对，就会收到**“You have an error in your SQL syntax”的错误提醒**，比如下面这个语句select少打了开头的字母“s”。

```sql
mysql> elect * from t where ID=1;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'elect * from t where ID=1' at line 1
```

一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。



## 优化器 

经过了分析器，MySQL就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。

**优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序**。

比如你执行下面这样的语句，这个语句是执行两个表的join：

```sql
mysql> select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20;
```

- 既可以先从表t1里面取出c=10的记录的ID值，再根据ID值关联到表t2，再判断t2里面d的值是否等于20。
- 也可以先从表t2里面取出d=20的记录的ID值，再根据ID值关联到t1，再判断t1里面c的值是否等于10。

这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。

优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。如果你还有一些疑问，比如优化器是怎么选择索引的，有没有可能选择错等等，没关系，在后面的文章中单独展开说明优化器的内容。

（小表驱动大表会好点？）



## 执行器

MySQL通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。

开始执行的时候，要先判断一下你对这个表T有没有执行查询的**权限**，如果没有，就会返回没有权限的错误，如下所示：

```sql
mysql> select * from T where ID=10;
ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T'
```

如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。

比如我们这个例子中的表T中，ID字段没有索引，那么执行器的执行流程是这样的（全表查询）：

1. 调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则

将这行存在结果集中；

2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。

3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

至此，这个语句就执行完成了，

对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。



你会在数据库的慢查询日志中看到一个**rows_examined的字段，表示这个语句执行过程中扫描了多少行**。这个值就是在执行器每次调用引擎获取数据行的时候累加的。

在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟rows_examined 并不是完全相同的。 后面会专门有一篇文章来讲存储引擎的内部机制，里面会有详细的说明。



## 思考题

如果表T中没有字段k，而你执行了这个语句 select *fromTwhere k=1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢？

（分析器？

查询解析阶段：在这个阶段，MySQL 解析查询语句的语法和语义，检查语句的正确性，并生成相应的查询执行计划。在这个阶段，MySQL 会检查表和列是否存在，但不会检查列的值）



# 02 | 日志系统：一条SQL更新语句是如何执行的？

一条更新语句的执行流程又是怎样的呢？

**之前你可能经常听DBA同事说，MySQL可以恢复到半个月内任意一秒的状态**，惊叹的同时，你是不是心中也会不免会好奇，这是怎样做到的呢？

我们还是从一个表的一条更新语句说起，下面是这个表的创建语句，这个表有一个主键ID和一个整型字段c：

```sql
mysql> create table T(ID int primary key, c int);
```

如果要将ID=2这一行的值加1，SQL语句就会这么写：

```sql
mysql> update T set c=c+1 where ID=2;
```

<img src="MYSQL实战45讲.assets/image-20240429180703572.png" alt="image-20240429180703572" style="zoom:50%;" />

还是看这个架构图。

你执行语句前要先连接数据库，这是连接器的工作。

前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表T上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。

接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用ID这个索引。然后，执行器负责具体执行，找到这一行，然后更新。

**与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）**。



## **redo log**（重做日志）

在MySQL里，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。

为了解决这个问题，MySQL的设计者就用了类似酒店掌柜粉板（临时记录，最后再一起核算）的思路来提升更新效率。

而粉板和账本配合的整个过程，其实就是MySQL里经常说到的WAL技术，WAL的全称是**Write Ahead Logging（写前日志）**，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。



具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里面，**并更新内存**，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。



InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么这块“粉板”总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。

<img src="MYSQL实战45讲.assets/image-20240429190635968.png" alt="image-20240429190635968" style="zoom:50%;" />

**write pos**是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。**checkpoint**是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。

write pos和checkpoint之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。（双指针设计得很合理）

有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为**crash-safe**（奔溃安全）。

**innodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。**这个参数我建议你设置成1，这样可以保证MySQL异常重启之后数据不丢失。



这个Redo Log 是在磁盘上存储的，而不是在内存中。

Redo Log 记录了对数据库进行的修改操作，例如插入、更新和删除操作。当事务提交时，相关的 Redo Log 记录会被写入磁盘，以确保在数据库崩溃或故障恢复时，可以通过重放 Redo Log 来恢复未完成的事务。

尽管 Redo Log 是在磁盘上存储的，但在实际操作中，MySQL 会**使用缓冲区（Buffer）来管理 Redo Log 的写入**。这是为了提高性能，避免每次修改都直接写入磁盘。MySQL 会将 Redo Log 记录先写入到内存的缓冲区中，然后根据一定的策略将缓冲区中的内容刷新到磁盘上的 Redo Log 文件中。





## **binlog（归档日志）**

MySQL整体来看，其实就有两块：一块是Server层，它主要做的是MySQL功能

层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板redo log是InnoDB引擎特有的日志，而**Server层也有自己的日志，称为binlog（归档日志）**

你肯定会问，为什么会有两份日志呢？

因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统— — 也就是redo log来实现crash-safe能力。

**sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。**这个参数我也建议你设置成1，这样可以保证MySQL异常重启之后binlog不丢失



这两种日志有以下三点不同。

1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。

2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。

3. redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。



有了对这两个日志的概念性理解，我们再来看执行器和InnoDB引擎在执行这个简单的update语句时的内部流程：

1. 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。

2. 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。

3. 引擎将这行新数据更新到内存中，同时将这个更新操作**记录到redo log里面**，此时redo log处于prepare状态。然后**告知执行器执行完成了**，随时可以提交事务。

4. 执行器生成这个操作的binlog，并把binlog写入磁盘.(异步的？)

5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。

下面是update语句的执行流程图，图中浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的。

<img src="MYSQL实战45讲.assets/image-20240429192224620.png" alt="image-20240429192224620" style="zoom:50%;" />

<center>update语句执行流程</center>

你可能注意到了，最后三步看上去有点“绕”，将redo log的写入拆成了两个步骤：prepare和commit，这就是"两阶段提交"。



## 两阶段提交

为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得

从文章开头的那个问题说起：怎么让数据库恢复到半个月内任意一秒的状态？

前面我们说过了，**binlog会记录所有的逻辑操作**，并且是采用“追加写”的形式。如**果你的DBA承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有binlog，同时系统会定期做整库备份。**这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备

当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数

据，那你可以这么做：

- 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；
- 然后，从备份的时间点开始，将备份的binlog依次取出来，重放到中午误删表之前的那个时刻。

这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。

好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。

由于redo log和binlog是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redo log再写binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。仍然用前面的update语句来做例子。假设当前ID=2的行，字段c的值是0，再假设执行update语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？

（数据以redo log为准，这个WAL写了就表示操作成功了）

**1、先写redo log在写binlog 会造成恢复数据时的数据缺失。**假设在redo log写完，binlog还没有写完的时候，MySQL进程异常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1。但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份日志的时候，存起来的binlog里面就没有这条语句。然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同

2、先写binlog在写redo log 。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是1，与原库的值不同



可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。

你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀？

其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要**扩容的时候**，也就是需要再多搭建一些备库来增加系统的读能力的时候，**现在常见的做法也是用全量备份加上应用binlog来实现的**，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。

简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。



**两阶段提交是==跨系统==维持数据逻辑一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到**。



## 思考题

定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？

> chatGPT回答

一天一备的优势：

1. 数据恢复点更近：一天一备意味着在发生故障或数据丢失时，可以恢复到最近一天的备份。**这可以减少数据丢失的程度，尤其是对于频繁发生数据变更的系统**。
2. 数据恢复时间较短：由于备份频率较高，恢复数据所需的时间相对较短。这对于需要快速恢复数据库的系统非常重要，以减少业务中断时间。

一周一备的优势：

1. 节省存储空间：一周一备相对于一天一备可以减少备份数据的存储需求。如果数据量庞大，每天进行全量备份可能会占用大量的存储空间。一周一备可以在一定程度上减少存储成本。
2. 减少备份过程对系统性能的影响：进行全量备份可能会对数据库系统的性能产生一定的影响。一周一备相对于一天一备可以减少备份过程对系统性能的影响，尤其是对于大型数据库系统。

备份策略的选择会影响数据库系统的可恢复性和可用性指标。一天一备可以提供更近的数据恢复点和较短的恢复时间，从而减少数据丢失和业务中断时间。而一周一备可以节省存储空间和减少备份过程对系统性能的影响。

需要根据具体的业务需求、数据变更频率和系统重要性来评估并选择适合的备份策略。有些系统可能需要更频繁的备份以确保数据的安全性和可恢复性，而对于其他系统，一周一备可能已经足够满足需求。

> 作者回答

好处是“最长恢复时间”更短。

在一天一备的模式里，最坏情况下需要应用一天的binlog。比如，你每天0点做一次全量备份，

而要恢复出一个到昨天晚上23点的备份。

一周一备最坏情况就要应用一周的binlog了。

系统的对应指标就是 RTO（**Recovery Time Objective**，恢复时间目标），具体见[这里](https://www.dell.com/community/zh/conversations/%E6%95%B0%E6%8D%AE%E4%BF%9D%E6%8A%A4%E8%AE%A8%E8%AE%BA%E5%8C%BA/%E6%81%A2%E5%A4%8D%E6%97%B6%E9%97%B4%E7%9B%AE%E6%A0%87rto%E5%92%8C%E6%81%A2%E5%A4%8D%E7%82%B9%E7%9B%AE%E6%A0%87rpo/647f86edf4ccf8a8de60d31e)。

当然这个是有成本的，因为更频繁全量备份需要消耗更多存储空间，所以这个RTO是成本换来

的，就需要你根据业务重要性来评估了





# 03 | 事务隔离：为什么你改了我还看不见？

事务就是要保证一组数据库操作，要么全部成功，要么全部失败（原子性）。在MySQL中，**事务支持是在引擎层实现的**。

道，MySQL是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如M**ySQL原生的MyISAM引擎就不支持事务，这也是MyISAM被InnoDB取代**

**的重要原因之一**。

今天的文章以InnoDB为例，剖析MySQL在事务支持方面的特定实现，并基于原理给出相应的实践建议，希望这些案例能加深你对MySQL事务原理的理解



## 隔离性与隔离级别

提到事务，你肯定会想到ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中I，也就是“隔离性”。

当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、**幻读（phantomread）**的问题，为了解决这些问题，就有了“隔离级别”的概念。



隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。SQL标准的事务隔离级别包括：读未提交（read uncommitted）、

读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。下面逐一为你解释：

- 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。
- 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。
- 可重复读是指，**一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的**。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
- 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。



其中“读提交”和“可重复读”比较难理解，下面一个例子说明这几种隔离级别。假设数据表T中只有一列，其中一行的值为1，下面是按照时间顺序执行两个事务的行为。

```bash
mysql> create table T(c int) engine=InnoDB;
insert into T(c) values(1);
```

<img src="MYSQL实战45讲.assets/image-20240511174838564.png" alt="image-20240511174838564" style="zoom:50%;" />



在不同的隔离级别下，事务A会有哪些不同的返回结果，也就是图里面V1、V2、V3的返回值分别是什么。

- 若隔离级别是“读未提交”， 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被A看到了。因此，V2、V3也都是2。
- 若隔离级别是“读提交”，则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以，V3的值也是2。
- 若隔离级别是“可重复读”，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。
- **若隔离级别是“串行化”，则在事务B执行“将1改成2”的时候，会被锁住**。直到事务A提交后，事务B才可以继续执行。所以从A的角度看， V1、V2值是1，V3的值是2。

**在实现上，数据库里面会创建一个视图**，访问的时候以视图的逻辑结果为准。

- 在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。
- 在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的
- “读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；
- 而“串行化”隔离级别下直接用加锁的方式来避免并行访问

在不同的隔离级别下，数据库行为是有所不同的。Oracle数据库的默认隔离级别其实就是“读提交”，因此对于一些从Oracle迁移到MySQL的应用，为保证数据库隔离级别的一致，你一定要记得将MySQL的隔离级别设置为“读提交”。

配置的方式是，将启动参数transaction-isolation的值设置成READ-COMMITTED。你可以用showvariables来查看当前的值

```bash
mysql> show variables like 'transaction_isolation';
+-----------------------+----------------+
| Variable_name | Value |
+-----------------------+----------------+
| transaction_isolation | READ-COMMITTED |
+-----------------------+----------------+
```



总结来说，存在即合理，哪个隔离级别都有它自己的使用场景，你要根据自己的业务情况来定。

什么情况下会用到“可重复读“的场景？

假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。这时候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。



## 事务隔离的实现

理解了事务的隔离级别，我们再来看看事务隔离具体是怎么实现的。这里我们展开说明“可重复读”。

在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

假设一个值从1被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录。

<img src="MYSQL实战45讲.assets/image-20240511175531780.png" alt="image-20240511175531780" style="zoom:50%;" />

当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图中看到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于read-viewA，要得到1，就必须将当前值依次执行图中所有的回滚操作得到。



同时你会发现，即使现在有另外一个事务正在将4改成5，这个事务跟read-viewA、B、C对应的事务是不会冲突的。

你一定会问，回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，**系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除**。

什么时候才不需要了呢？**就是当系统里没有比这个回滚日志更早的read-view的时候**。



基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。

1、**长事务意味着系统里面会存在很老的事务视图**。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。

**在MySQL 5.5及以前的版本，回滚日志是跟数据字典一起放在ibdata文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小**。我见过数据只有20GB，而回滚段有200GB的库。最终只好为了清理回滚段，重建整个库。

2、除了对回滚段的影响，**长事务还占用锁资源**，也可能拖垮整个库，这个我们会在后面讲锁的时候展开。



## 事务的启动方式

如前面所述，长事务有这些潜在风险，我当然是建议你尽量避免。其实很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。MySQL的事务启动方式有以下几种：

1. **显式启动事务语句**， begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback。

2. **set autocommit=0**，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行commit 或 rollback 语句，或者断开连接。

有些客户端连接框架会默认连接成功后先执行一个set autocommit=0的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。

因此，建议你总是使用set autocommit=1, 并通过显式语句的方式来启动事务。

PS: 客户端请求时总会新建一个事务， autocommit只是设置了是否自动提交事务。且当begin显式开启事务时，只有commit才会提交事务（无关autocommit设置）



但是有的开发同学会**纠结“多一次交互”的问题**。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾虑，**建议你使用commit work and chain语法**。

在autocommit为1的情况下，用begin显式启动的事务，如果执行commit则提交事务。**如果执行commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语句的开销**。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。



你可以在information_schema库的innodb_trx这个表中查询长事务，比如下面这个语句，用于查找持续时间超过60s的事务。

```mysql
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60;
```

PS：事务表表结构

```mysql
mysql> describe information_schema.innodb_trx;
+----------------------------+---------------------+------+-----+---------------------+-------+
| Field                      | Type                | Null | Key | Default             | Extra |
+----------------------------+---------------------+------+-----+---------------------+-------+
| trx_id                     | varchar(18)         | NO   |     |                     |       |
| trx_state                  | varchar(13)         | NO   |     |                     |       |
| trx_started                | datetime            | NO   |     | 0000-00-00 00:00:00 |       |
| trx_requested_lock_id      | varchar(81)         | YES  |     | NULL                |       |
| trx_wait_started           | datetime            | YES  |     | NULL                |       |
| trx_weight                 | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_mysql_thread_id        | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_query                  | varchar(1024)       | YES  |     | NULL                |       |
| trx_operation_state        | varchar(64)         | YES  |     | NULL                |       |
| trx_tables_in_use          | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_tables_locked          | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_lock_structs           | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_lock_memory_bytes      | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_rows_locked            | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_rows_modified          | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_concurrency_tickets    | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_isolation_level        | varchar(16)         | NO   |     |                     |       |
| trx_unique_checks          | int(1)              | NO   |     | 0                   |       |
| trx_foreign_key_checks     | int(1)              | NO   |     | 0                   |       |
| trx_last_foreign_key_error | varchar(256)        | YES  |     | NULL                |       |
| trx_adaptive_hash_latched  | int(1)              | NO   |     | 0                   |       |
| trx_adaptive_hash_timeout  | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_is_read_only           | int(1)              | NO   |     | 0                   |       |
| trx_autocommit_non_locking | int(1)              | NO   |     | 0                   |       |
+----------------------------+---------------------+------+-----+---------------------+-------+
24 rows in set (0.00 sec)
```

## 思考题

你现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？

> chatGPT回答

作为业务开发负责人和数据库负责人，以下是一些方案来避免或处理长事务的情况：

1. 优化查询和事务逻辑：通过优化查询语句和事务逻辑，可以减少事务的执行时间。这包括使用合适的索引、避免全表扫描、减少不必要的查询和锁定等。定期进行性能优化和代码审查，确保查询和事务逻辑的效率和合理性。
2. 分批处理和批量提交：对于大批量的数据操作，可以将其分批处理，避免一次性处理过多的数据。同时，可以使用批量提交的方式，将多个操作合并为一个事务，减少事务的数量和执行时间。
3. 合理设置事务超时时间：设置适当的事务超时时间，避免长时间占用数据库资源。如果事务执行时间超过设定的超时时间，可以主动回滚事务并记录相关日志，以便后续分析和处理。
4. 监控和报警：建立监控系统，实时监测数据库的事务执行情况和性能指标。设置合适的阈值和报警规则，及时发现长事务的存在，并采取相应的措施进行处理。
5. 分布式事务管理：对于分布式系统中的事务，可以采用分布式事务管理方案，如使用消息队列、分布式事务协调器等，确保事务的一致性和可靠性。
6. 定期维护和优化数据库：定期进行数据库维护和优化工作，包括索引重建、统计信息更新、碎片整理等，以提高数据库的性能和稳定性。
7. 数据库复制和备份：使用数据库复制和备份机制，确保数据的可靠性和可恢复性。在出现长事务或其他故障时，可以通过备份数据进行恢复，避免数据丢失和业务中断。

总之，通过优化查询和事务逻辑、分批处理和批量提交、合理设置事务超时时间、监控和报警、分布式事务管理、定期维护和优化数据库以及数据库复制和备份等方案，可以有效地避免或处理长事务的情况，提高数据库的性能和可靠性。

> 作者回答

首先，从应用开发端来看：

1. 确认是sdk否使用了set autocommit=0。这个确认工作可以在测试环境中开展，把MySQL的**general_log**开起来，然后随便跑一个业务逻辑，通过general_log的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成1。

2. 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用begin/commit框起来。我见过有些是业务并没有这个需要，但是也把好几个select语句放到了事务中。这种只读事务可以去掉。

3. 业务连接数据库的时候，根据业务本身的预估，通过**SETMAX_EXECUTION_TIME**命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）

其次，从数据库端来看：

1. 监控 information_schema.Innodb_trx表，设置长事务阈值，超过就报警/或者kill；

2. Percona的pt-kill这个工具不错，推荐使用；

3. 在业务功能测试阶段要求输出所有的general_log，分析日志行为提前发现问题；

4. 如果使用的是MySQL 5.6或者更新版本，把innodb_undo_tablespaces设置成2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。



## 补充

### 事务相关指令

1. 显示当前事务状态：

   ```
   SHOW ENGINE INNODB STATUS;
   ```

   这个命令将显示InnoDB存储引擎的状态信息，其中包含了当前事务的相关信息，如事务ID、事务状态等。

2. 开启事务：

   ```
   START TRANSACTION;
   ```

   这个命令用于显式地开启一个事务。在事务中执行的SQL语句将被视为一个原子操作，要么全部执行成功，要么全部回滚。

3. 提交事务：

   ```
   COMMIT;
   ```

   这个命令用于提交当前事务，将之前在事务中执行的SQL语句永久保存到数据库中。

4. 回滚事务：

   ```
   ROLLBACK;
   ```

   这个命令用于回滚当前事务，撤销之前在事务中执行的SQL语句对数据库的修改。

需要注意的是，MySQL默认情况下处于自动提交模式（autocommit），即每个SQL语句都被视为一个独立的事务并自动提交。如果要使用显式事务管理，可以通过设置`autocommit`参数为0来禁用自动提交：

```
SET autocommit = 0;
```

这样，在执行SQL语句时，~~需要显式地使用`START TRANSACTION`来开启事务，并~~使用`COMMIT`或`ROLLBACK`来结束事务。



### 开启general_log

MySQL的general_log是一种日志记录功能，它可以记录MySQL服务器上执行的所有SQL语句，包括查询、更新、事务等操作。通过开启general_log，可以详细了解MySQL服务器上的数据库操作情况，方便进行故障排查、性能分析和安全审计等工作。

要开启MySQL的general_log，可以按照以下步骤进行：

0、查看开启情况

```sql
mysql> show variables like '%general_log%';
+------------------+-------------------------------------+
| Variable_name    | Value                               |
+------------------+-------------------------------------+
| general_log      | OFF                                 |
| general_log_file | /var/lib/mysql/VM-32-165-centos.log |
+------------------+-------------------------------------+

## 临时生效
mysql> set global general_log=on;
```

1、编辑MySQL的配置文件（my.cnf或my.ini），找到并修改以下参数：

```
general_log = on
general_log_file = /data/mysql/log/general.log
```

将general_log设置为1表示开启general_log功能，将general_log_file设置为指定的日志文件路径。

2、重启MySQL服务器，使配置生效。

```bash
systemctl restart mysqld.service 
```

3、执行业务逻辑或操作数据库，MySQL将会将所有的SQL语句记录到指定的general_log文件中。

```bash
cat  /data/mysql/log/general.log 
/usr/libexec/mysqld, Version: 8.0.36 (Source distribution). started with:
Tcp port: 3306  Unix socket: /var/lib/mysql/mysql.sock
Time                 Id Command    Argument
2024-05-14T12:23:10.904093Z         8 Query     show variables like '%general_log%'
2024-05-14T12:24:10.710862Z         8 Quit
```



请注意，开启general_log会产生大量的日志记录，因此在生产环境中应谨慎使用，以免对性能产生负面影响。在测试环境中使用general_log进行确认工作是一种常见的做法，但在生产环境中，应该根据具体需求和安全考虑来决定是否开启general_log。

另外，为了避免general_log文件过大，可以定期清理或限制其大小，以免占用过多的磁盘空间。



# 04 | 深入浅出索引（上）

提到数据库索引，我想你并不陌生，在日常工作中会经常接触到。比如某一个SQL查询比较慢， 分析完原因之后，你可能就会说“给某个字段加个索引吧”之类的解决方案。但到底什么是索引， 索引又是如何工作的呢？今天就让我们一起来聊聊这个话题吧。



数据库索引的内容比较多，我分成了上下两篇文章。索引是数据库系统里面最重要的概念之一， 所以我希望你能够耐心看完。在后面的实战文章中，我也会经常引用这两篇文章中提到的知识 点，加深你对数据库索引的理解。



一句话简单来说，**索引的出现其实就是为了提高数据查询的效率，就像书的目录index一样**。一本500 页的书，如果你想快速找到其中的某一个知识点，在不借助目录的情况下，那我估计你可得找一 会儿。同样，对于数据库的表而言，索引其实就是它的“目录”。



## 索引的常见模型 

索引的出现是为了提高查询效率，但是实现索引的方式却有很多种，所以这里也就引入了**索引模型**的概念。

可以用于提高读写效率的数据结构很多，这里我先给你介绍三种常见、也比较简单的数据结构，它们分别是**哈希表、有序数组和搜索树**。



### 哈希表

下面我主要从使用的角度，为你简单分析一下这三种模型的区别。 

**哈希表**是一种以键-值（key-value）存储数据的结构，我们只要输入待查找的值即key，就可以找 到其对应的值即Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置。

不可避免地，多个key值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方 法是，拉出一个链表。 假设，你现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时对应 的哈希索引的示意图如下所示：

<img src="MYSQL实战45讲.assets/image-20241227122512264.png" alt="image-20241227122512264" style="zoom:50%;" />



图中，User2和User4根据身份证号算出来的值都是N，但没关系，后面还跟了一个链表。假设， 这时候你要查ID_card_n2对应的名字是什么，处理步骤就是：首先，将ID_card_n2通过哈希函 数算出N；然后，按顺序遍历，找到User2。

需要注意的是，图中四个ID_card_n的值并不是递增的，这样做的好处是增加新的User时速度会 很快，只需要往后追加。但缺点是，因为不是有序的，所以**哈希索引做区间查询的速度是很慢的**。

你可以设想下，如果你现在要找身份证号在[ID_card_X, ID_card_Y]这个区间的所有用户，就必须全部扫描一遍了。 

所以，**哈希表这种结构适用于只有等值查询的场景**，比如Memcached及其他一些NoSQL引擎。



### 有序数组

而有序数组在等值查询和范围查询场景中的性能就都非常优秀。还是上面这个根据身份证号 查名字的例子，如果我们使用有序数组来实现的话，示意图如下所示：

<img src="MYSQL实战45讲.assets/image-20241227122829283.png" alt="image-20241227122829283" style="zoom: 50%;" />



这里我们假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候如果你要查ID_card_n2对应的名字，用二分法就可以快速得到，这个时间复杂度是O(log(N))。

同时很显然，这个索引结构支持范围查询。你要查身份证号在[ID_card_X, ID_card_Y]区间的 User，可以先用二分法找到ID_card_X（如果不存在ID_card_X，就找到大于ID_card_X的第一 个User），然后向右遍历，直到查到第一个大于ID_card_Y的身份证号，退出循环。 如果**仅仅看查询效率，有序数组就是最好的数据结构**了。但是，在需要**更新数据的时候就麻烦**了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。



所以，**有序数组索引只适用于静态存储引擎**，比如你要保存的是2017年某个城市的所有人口 信息，这类不会再修改的数据。



### 二叉搜索树

二叉搜索树也是课本里的经典数据结构了。还是上面根据**身份证号查名字**的例子，如果我们用二叉搜索树来实现的话，示意图如下所示：

<img src="MYSQL实战45讲.assets/image-20241227123154069.png" alt="image-20241227123154069" style="zoom:50%;" />

二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。这样如果你要查 ID_card_n2的话，按照图中的搜索顺序就是按照UserA ->UserC->UserF ->User2这个路径得 到。这个时间复杂度是O(log(N))。

当然为了维持O(log(N))的查询复杂度，你就需要**保持这棵树是平衡二叉树**。为了做这个保证，更新的时间复杂度也是O(log(N))。



树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右 递增。

二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。



你可以想象一下一棵100万节点的平衡二叉树，树高20。一次查询可能需要访问20个数据块。在 机械硬盘时代，从磁盘随机读一个数据块需要10 ms左右的寻址时间。也就是说，对于一个100 万行的表，如果使用二叉树来存储，单独访问一个行可能需要20个10 ms的时间，这个查询可真够慢的。



为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该 使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于**数据块**的大小。



以InnoDB的一个整数字段索引为例，这个N差不多是1200。这棵树高是4的时候，就可以存 1200的3次方个值，这已经17亿了。考虑到树根的数据块总是在内存中的，一个10亿行的表上一 个整数字段的索引，查找一个值最多只需要访问3次磁盘。其实，树的第二层也有很大概率在内 存中，那么访问磁盘的平均次数就更少了。



N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中 了。 不管是哈希还是有序数组，或者N叉树，它们都是不断迭代、不断优化的产物或者解决方案。数 据库技术发展到今天，跳表、LSM树等数据结构也被用于引擎设计中，这里我就不再一一展开 了。



你心里要有个概念，数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据库，我 们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。 截止到这里，我用了半篇文章的篇幅和你介绍了不同的数据结构，以及它们的适用场景，你可能 会觉得有些枯燥。但是，我建议你还是要多花一些时间来理解这部分内容，毕竟这是数据库处理 数据的核心概念之一，在分析问题的时候会经常用到。当你理解了索引的模型后，就会发现在分 析问题的时候会有一个更清晰的视角，体会到引擎设计的精妙之处。 



## InnoDB 的索引模型

现在，我们一起进入相对偏实战的内容吧。在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索 引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不 同。由于InnoDB存储引擎在MySQL数据库中使用最为广泛，所以下面我就以InnoDB为例，和 你分析一下其中的索引模型。



在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。 又因为前面我们提到的，InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。 每一个索引在InnoDB里面对应一棵B+树。 假设，我们有一个主键列为ID的表，表中有字段k，并且在k上有索引。



这个表的建表语句是：

```mysql
mysql> create table T (
    id int primary key,
    k int not null,
    name varchar(16),
    index (k))engine=InnoDB;
```



表中R1~R5的(id,k)值分别为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)，两棵树的示例示意 图如下。

<img src="MYSQL实战45讲.assets/image-20241227174542135.png" alt="image-20241227174542135" style="zoom:50%;" />

从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存的是整行数据。

在InnoDB里，主键索引也被称为聚簇索引（clustered index）。 

非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引 （secondary index）。



根据上面的索引结构说明，我们来讨论一个问题：基于主键索引和普通索引的查询有什么区别？

- 如果语句是select *fromTwhere ID=500，即主键查询方式，则只需要搜索ID这棵B+树； 
- 如果语句是select *fromTwhere k=5，即普通索引查询方式，则需要先搜索k索引树，得到ID 的值为500，再到ID索引树搜索一次。这个过程称为**回表**。 

也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主 键查询。



## 索引维护(更新)

B+树为了维护**索引有序性**，在插入新值的时候需要做必要的维护。



以上面这个图为例，如果插 入新的行ID值为700，则只需要在R5的记录后面插入一个新记录。

如果新插入的ID值为400，就 相对麻烦了，需要逻辑上挪动后面的数据，空出位置。

而更糟的情况是，如果R5所在的数据页已经满了，根据B+树的算法，这时候需要申请一个新的 数据页，然后挪动部分数据过去。这个过程称为**页分裂**。在这种情况下，性能自然会受影响。



除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中， 整体空间利用率降低大约50%。



当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做**合并**。合并的过程，可以认为是分裂过程的逆过程。 基于上面的索引维护过程说明，我们来讨论一个案例：

> 你可能在一些建表规范里面见到过类似的描述，要求建表语句里一定要有自增主键。当然事无 绝对，我们来分析一下哪些场景下应该使用自增主键，而哪些场景下不应该。

自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。

插入新记录的时候可以不指定ID的值，系统会获取当前ID最大值加1作为下一条记录的ID值。 也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条 新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。



而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。

除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如 字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？ 

由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的 叶子节点占用约20个字节，而如果用整型做主键，则只要4个字节，如果是长整型（bigint）则是8个字节。 显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 所以，**从性能和存储空间方面考量，自增主键往往是更合理的选择**。



有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样 的： 1. 只有一个索引； 2. 该索引必须是唯一索引。 你一定看出来了，这就是典型的KV场景。 由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。 这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键， 可以避免每次查询需要搜索两棵树。



## 小结

今天，我跟你分析了数据库引擎可用的数据结构，介绍了InnoDB采用的B+树结构，以及为什么 InnoDB要这么选择。

B+树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。 

由于InnoDB是索引组织表，一般情况下我会建议你创建一个自增主键，这样非主键索引占用的 空间最小。

但事无绝对，我也跟你讨论了使用业务逻辑字段做主键的应用场景。 



## 问题

最后，我给你留下一个问题吧。对于上面例子中的InnoDB表T，如果你要重建索引 k，你的两个 SQL语句可以这么写：

```
alter table T drop index k;
alter table T add index(k);
```

如果你要重建主键索引，也可以这么写：

```
alter table T drop primary key;
alter table T add primary key(id);
```

我的问题是，对于上面这两个重建索引的作法，说出你的理解。如果有不合适的，为什么，更好 的方法是什么？

你可以把你的思考和观点写在留言区里，我会在下一篇文章的末尾给出我的参考答案。感谢你的 收听，也欢迎你把这篇文章分享给更多的朋友一起阅读。



### 回答

在评论区，有同学问到为什么要重建索引。我们文章里面有提到，索引可能因为删除，或者页分 裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样 页面的利用率最高，也就是索引更紧凑、更省空间。 

这道题目，我给你的“参考答案”是： 重建索引k的做法是合理的，可以达到省空间的目的。但是，重建主键的过程不合理。不论是删 除主键还是创建主键，都会将整个表重建。所以连着执行这两个语句的话，第一个语句就白做 了。这两个语句，你可以用这个语句代替 ： `alter table T engine=InnoDB`





### chatGPT

不合适的地方

1. **性能问题**：
   - 在删除索引和重新添加索引的过程中，表会被锁定，可能会导致长时间的写入延迟，影响应用程序的性能。
   - 对于大型表，删除和重新创建索引可能会消耗大量的时间和资源。
2. **数据完整性**：
   - 在删除主键索引期间，表的完整性可能会受到影响，尤其是如果有外键依赖于该主键。
3. **事务安全性**：
   - 如果在删除索引和添加索引之间发生错误，可能会导致表处于不一致状态。

更好的方法

1. **使用 `ALTER TABLE ... DROP INDEX ... ADD INDEX ...` 一次性重建索引**：

   - MySQL 允许在同一个 `ALTER TABLE` 语句中同时删除和添加索引，这样可以减少锁定时间和资源消耗。

   ```sql
   ALTER TABLE T DROP INDEX k, ADD INDEX  k_name(k);
   ```

   ![image-20241227181041450](MYSQL实战45讲.assets/image-20241227181041450.png)

2. **使用 `ALTER TABLE ... CHANGE` 或 `MODIFY`**：

   - 对于主键索引，可以使用 `CHANGE` 或 `MODIFY` 来重建主键，而不需要显式地删除它。

   ```sql
   ALTER TABLE employees CHANGE id id INT NOT NULL PRIMARY KEY;
   ALTER TABLE employees MODIFY id INT NOT NULL PRIMARY KEY;
   
   ```

3. **使用 `ONLINE` 选项**（如果使用的是 MySQL 5.6 及以上版本）：

   - 在某些情况下，可以使用 `ONLINE` 选项来重建索引而不锁定表。

   ```
   ALTER TABLE T ADD INDEX k (column_name) ALGORITHM=INPLACE, LOCK=NONE;
   ```





# 05 | 深入浅出索引（下）

今天我们再继续聊聊跟MySQL索 引有关的概念。 

在开始这篇文章之前，我们先来看一下这个问题： 在下面这个表T中，如果我执行 `select * from T where k between 3 and 5`，需要执行几次树的搜索操作，会扫描多少行？ 下面是这个表的初始化语句。

```sql
mysql> create table T (
ID int primary key,
k int NOT NULL DEFAULT 0,
s varchar(16) NOT NULL DEFAULT '',
index k(k))
engine=InnoDB;


insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg')
```

<img src="MYSQL实战45讲.assets/image-20241227200431625.png" alt="image-20241227200431625" style="zoom:50%;" />



现在，我们一起来看看这条SQL查询语句的执行流程： 

1. 在k索引树上找到k=3的记录，取得 ID = 300； 
2.  再到ID索引树查到ID=300对应的R3； 
3. 在k索引树取下一个值k=5，取得ID=500； 
4. 再回到ID索引树查到ID=500对应的R4； 
5. 在k索引树取下一个值k=6，不满足条件，循环结束。



在这个过程中，回到主键索引树搜索的过程，我们称为回表。

可以看到，这个查询过程读了k 索引树的3条记录（步骤1、3和5），回表了两次（步骤2和4）。 在这个例子中，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。那么，有没有可能经过索引优化，避免回表过程呢？



## 覆盖索引

如果执行的语句是select ID fromTwhere k between 3 and 5，这时只需要查ID的值，而ID的值 已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，**在这个查询里面， 索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引**。



由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。 

需要注意的是，在引擎内部使用覆盖索引在索引k上其实读了三个记录，R3~R5（对应的索引k 上的记录项），但**是对于MySQL的Server层来说，它就是找引擎拿到了两条记录，因此MySQL 认为扫描行数是2(两次回表）**。

> 备注：关于如何查看扫描行数的问题，我将会在第16文章《如何正确地显示随机消息？》 中，和你详细讨论。



基于上面覆盖索引的说明，我们来讨论一个问题：在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？ 假设这个市民表的定义是这样的：

```sql
CREATE TABLE `tuser` (
    `id` int(11) NOT NULL,
    `id_card` varchar(32) DEFAULT NULL,
    `name` varchar(32) DEFAULT NULL,
    `age` int(11) DEFAULT NULL,
    `ismale` tinyint(1) DEFAULT NULL,
    PRIMARY KEY (`id`),
    KEY `id_card` (`id_card`),
    KEY `name_age` (`name`,`age`)
) ENGINE=InnoDB

```



我们知道，身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求， 我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是 不是浪费空间？

如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它 可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。

当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑 了。这正是业务DBA，或者称为业务数据架构师的工作。



## 最左前缀原则

看到这里你一定有一个疑问，如果为每一种查询都设计一个索引，索引是不是太多了。如果我现 在要按照市民的身份证号去查他的家庭地址呢？虽然这个查询需求在业务中出现的概率不高，但 总不能让它走全表扫描吧？反过来说，单独为一个不频繁的请求创建一个（身份证号，地址）的 索引又感觉有点浪费。应该怎么做呢？ 

这里，我先和你说结论吧。**B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录。** 为了直观地说明这个概念，我们用（name，age）这个联合索引来分析。

<img src="MYSQL实战45讲.assets/image-20250102122626442.png" alt="image-20250102122626442" style="zoom: 50%;" />



可以看到，索引项是按照索引定义里面出现的字段顺序排序的。 当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到ID4，然后向后遍历得到所有 需要的结果。

如果你要查的是所有名字第一个字是“张”的人，你的SQL语句的条件是"where name like ‘张%’"。这时，你也能够用上这个索引，查找到第一个符合条件的记录是ID3，然后向后遍历， 直到不满足条件为止。 

可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。**这个最左 前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符**。



基于上面对最左前缀索引的说明，我们来讨论一个问题：在建立联合索引的时候，**如何安排索 引内的字段顺序**。 

这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了(a,b)这个联 合索引后，一般就不需要单独在a上建立索引了。

因此，**第一原则是，如果通过调整顺序，可 以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的**。



所以现在你知道了，这段开头的问题里，我们要为高频请求创建(身份证号，姓名）这个联合索 引，并用这个索引支持“根据身份证号查询地址”的需求。 

那么，如果既有联合查询，又有基于a、b各自的查询呢？查询条件里面只有b的语句，是无法使 用(a,b)这个联合索引的，这时候你不得不维护另外一个索引，也就是说你**需要同时维护(a,b)、 (b)** 这两个索引。

这时候，我们要考虑的原则就是空间了。比如上面这个市民表的情况，name字段是比age字段 大的 ，那我就建议你创建一个（name,age)的联合索引和一个(age)的单字段索引。



## 索引下推

上一段我们说到满足最左前缀原则的时候，最左前缀可以用于在索引中定位记录。这时，你可能 要问，那些不符合最左前缀的部分，会怎么样呢？ 

我们还是以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一 个字是张，而且年龄是10岁的所有男孩”。那么，SQL语句是这么写的：

```sql
mysql> select * from tuser where name like '张%' and age=10 and ismale=1;
```



你已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足 条件的记录ID3。当然，这还不错，总比全表扫描要好。



然后呢？ 当然是判断其他条件是否满足。 在MySQL 5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。



而MySQL 5.6 引入的**索引下推优化（index condition pushdown)**， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。



图3和图4，是这两个过程的执行流程图。

<img src="MYSQL实战45讲.assets/image-20250103121301961.png" alt="image-20250103121301961" style="zoom: 50%;" />

<img src="MYSQL实战45讲.assets/image-20250103121414858.png" alt="image-20250103121414858" style="zoom:50%;" />



在图3和4这两个图里面，每一个虚线箭头表示回表一次。

图3中，在(name,age)索引里面我特意去掉了age的值，这个过程InnoDB并不会去看age的值， 只是按顺序把“name第一个字是’张’”的记录一条条取出来回表。因此，需要回表4次。 

图4跟图3的区别是，InnoDB在(name,age)索引内部就判断了age是否等于10，对于不等于10的 记录，直接判断并跳过。

在我们的这个例子中，只需要对ID4、ID5这两条记录回表取数据判 断，就只需要回表2次。



## 小结 

今天这篇文章，我和你继续讨论了数据库索引的概念，包括了**覆盖索引、前缀索引、索引下推**。 你可以看到，在满足语句需求的情况下， 尽量少地访问资源是数据库设计的重要原则之一。我 们在使用数据库的时候，尤其是在设计表结构时，也要以减少资源消耗作为目标。



### 问题

接下来我给你留下一个问题吧。 实际上主键索引也是可以使用多个字段的。DBA小吕在入职新公司的时候，就发现自己接手维 护的库里面，有这么一个表，表结构定义类似这样的：

```sql
CREATE TABLE `geek` (
    `a` int(11) NOT NULL,
    `b` int(11) NOT NULL,
    `c` int(11) NOT NULL,
    `d` int(11) NOT NULL,
    PRIMARY KEY (`a`,`b`),
    KEY `c` (`c`),
    KEY `ca` (`c`,`a`),
    KEY `cb` (`c`,`b`)
) ENGINE=InnoDB;

```

公司的同事告诉他说，由于历史原因，这个表需要a、b做联合主键，这个小吕理解了。 但是，学过本章内容的小吕又纳闷了，既然主键包含了a、b这两个字段，那意味着单独在字段c 上创建一个索引，就已经包含了三个字段了呀，为什么要创建“ca”“cb”这两个索引？ 同事告诉他，是因为他们的业务里面有这样的两种语句：

```
select * from geek where c=N order by a limit 1;
select * from geek where c=N order by b limit 1;
```

我给你的问题是，这位同事的解释对吗，为了这两个查询模式，这两个索引是否都是必须的？为 什么呢？



### 回答

上期的问题是关于对联合主键索引和InnoDB索引组织表的理解。

我直接贴@老杨同志 的回复略作修改如下（我修改的部分用橙色标出）： 

表记录

```
–a--|–b--|–c--|–d--
1 2 3 d
1 3 2 d
1 4 3 d
2 1 3 d
2 2 2 d
2 3 4 d
```

主键 a，b的聚簇索引组织顺序相当于 order bya,b ，也就是先按a排序，再按b排序，c无序。



索引 ca 的组织是先按c排序，再按a排序，同时记录主键

```
–c--|–a--|–主键部分b-- （注意，这里不是ab，而是只有b）
2 1 3
2 2 2
3 1 2
3 1 4
3 2 1
4 2 3
```

这个跟索引c的数据是一模一样的。



索引 cb 的组织是先按c排序，在按b排序，同时记录主键

```
–c--|–b--|–主键部分a-- （同上）
2 2 2
2 3 1
3 1 2
3 2 1
3 4 1
4 3 2
```

所以，结论是ca可以去掉，cb需要保留。

> PS：索引可以加快排序速度

如果您在MySQL中创建了一个索引，例如`KEY('a', 'b')`，并且您的SQL语句中包含了`ORDER BY b`子句，那么这个索引可以提高查询语句的性能。

索引的作用是加快数据库的查询速度，通过创建索引，MySQL可以更快地定位到满足查询条件的数据行。当您在索引中包含了`b`列，并且查询语句中使用了`ORDER BY b`进行排序时，MySQL可以直接利用索引中的排序信息，避免对所有数据进行排序操作。

具体来说，当查询语句包含`ORDER BY b`时，MySQL可以使用索引中的排序顺序来直接返回按照`b`列排序的结果，而无需额外的排序操作。这样可以大大减少查询的时间和资源消耗，提高查询性能。









# 06 | 全局锁和表锁 ：给表加个字段怎么有这么多阻碍？

数据库锁设计的初衷是处理并发问题。

作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。

根据加锁的范围，MySQL中的锁大致可以分为**全局锁、表级锁和行锁**。

这篇文章分享全局锁和表级锁。

需要说明的是，锁的设计比较复杂，这两篇文章不会涉及锁的具体实现细节，主要介绍的是碰到锁时的现象和其背后的原理。



## 全局锁

顾名思义，全局锁就是对整个数据库实例加锁。

### FTWRL

MySQL提供了一个加全局读锁的方法，命令是**Flush tables with read lock (FTWRL)**。

当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句

**全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本。**

以前有一种做法，是通过FTWRL确保不会有其他线程对数据库做更新，然后对整个库做备份。

注意，在备份过程中整个库完全处于只读状态。但是让整库都只读，听上去就很危险（有以下问题）：

- 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；
- 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟。

看来加全局锁不太好。但是细想一下，备份为什么要加锁呢？我们来看一下不加锁会有什么问题。

假设你现在要维护“极客时间”的购买系统，关注的是用户账户余额表和用户课程表。

 现在发起一个逻辑备份。假设备份期间，有一个用户，他购买了一门课程，业务逻辑里就要扣掉 他的余额，然后往已购课程里面加上一门课。 如果时间顺序上是先备份账户余额表(u_account)，然后用户购买，然后备份用户课程表 (u_course)，会怎么样呢？你可以看一下这个图：

<img src="MYSQL实战45讲.assets/image-20250103193922935.png" alt="image-20250103193922935" style="zoom:50%;" />



可以看到，这个备份结果里，用户A的数据状态是“账户余额没扣，但是用户课程表里面已经多了一门课”。如果后面用这个备份来恢复数据的话，用户A就发现，自己赚了。

作为用户可别觉得这样可真好啊，你可以试想一下：如果备份表的顺序反过来，先备份用户课程表再备份账户余额表，又可能会出现什么结果？ 



也就是说，不加锁的话，备份系统备份得到的库（不同的表）不是一个逻辑时间点，这个视图是逻辑不一致的。

说到视图你肯定想起来了，我们在前面讲事务隔离的时候，其实是有一个方法能够拿到一致性视图的，对吧？是的，就是**在可重复读隔离级别下开启一个事务**。



### mysqldump工具

官方自带的逻辑备份工具是mysqldump。

**当mysqldump使用参数–single-transaction的时候**，导数据之前就会启动一个事务，来确保拿到一致性视图。

而由于MVCC的支持，这个过程中数据是可以正常更新的。

为什么还需要FTWRL呢？

一致性读是好，但前提是引擎要支持这个隔离级别。 

比如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。

这时，我们就需要使用FTWRL命令了。如果有的表使用了不支持事务的引擎，那么备份就只能通过FTWRL方法。这往往是DBA要求业务开发人员使用InnoDB替代MyISAM的原因之一。



你也许会问，既然要全库只读，为什么不使用 既 set global readonly=true 的方式呢 的 ？确实readonly方式也可以让全库进入只读状态，但还是会建议你用FTWRL方式，主要有两个原因：

- 一是，在有些系统中，**readonly的值会被用来做其他逻辑**，比如用来判断一个库是主库还是备库。因此，修改global变量的方式影响面更大，我不建议你使用。
- 二是，**在异常处理机制上有差异**。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。

**业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。**不论是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操作，都是会被锁住的。但是，即使没有被全局锁住，加字段也不是就能一帆风顺的，因为你还会碰到接下来我们要介绍的表级锁



## 表级锁

MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。

### 表锁

表锁的语法是 `lock tables ... read/write`.

与FTWRL类似，可以用`unlock tables`主动释放锁，也可以在客户端断开的时候自动释放。

需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。

举个例子, 如果在某个线程A中执行**lock tables t1 read, t2 write;** 这个语句，则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许，自然也不能访问其他表。

在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。

而对于InnoDB这种支持**行锁**的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。



### 元数据锁（meta data lock，MDL)

MDL不需要显式使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。

因此，**在MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁**。

- 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。(互斥的意思是只有一个锁)
- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行

虽然MDL锁是系统默认会加的，但却是你不能忽略的一个机制。比如下面这个例子，**经常看到有人掉到这个坑里：给一个小表加个字段，导致整个库挂了**。

**给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据**。

在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，即使是小表，操作不慎也会出问题。我们来看一下下面的操作序列，假设表t是一个小表。

<img src="MYSQL实战45讲.assets/image-20240604194259700.png" alt="image-20240604194259700" style="zoom:50%;" />

我们可以看到session A先启动，这时候会对表t加一个MDL读锁。由于session B需要的也是MDL读锁，因此可以正常执行。

之后session C会被blocked，是因为session A的MDL读锁还没有释放，而session C需要MDL写锁，因此只能被阻塞。



如果只有session C自己被阻塞还没什么关系，但是之后所有要在表t上新申请MDL读锁的请求也会被session C阻塞。前面我们说了，所有对表的增删改查操作都需要先申请MDL读锁，就都被锁住，等于这个表现在完全不可读写了。

如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新session

再请求的话，这个库的线程很快就会爆满。

你现在应该知道了，事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。



基于上面的分析，我们来讨论一个问题，

**首先我们要解决掉长事务，事务不提交，就会一直占着MDL锁。**在MySQL的information_schema库的 innodb_trx表中，你可以查到当前执行中的事务。如果你要做DDL变更的表刚好有长事务在执行，要考虑先暂停DDL，或者kill掉这个长事务。



但考虑一下这个场景。如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？

这时候kill可能未必管用，因为新的请求马上就来了。

比较理想的机制是，在alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。

MariaDB已经合并了AliSQL的这个功能，所以这两个开源分支目前都支持DDL NOWAIT/WAITn这个语法

```
ALTER TABLE tbl_name NOWAIT add column ...
ALTER TABLE tbl_name WAIT N add column ...
```

## 小结

全局锁主要用在逻辑备份过程中。对于全部是InnoDB引擎的库，我建议你选择使用–singletransaction参数，对应用会更友好

表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有lock tables这样的语句，你需要追查一下，比较可能的情况是： 要么是你的系统现在还在用MyISAM这类不支持事务的引擎，那要安排升级换引擎； 要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。

MDL会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和 更新。

### 问题

最后，我给你留一个问题吧。备份一般都会在备库上执行，你在用–single-transaction方法做逻 辑备份的过程中，如果主库上的一个小表做了一个DDL，比如给一个表上加了一列。这时候，从 备库上会看到什么现象呢？

> 回答

假设这个DDL是针对表t1的， 这里我把备份过程中几个关键的语句列出来：

```
Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;
Q2:START TRANSACTION WITH CONSISTENT SNAPSHOT；
/* other tables */
Q3:SAVEPOINT sp;
/* 时刻 1 */
Q4:show create table `t1`;
/* 时刻 2 */
Q5:SELECT * FROM `t1`;
/* 时刻 3 */
Q6:ROLLBACK TO SAVEPOINT sp;
/* 时刻 4 */
/* other tables */

```

1、在备份开始的时候，为了确保RR（可重复读）隔离级别，再设置一次RR隔离级别(Q1); 

2、启动事务，这里用 WITH CONSISTENTSNAPSHOT确保这个语句执行完就可以得到一个一致性 视图（Q2)；

3、设置一个保存点，这个很重要（Q3）；

4、showcreate 是为了拿到表结构(Q4)，然后正式导数据 （Q5），回滚到SAVEPOINTsp，在这里的作用是释放 t1的MDL锁 （Q6）。当然这部分属于“超纲”，上文正文里面都没提到。



DDL从主库传过来的时间按照效果不同，我打了四个时刻。题目设定为小表，我们假定到达后， 如果开始执行，则很快能够执行完成。

参考答案如下： 

1. 如果在Q4语句执行之前到达，现象：没有影响，备份拿到的是DDL后的表结构。 
2. 如果在“时刻 2”到达，则表结构被改过，Q5执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump终止； 
3. 如果在“时刻2”和“时刻3”之间到达，mysqldump占着t1的MDL读锁，binlog被阻塞，**现象： 主从延迟**，直到Q6执行完成。 
4. 从“时刻4”开始，mysqldump释放了MDL读锁，现象：没有影响，备份拿到的是DDL前的表结构。





# 07 | 行锁功过：怎么减少行锁对性能的影响？



在上一篇文章中，我跟你介绍了MySQL的全局锁和表级锁，今天我们就来讲讲MySQL的**行锁**。

MySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 **MyISAM引擎就不支持行锁**。

不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。

InnoDB是支持行锁的， 这也是MyISAM被InnoDB替代的重要原因之一。



我们今天就主要来聊聊InnoDB的行锁，以及如何通过减少锁冲突来提升业务并发度。

顾名思义，行锁就是针对数据表中行记录的锁。这很好理解，比如事务A更新了一行，而这时候 事务B也要更新同一行，则必须等事务A的操作完成后才能进行更新。



当然，数据库中还有一些没那么一目了然的概念和设计，这些概念如果理解和使用不当，容易导 致程序出现非预期行为，比如两阶段锁。



## 从两阶段锁说起

我先给你举个例子。在下面的操作序列中，事务B的update语句执行时会是什么现象呢？假设字 段id是表t的主键。

<img src="MYSQL实战45讲.assets/image-20250108181942677.png" alt="image-20250108181942677" style="zoom: 50%;" />

这个问题的结论取决于事务A在执行完两条update语句后，持有哪些锁，以及在什么时候释放。 你可以验证一下：实际上事务B的update语句会被阻塞，直到事务A执行commit之后，事务B才 能继续执行。



知道了这个答案，你一定知道了**事务A持有的两个记录的行锁，都是在commit的时候才释放的**。



也就是说，**在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放**。这个就是**两阶段锁协议**。



知道了这个设定，对我们使用事务有什么帮助呢？

那就是，如果你的事务中需要锁多个行，要**把最可能造成锁冲突、最可能影响并发度的锁尽量往后放**。我给你举个例子。



假设你负责实现一个电影票在线交易业务，顾客A要在影院B购买电影票。我们简化一点，这个 业务需要涉及到以下操作：

1. 从顾客A账户余额中扣除电影票价； 
2. 给影院B的账户余额增加这张电影票价；
3. 记录一条交易日志。

也就是说，要完成这个交易，我们需要update两条记录，并insert一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。那么，你会怎样安排这三个语句在事务中的 顺序呢？

试想如果同时有另外一个顾客C要在影院B买票，那么这两个事务冲突的部分就是语句2了。因为 它们要更新同一个影院账户的余额，需要修改同一行数据。



根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你**把语句2安排在最后**，比如按照3、1、2这样的顺序，那么影院账户余额 这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。



好了，现在由于你的正确设计，影院余额这一行的行锁在一个事务中不会停留很长时间。但是， 这并没有完全解决你的困扰。



如果这个影院做活动，可以低价预售一年内所有的电影票，而且这个活动只做一天。于是在活动 时间开始的时候，你的MySQL就挂了。你登上服务器一看，CPU消耗接近100%，但整个数据库每秒就执行不到100个事务。这是什么原因呢？



这里，我就要说到死锁和死锁检测了。



## 死锁和死锁检测

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致 这几个线程都进入无限等待的状态，称为死锁。这里我用数据库中的行锁举个例子。

<img src="MYSQL实战45讲.assets/image-20250108182637092.png" alt="image-20250108182637092" style="zoom:50%;" />

这时候，事务A在等待事务B释放id=2的行锁，而事务B在等待事务A释放id=1的行锁。 事务A和 事务B在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：

- 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 **innodb_lock_wait_timeout**来设置。 
- 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数**innodb_deadlock_detect**设置为on，表示开启这个逻辑。

在InnoDB中，innodb_lock_wait_timeout的默认值是50s，意味着如果采用第一个策略，当出现 死锁以后，第一个被锁住的线程要过50s才会超时退出，然后其他线程才有可能继续执行。对于 在线服务来说，这个等待时间往往是无法接受的。

但是，我们又不可能直接把这个时间设置成一个很小的值，比如1s。这样当出现死锁的时候，确 实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会 出现很多误伤。



所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect的默认值本身就是on。主动死锁检测在发生死锁的时候，是能够快速发 现并进行处理的，但是它也是有额外负担的。



你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。

那如果是我们上面说到的所有事务都要更新同一行的场景呢？



每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级 的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，你就会看到 CPU利用率很高，但是每秒却执行不了几个事务。



根据上面的分析，我们来讨论一下，**怎么解决由这种热点行更新导致的性能问题呢**？问题的**症结在于，死锁检测要耗费大量的CPU资源**。



**一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉**。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严 重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关 掉死锁检测意味着可能会出现大量的超时，这是业务有损的。



**另一个思路是控制并发度**。根据上面的分析，你会发现如果并发能够控制住，比如同一行同时最多只有10个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有600个客户端，这样即使每个客户端控制到只有5个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到3000。

因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的 团队有能修改MySQL源码的人，也可以做在MySQL里面。**基本思路就是，对于相同行的更新， 在进入引擎之前排队。**这样在InnoDB内部就不会有大量的死锁检测工作了。



可能你会问，如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设 计上优化这个问题呢？

**你可以考虑通过将一行改成逻辑上的多行来减少锁冲突**。还是以影院账户为例，可以考虑放在多 条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账 户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的1/10，可以减少锁等 待个数，也就减少了死锁检测的CPU消耗。

这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会 减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成0的时候，代码要有特殊处理。



## 小结

今天，我和你介绍了MySQL的行锁，涉及了两阶段锁协议、死锁和死锁检测这两大部分内容。

其中，我以两阶段协议为起点，和你一起讨论了在开发的时候如何安排正确的事务语句。这里的 原则/我给你的建议是：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并 发度的锁的申请时机尽量往后放。

但是，调整语句顺序并不能完全避免死锁。所以我们引入了死锁和死锁检测的概念，以及提供了 三个方案，来减少死锁对数据库的影响。减少死锁的主要方向，就是控制访问相同资源的并发事 务量。



### 问题

最后，我给你留下一个问题吧。如果你要删除一个表里面的前10000行数据，有以下三种方法可 以做到：

- 第一种，直接执行delete fromTlimit 10000; 
- 第二种，在一个连接中循环执行20次 delete fromTlimit 500; 
- 第三种，在20个连接中同时执行delete fromTlimit 500。

你会选择哪一种方法呢？为什么呢？



### 回答

比较多的留言都选择了第二 种方式，即：在一个连接中循环执行20次 delete fromTlimit 500。 确实是这样的，第二种方式是相对较好的。

第一种方式（即：直接执行delete fromTlimit 10000）里面，单个语句占用时间长，锁的时间也 比较长；而且大事务还会导致主从延迟。

第三种方式（即：在20个连接中同时执行delete fromTlimit 500），会人为造成锁冲突（并发大）。



# 08 | 事务到底是隔离的还是不隔离的？













































































# 11 | 怎么给字符串字段加索引？

现在，几乎所有的系统都支持邮箱登录，如何在邮箱这样的字段上建立合理的索引，是我们今天要讨论的问题。

假设，你现在维护一个支持邮箱登录的系统，用户表是这么定义的：

```sql
mysql> create table SUser(
ID bigint unsigned primary key,
email varchar(64),
...
)engine=innodb;
```

由于要使用邮箱登录，所以业务代码中一定会出现类似于这样的语句：

```
mysql> select f1, f2 from SUser where email='xxx';
```

如果email这个字段上没有索引，那么这个语句就只能做全表扫描。

同时，**MySQL是支持前缀索引的，也就是说，你可以定义字符串的一部分作为索引**。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串.

比如，这两个在email字段上创建索引的语句：

```mysql
mysql> alter table SUser add index index1(email);
或
mysql> alter table SUser add index index2(email(6));
```

第一个语句创建的index1索引里面，包含了每个记录的整个字符串；而第二个语句创建的index2索引里面，对于每个记录都是只取前6个字节。

那么，这两种不同的定义在数据结构和存储上有什么区别呢？如图2和3所示，就是这两个索引的示意图。

<img src="MYSQL实战45讲.assets/image-20240612200303046.png" alt="image-20240612200303046" style="zoom:50%;" />

<img src="MYSQL实战45讲.assets/image-20240612200316004.png" alt="image-20240612200316004" style="zoom:50%;" />



从图中你可以看到，由于email(6)这个索引结构中每个邮箱字段都只取前6个字节（即：zhangs），所以**占用的空间会更小**，这就是使用前缀索引的优势。

但，这同时带来的损失是，**可能会增加额外的记录扫描次数**。

接下来，我们再看看下面这个语句，在这两个索引定义下分别是怎么执行的。

```
select id,name,email from SUser where email='zhangssxyz@xxx.com';
```

如果使用index1全字符索引，，执行顺序是这样的：

1. 从index1索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得ID2的值；

2. 到主键上查到主键值是ID2的行，判断email的值是正确的，将这行记录加入结果集；

3. 取index1索引树上刚刚查到的位置的下一条记录，发现已经不满足email='zhangssxyz@xxx.com’的条件了，循环结束。

这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。



如果使用index2前缀索引，，执行顺序是这样的：

1. 从index2索引树找到满足索引值是’zhangs’的记录，找到的第一个是ID1；

2. 到主键上查到主键值是ID1的行，判断出email的值不是’zhangssxyz@xxx.com’，这行记录丢

弃；

3. 取index2上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出ID2，再到ID索引上取

整行然后判断，这次值对了，将这行记录加入结果集；

4. 重复上一步，直到在idxe2上取到的值不是’zhangs’时，循环结束。

在这个过程中，要回主键索引取4次数据，也就是扫描了4行。通过这个对比，你很容易就可以发现，使用前缀索引后，可能会导致查询语句读数据的次数变

多。

但是，对于这个查询语句来说，如果你定义的index2不是email(6)而是email(7），也就是说取email字段的前7个字节来构建索引的话，即满足前缀’zhangss’的记录只有一个，也能够直接查到ID2，只扫描一行就结束了。



也就是说使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多查询成本（增加查询次数）



# 16 | “order by”是怎么工作的？

在你开发应用的时候，一定会经常碰到需要根据指定的字段排序来显示结果的需求。还是以我们前面举例用过的市民表为例，假设你要查询城市是“杭州”的所有人名字，并且按照姓名排序返回前1000个人的姓名、年龄。

假设这个表的部分定义是这样的：

```sql
CREATE TABLE `t` (
    `id` int(11) NOT NULL,
    `city` varchar(16) NOT NULL,
    `name` varchar(16) NOT NULL,
    `age` int(11) NOT NULL,
    `addr` varchar(128) DEFAULT NULL,
    PRIMARY KEY (`id`),
    KEY `city` (`city`)
) ENGINE=InnoDB;
```

这时，你的SQL语句可以这么写：

```SQL
select city,name,age from t where city='杭州' order by name limit 1000 ;
```

这个语句看上去逻辑很清晰，但是你了解它的执行流程吗？今天，我就和你聊聊这个语句是怎么执行的，以及有什么参数会影响执行的行为。

## 全字段排序

前面我们介绍过索引，所以你现在就很清楚了，为避免全表扫描，我们需要在city字段加上索引。

在city字段上创建索引之后，我们用explain命令来看看这个语句的执行情况。

```sql
mysql> explain select city,name,age from t where city='杭州' order by name limit 1000 ;
```

![image-20240426192519643](MYSQL实战45讲.assets/image-20240426192519643.png)

​                                                                                                  图1 使用explain命令查看语句的执行情况

这是一个 MySQL 的 `EXPLAIN` 查询结果，用于解释查询语句的执行计划和性能优化信息。下面是对每个列的解释：

- `id`: 查询中每个操作的标识符。在这个例子中，只有一个操作，因此为 1。
- `select_type`: 查询的类型。在这个例子中，是 `SIMPLE`，表示这是一个简单的 SELECT 查询。
- `table`: 查询涉及的表名。
- `partitions`: 表的分区信息。在这个例子中，为 `NULL`，表示没有使用分区。
- `type`: 访问类型，表示查询时使用的访问方法。在这个例子中，是 `ref`，表示使用索引进行查询。
- `possible_keys`: 可能使用的索引列表。
- `key`: 实际使用的索引。
- `key_len`: 使用的索引长度。
- `ref`: 表示索引的哪个列或常量与查询条件进行比较。
- `rows`: 估计的扫描行数。
- `filtered`: 表示在表中过滤的行的百分比。
- `Extra`: 额外的信息和优化提示。在这个例子中，表示使用了索引条件和文件排序。

Extra这个字段中的“Using filesort”表示的就是需要排序，**MySQL会给每个线程分配一块内存用于排序，称为sort_buffer。**

为了说明这个SQL查询语句的执行过程，我们先来看一下city这个索引的示意图。

<img src="MYSQL实战45讲.assets/image-20240426192103573.png" alt="image-20240426192103573" style="zoom:50%;" />

​                                                                                                      图2 city字段的索引示意图



从图中可以看到，满足city='杭州’条件的行，是从ID_X到ID_(X+N)的这些记录。

通常情况下，这个语句执行流程如下所示 ：（聚簇索引+非聚簇索引）

1. 初始化sort_buffer，确定放入name、city、age这三个字段；

2. 从索引city找到第一个满足city='杭州’条件的主键id，也就是图中的ID_X；

3. 到主键id索引取出整行，取name、city、age三个字段的值，存入sort_buffer中；

4. 从索引city取下一个记录的主键id；

5. 重复步骤3、4直到city的值不满足查询条件为止，对应的主键id也就是图中的ID_Y；

6. 对sort_buffer中的数据按照字段name做快速排序；

7. 按照排序结果取前1000行返回给客户端。

我们暂且把这个排序过程，称为**全字段排序**，执行流程的示意图如下所示，下一篇文章中我们还会用到这个排序。

<img src="MYSQL实战45讲.assets/image-20240426192338252.png" alt="image-20240426192338252" style="zoom:50%;" />

​                                                                                                                             图3 全字段排序

图中“按name排序”这个动作，**可能在内存中完成，也可能需要使用外部排序**，这取决于排序所需的**内存和参数sort_buffer_size**。

sort_buffer_size，就是MySQL为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序

你可以用下面介绍的方法，来确定一个排序语句是否使用了临时文件。

```sql
/* 打开optimizer_trace，只对本线程有效 */
SET optimizer_trace='enabled=on';
/* @a保存Innodb_rows_read的初始值 */
select VARIABLE_VALUE into @a from performance_schema.session_status where variable_name = 'Innodb_rows_read';
/* 执行语句 */
select city, name,age from t where city='杭州' order by name limit 1000;
/* 查看 OPTIMIZER_TRACE 输出 */
SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G
/* @b保存Innodb_rows_read的当前值 */
select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read';
/* 计算Innodb_rows_read差值 */
select @b-@a;
```

这个方法是通过查看 OPTIMIZER_TRACE 的结果来确认的，你可以从 **number_of_tmp_files**中看到是否使用了临时文件

![image-20240426192804478](MYSQL实战45讲.assets/image-20240426192804478.png)

​                                                                                    图4 全排序的OPTIMIZER_TRACE部分结果

number_of_tmp_files表示的是，排序过程中使用的临时文件数。你一定奇怪，为什么需要12个文件？内存放不下时，就需要使用外部排序，外部排序一般使用**归并排序算法**。

示例表中有4000条满足city='杭州’的记录，所以你可以看到 examined_rows=4000，表示参与排序的行数是4000行。

sort_mode 里面的packed_additional_fields的意思是，排序过程对字符串做了“紧凑”处理。即使name字段的定义是varchar(16)，在排序过程中还是要按照实际长度来分配空间的。

同时，最后一个查询语句select @b-@a 的返回结果是4000，表示整个执行过程只扫描了4000行。

这里需要注意的是，为了避免对结论造成干扰，我把internal_tmp_disk_storage_engine设置成MyISAM。否则，select @b-@a的结果会显示为4001。这是因为查询OPTIMIZER_TRACE这个表时，需要用到临时表，而**internal_tmp_disk_storage_engine**的默认值是InnoDB。如果使用的是InnoDB引擎的话，把数据从临时表取出来的时候，会让Innodb_rows_read的值加1。



## **rowId**排序排

在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在sort_buffer和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么sort_buffer里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。所以如果单行很大，这个方法效率不够好。

接下来，我来修改一个参数，让MySQL采用另外一种算法

```sql
SET max_length_for_sort_data = 16;
```

max_length_for_sort_data，是MySQL中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL就认为单行太大，要换一个算法。city、name、age 这三个字段的定义总长度是36，我把max_length_for_sort_data设置为16，我们再来看看计算过程有什么改变。

新的算法放入sort_buffer的字段，只有要排序的列（即name字段）和主键id

但这时，排序的结果就因为少了city和age字段的值，不能直接返回了，整个执行流程就变成如下所示的样子：

1. 初始化sort_buffer，确定放入两个字段，即name和id；

2. 从索引city找到第一个满足city='杭州’条件的主键id，也就是图中的ID_X；

3. 到主键id索引取出整行，取name、id这两个字段，存入sort_buffer中；

4. 从索引city取下一个记录的主键id；

5. 重复步骤3、4直到不满足city='杭州’条件为止，也就是图中的ID_Y；

6. 对sort_buffer中的数据按照字段name进行排序；

7. 遍历排序结果，取前1000行，并按照id的值回到原表中取出city、name和age三个字段返回给客户端。（**多了一次回表查询**）

这个执行流程的示意图如下，我把它称为rowid排序。

![image-20240426194011936](MYSQL实战45讲.assets/image-20240426194011936.png)

​                                                                                        图5 rowid排序

根据这个说明过程和图示，你可以想一下，这个时候执行select @b-@a，结果会是多少呢？

现在，我们就来看看结果有什么不同。

首先，图中的examined_rows的值还是4000，表示用于排序的数据是4000行。但是select @b-@a这个语句的值变成5000了。

因为这时候除了排序过程外，在排序完成后，还要根据id去原表取值。由于语句是limit 1000，因此会多读1000行。

<img src="MYSQL实战45讲.assets/image-20240426194738764.png" alt="image-20240426194738764" style="zoom:50%;" />

​                                                                                        图6 rowid排序的OPTIMIZER_TRACE部分输出

从OPTIMIZER_TRACE的结果中，你还能看到另外两个信息也变了。sort_mode变成了<sort_key, rowid>，表示参与排序的只有name和id这两个字段。

number_of_tmp_files变成10了，是因为这时候参与排序的行数虽然仍然是4000行，但是每一行都变小了，因此需要排序的总数据量就变小了，需要的临时文件也相应地变少了



## 全字段排序 VS rowid排序

对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。

这体现了**MySQL的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。**

看到这里，你就了解了，MySQL做排序是一个成本比较高的操作。那么你会问，是不是所有的order by都需要排序操作呢？如果不排序就能得到正确的结果，那对系统的消耗会小很多，语句的执行时间也会变得更短。

你可以设想下，如果能够保证从city这个索引上取出来的行，天然就是按照name递增排序的话，是不是就可以不用再排序了呢？确实是这样的。

所以，我们可以在这个市民表上创建一个city和name的联合索引，对应的SQL语句是：

```sql
alter table t add index city_user(city, name);
```

作为与city索引的对比，我们来看看这个索引的示意图。

<img src="MYSQL实战45讲.assets/image-20240426195330323.png" alt="image-20240426195330323" style="zoom:50%;" />

​                                                                                          图7 city和name联合索引示意图

在这个索引里面，我们依然可以用树搜索的方式定位到第一个满足city='杭州’的记录，并且额外确保了，接下来按顺序取“下一条记录”的遍历过程中，只要city的值是杭州，name的值就一定是有序的。

这样整个查询过程的流程就变成了：

1. 从索引(city,name)找到第一个满足city='杭州’条件的主键id；

2. 到主键id索引取出整行，取name、city、age三个字段的值，作为结果集的一部分直接返

回；

3. 从索引(city,name)取下一个记录主键id；
4.  重复步骤2、3，直到查到第1000条记录，或者是不满足city='杭州’条件时循环结束。

<img src="MYSQL实战45讲.assets/image-20240426195530556.png" alt="image-20240426195530556" style="zoom:50%;" />

​                                                                    图8 引入(city,name)联合索引后，查询语句的执行计划

可以看到，这个查询过程不需要临时表，也不需要排序。接下来，我们用explain的结果来印证一下。

![image-20240426195601594](MYSQL实战45讲.assets/image-20240426195601594.png)

​                                                         图9 引入(city,name)联合索引后，查询语句的执行计划

从图中可以看到，Extra字段中没有Using filesort了，也就是不需要排序了。而且由于(city,name)这个联合索引本身有序，所以这个查询也不用把4000行全都读一遍，只要找到满足条件的前1000条记录就可以退出了。也就是说，在我们这个例子里，只需要扫描1000次。

進一步簡化：覆盖索引。指索引上的信息足够满足查询请求，不需要再 覆回到主键索引上去取数据。

按照覆盖索引的概念，我们可以再优化一下这个查询语句的执行流程。

针对这个查询，我们可以创建一个city、name和age的联合索引，对应的SQL语句就是：

```sql
alter table t add index city_user_age(city, name, age);
```

这时，对于city字段的值相同的行来说，还是按照name字段的值递增排序的，此时的查询语句也

就不再需要排序了。这样整个查询语句的执行流程就变成了：

1. 从索引(city,name,age)找到第一个满足city='杭州’条件的记录，取出其中的city、name和age

这三个字段的值，作为结果集的一部分直接返回；

2. 从索引(city,name,age)取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接

返回；

3. 重复执行步骤2，直到查到第1000条记录，或者是不满足city='杭州’条件时循环结束。

<img src="MYSQL实战45讲.assets/image-20240426195944226.png" alt="image-20240426195944226" style="zoom:50%;" />

​                                                                           图10 引入(city,name,age)联合索引后，查询语句的执行流程

然后，我们再来看看explain的结果：

![image-20240426200021609](MYSQL实战45讲.assets/image-20240426200021609.png)

​                                                                                   图11 引入(city,name,age)联合索引后，查询语句的执行计划

可以看到，Extra字段里面多了“Using index”，表示的就是使用了覆盖索引，性能上会快很多。

当然，这里并不是说要为了每个查询能用上覆盖索引，就要把语句中涉及的字段都建上联合索引，毕竟索引还是有维护代价的。这是一个需要权衡的决定。



## 小结

今天这篇文章，我和你介绍了MySQL里面order by语句的几种算法流程。

在开发系统的时候，你总是不可避免地会使用到order by语句。你心里要清楚每个语句的排序逻辑是怎么实现的，还要能够分析出在最坏情况下，每个语句的执行对系统资源的消耗，这样才能做到下笔如有神，不犯低级错误。

## 思考题

假设你的表里面已经有了city_name(city, name)这个联合索引，然后你要查杭州和苏州两个城市中所有的市民的姓名，并且按名字排序，显示前100条记录。如果SQL查询语句是这么写的 ：

```
mysql> select * from t where city in ('杭州',"苏州") order by name limit 100;
```

那么，这个语句执行的时候会有排序过程吗，为什么？

如果业务端代码由你来开发，需要实现一个在数据库端不需要排序的方案，你会怎么实现呢？

进一步地，如果有分页需求，要显示第101页，也就是说语句最后要改成 “limit 10000,100”， 你的实现方法又会是什么呢？





# 22 | MySQL有哪些“饮鸩止渴”提高性能的方法？

不知道你在实际运维过程中有没有碰到这样的情景：业务高峰期，生产环境的MySQL压力太 大，没法正常响应，需要短期内、临时性地提升一些性能。 我以前做业务护航的时候，就偶尔会碰上这种场景。用户的开发负责人说，不管你用什么方案， 让业务先跑起来再说。 但，如果是无损方案的话，肯定不需要等到这个时候才上场。今天我们就来聊聊这些临时方案， 并着重说一说它们可能存在的风险。



## 短连接风暴

 正常的短连接模式就是连接到数据库后，执行很少的SQL语句就断开，下次需要的时候再重连。 如果使用的是短连接，在业务高峰期的时候，就可能出现连接数突然暴涨的情况。 



我在第1篇文章《基础架构：一条SQL查询语句是如何执行的？》中说过，

MySQL建立连接的过 程，成本是很高的。除了正常的网络连接三次握手外，还需要做登录权限判断和获得这个连接的 数据读写权限。 在数据库压力比较小的时候，这些额外的成本并不明显。 但是，短连接模型存在一个风险，就是一旦数据库处理得慢一些，连接数就会暴涨。

**max_connections参数**，用来控制一个MySQL实例同时存在的连接数的上限，超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。对于被拒绝连接的请求来 说，从业务角度看就是数据库不可用。



**在机器负载比较高的时候**，处理现有请求的时间变长，每个连接保持的时间也更长。这时，再有 新建连接的话，就可能会超过max_connections的限制。



碰到这种情况时，一个比较自然的想法，就是调高max_connections的值。但这样做是有风险 的。因为设计max_connections这个参数的目的是想保护MySQL，如果我们把它改得太大，让更 多的连接都可以进来，那么系统的负载可能会进一步加大，大量的资源耗费在权限验证等逻辑 上，结果可能是适得其反，已经连接的线程拿不到CPU资源去执行业务的SQL请求。



### 两种有损方法

 那么这种情况下，你还有没有别的建议呢？我这里还有两种方法，但要注意，这些方法都是**有损的**。



**第一种方法：先处理掉那些占着连接但是不工作的线程**。

 max_connections的计算，不是看谁在running，是只要连着就占用一个计数位置。对于那些不需 要保持的连接，我们可以通过kill connection主动踢掉。

这个行为跟事先设置wait_timeout的效果 是一样的。设置wait_timeout参数表示的是，一个线程空闲wait_timeout这么多秒之后，就会被 MySQL直接断开连接。



但是需要注意，在`show processlist`的结果里，踢掉显示为sleep的线程，可能是有损的。我们来 看下面这个例子。

![image-20241225203005539](MYSQL实战45讲.assets/image-20241225203005539.png)

在上面这个例子里，如果断开session A的连接，因为这时候session A还没有提交，所以MySQL 只能按照回滚事务来处理；而断开session B的连接，就没什么大影响。所以，如果按照优先级 来说，你应该优先断开像session B这样的事务外空闲的连接。(先断开读操作的sleep进程)



但是，怎么判断哪些是事务外空闲的呢？session C在T时刻之后的30秒执行showprocesslist， 看到的结果是这样的。

<img src="MYSQL实战45讲.assets/image-20241225203342065.png" alt="image-20241225203342065" style="zoom: 67%;" />



图中id=4和id=5的两个会话都是Sleep 状态。而要看事务具体状态的话，你可以查 information_schema库的innodb_trx表。

```sql
select * from information_schema.innodb_trx \G;
```

<img src="MYSQL实战45讲.assets/image-20241225203421367.png" alt="image-20241225203421367" style="zoom:50%;" />



这个结果里，trx_mysql_thread_id=4，表示id=4的线程还处在事务中。 因此，如果是连接数过多，你可以**优先断开事务外空闲太久的连接**；如果这样还不够，再考虑断 开事务内空闲太久的连接。



从服务端断开连接使用的是`KILL <connection_id>;` 的命令， 一个客户端处于sleep状态时，它的连接 被服务端主动断开后，这个客户端并不会马上知道。直到客户端在发起下一个请求的时候，才会 收到这样的报错“ERROR 2013 (HY000): Lost connection to MySQL server during query”。



从数据库端主动断开连接可能是有损的，尤其是有的应用端收到这个错误后，不重新连接，而是 直接用这个已经不能用的句柄重试查询。这会导致从应用端看上去，“MySQL一直没恢复”。 你可能觉得这是一个冷笑话，但实际上我碰到过不下10次。 所以，如果你是一个支持业务的DBA，不要假设所有的应用代码都会被正确地处理。即使只是 一个断开连接的操作，也要确保通知到业务开发团队。



**第二种方法：减少连接过程的消耗。** 



有的业务代码会在短时间内先大量申请数据库连接做备用，如果现在数据库确认是被连接行为打 挂了，那么一种可能的做法，是让数据库跳过权限验证阶段。

 跳过权限验证的方法是：重启数据库，并使用**–skip-grant-tables参数**启动。

这样，整个MySQL会 跳过所有的权限验证阶段，包括连接过程和语句执行过程在内。

 但是，这种方法特别符合我们标题里说的“饮鸩止渴”，风险极高，是我特别不建议使用的方案。

 尤其你的库外网可访问的话，就更不能这么做了。

 **在MySQL 8.0版本里，如果你启用–skip-grant-tables参数，MySQL会默认把 --skip-networking参 数打开，表示这时候数据库只能被本地的客户端连接**。

可见，MySQL官方对skip-grant-tables这 个参数的安全问题也很重视。 除了短连接数暴增可能会带来性能问题外，实际上，我们在线上碰到更多的是查询或者更新语句 导致的性能问题。其中，查询问题比较典型的有两类，一类是由新出现的慢查询导致的，一类是 由QPS（每秒查询数）突增导致的。而关于更新语句导致的性能问题，我会在下一篇文章和你展 开说明。



## 慢查询性能问题 



在MySQL中，会引发性能问题的慢查询，大体有以下三种可能： 1. 索引没有设计好； 2. SQL语句没写好； 3. MySQL选错了索引。

接下来，我们就具体分析一下这三种可能，以及对应的解决方案。



导致慢查询的第一种可能是，**索引没有设计好**。

这种场景一般就是通过紧急创建索引来解决。MySQL 5.6版本以后，创建索引都支持Online DDL 了，对于那种高峰期数据库已经被这个语句打挂了的情况，最高效的做法就是直接执行alter table 语句



比较理想的是能够在备库先执行。假设你现在的服务是一主一备，主库A、备库B，这个方案的 大致流程是这样的：

1. 在备库B上执行 `set sql_log_bin=off`，也就是不写binlog，然后执行alter table 语句加上索 引；
2. 执行主备切换； 
3. 这时候主库是B，备库是A。在A上执行 set sql_log_bin=off，然后执行alter table 语句加上 索引。



这是一个“古老”的DDL方案。平时在做变更的时候，你应该考虑类似gh-ost这样的方案，更加稳 妥。但是在需要紧急处理时，上面这个方案的效率是最高的。



导致慢查询的第二种可能是，**语句没写好**。 



比如，我们犯了在第18篇文章《为什么这些SQL语句逻辑相同，性能却差异巨大？》中提到的 那些错误，导致语句没有使用上索引。

这时，我们可以通过改写SQL语句来处理。

MySQL 5.7提供了**query_rewrite功能**，可以把输入的 一种语句改写成另外一种模式。 比如，语句被错误地写成了 `select *from t where id + 1 = 10000`，你可以通过下面的方式，增加 一个语句改写规则。

```sql
mysql> insert into query_rewrite.rewrite_rules(pattern, replacement, pattern_database) values ("select * from t where id + 1 = ?", "select * from t where id = ? - 1", "db1");

call query_rewrite.flush_rewrite_rules()
```

这里，call query_rewrite.flush_rewrite_rules()这个存储过程，是让插入的新规则生效，也就是我 们说的“查询重写”。你可以用图4中的方法来确认改写规则是否生效。



<img src="MYSQL实战45讲.assets/image-20241225204639614.png" alt="image-20241225204639614" style="zoom: 67%;" />



导致慢查询的第三种可能，就是碰上了我们在第10篇文章《MySQL为什么有时候会选错索 引？》中提到的情况，**MySQL选错了索引**。 

这时候，应急方案就是给这个语句加上force index。



这时候，应急方案就是给这个语句加上force index。 同样地，使用查询重写功能，给原来的语句加上force index，也可以解决这个问题。



上面我和你讨论的由慢查询导致性能问题的三种可能情况，实际上出现最多的是前两种，即：索 引没设计好和语句没写好。而这两种情况，恰恰是完全可以避免的。比如，通过下面这个过程， 我们就可以预先发现问题。

1. 上线前，在测试环境，把**慢查询日志（slowlog）**打开，并且把long_query_time设置成0， 确保每个语句都会被记录入慢查询日志；
2. 在测试表里插入模拟线上的数据，做一遍回归测试；

```yaml
#my.cnf
slow_query_log = 1
slow_query_log_file = /path/to/slow_query.log
long_query_time = 0 ## If a query takes longer than this many seconds, the server increments the Slow_queries status variable
```

3. 观察慢查询日志里每类语句的输出，特别留意Rows_examined字段是否与预期一致。（我 们在前面文章中已经多次用到过Rows_examined方法了，相信你已经动手尝试过了。如果 还有不明白的，欢迎给我留言，我们一起讨论）。



不要吝啬这段花在上线前的“额外”时间，因为这会帮你省下很多故障复盘的时间。 如果新增的SQL语句不多，手动跑一下就可以。而如果是新项目的话，或者是修改了原有项目的 表结构设计，**全量回归测试**都是必要的。这时候，你需要工具帮你检查所有的SQL语句的返回结 果。比如，你可以使用开源工具pt-query-digest(https://www.percona.com/doc/perconatoolkit/3.0/pt-query-digest.html)。



## QPS突增问题

有时候由于业务突然出现高峰，或者应用程序bug，导致某个语句的QPS突然暴涨，也可能导致 MySQL压力过大，影响服务。

我之前碰到过一类情况，是由一个新功能的bug导致的。当然，最理想的情况是让业务把这个功能下掉，服务自然就会恢复。

而下掉一个功能，如果从数据库端处理的话，对应于不同的背景，有不同的方法可用。我这里再 和你展开说明一下。

1. 一种是由全新业务的bug导致的。假设你的DB运维是比较规范的，也就是说白名单是一个个 加的。这种情况下，如果你能够确定业务方会下掉这个功能，只是时间上没那么快，那么就 可以从数据库端直接把白名单去掉。
2. 如果这个新功能使用的是单独的数据库用户，可以用管理员账号把这个用户删掉，然后断开 现有连接。这样，这个新功能的连接不成功，由它引发的QPS就会变成0。

3. 如果这个新增的功能跟主体功能是部署在一起的，那么我们只能通过处理语句来限制。这 时，我们可以使用上面提到的查询重写功能，**把压力最大的SQL语句直接重写成"select 1"返回**。 当然，这个操作的风险很高，需要你特别细致。它可能存在两个副作用：
   1. 如果别的功能里面也用到了这个SQL语句模板，会有误伤； 
   2. 很多业务并不是靠这一个语句就能完成逻辑的，所以如果单独把这一个语句以select 1的结 果返回的话，可能会导致后面的业务逻辑一起失败。



所以，方案3是用于止血的，跟前面提到的去掉权限验证一样，应该是你所有选项里优先级最低 的一个方案。 同时你会发现，其实方案1和2都要依赖于规范的运维体系：**虚拟化、白名单机制、业务账号分离**。由此可见，更多的准备，往往意味着更稳定的系统。



## 小结

今天这篇文章，我以业务高峰期的性能问题为背景，和你介绍了一些紧急处理的手段。 

这些处理手段中，既包括了粗暴地拒绝连接和断开连接，也有通过重写语句来绕过一些坑的方法；

既有临时的高危方案，也有未雨绸缪的、相对安全的预案。 

在实际开发中，我们也要尽量避免一些低效的方法，比如避免大量地使用短连接。

同时，如果你做业务开发的话，要知道，**连接异常断开是常有的事，你的代码里要有正确地重连并重试的机制**。 

DBA虽然可以通过语句重写来暂时处理问题，但是这本身是一个风险高的操作，做好SQL审计 可以减少需要这类操作的机会。 

其实，你可以看得出来，在这篇文章中我提到的解决方法主要集中在server层。

在下一篇文章 中，我会继续和你讨论一些跟InnoDB有关的处理方法。 

最后，又到了我们的思考题时间了。

今天，我留给你的课后问题是，你是否碰到过，在业务高峰期需要临时救火的场景？你又是怎么 处理的呢？



# 其他

### 精选评论







