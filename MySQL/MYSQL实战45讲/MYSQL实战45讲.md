MySQL 45讲

# 0 课程介绍

> 极客时间  MySQL 实战 45 讲 https://time.geekbang.org/column/intro/100020801

## 你将获得

- 前腾讯云数据库负责人独家经验分享；
- MySQL 核心技术详解与原理说明；
- 36 个 MySQL 常见痛点问题解析；
- 完整的 MySQL 学习路径。

## 课程介绍

MySQL 使用和面试中遇到的问题，很多人会通过搜索别人的经验来解决 ，零散不成体系。实际上只要理解了 MySQL 的底层工作原理，就能很快地直戳问题的本质。

本专栏一共 **45** 讲，**全集**更新完毕，通过探讨 MySQL 实战中最常见的 **36 个** 痛点问题，串起各个零散的知识点，配合 **100+** 手绘详解图，由线到面带你构建 MySQL 系统的学习路径。

在专栏中，丁奇结合自己十多年来，通过 MySQL 源码找答案的经历，梳理了 **MySQL 的主线知识**，比如**事务、索引、锁**等；并基于这条主线，带你缕清**概念、机制、原理、案例分析**以及本质，让你真正能掌握 **MySQL 核心技术与底层原理。**

值得一提的是，每篇文章中都附有**实践案例**，给你从理论到实战的系统性指导，让你少走弯路，彻底搞懂 MySQL。

本课程共包括两大模块。

**模块一，基础篇**。为你深入浅出地讲述 MySQL 核心知识，涵盖 MySQL 基础架构、日志系统、事务隔离、锁等内容。

**模块二，实践篇**。将从一个个关键的数据库问题出发，分析数据库原理，并给出实践指导。每个问题，都不只是简单地给出答案，而是从为什么要这么想、到底该怎样做出发，让你能够知其所以然，都将能够解决你平时工作中的一个疑惑点。

## 课程目录

<img src="MYSQL实战45讲.assets/b736f37014d28199c2457a67ed669bc2.jpg" alt="img" style="zoom:33%;" />





# 00 安装mysql

> 详见：https://www.runoob.com/mysql/mysql-install.html

```bash
# 下载
wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm
rpm -ivh mysql-community-release-el7-5.noarch.rpm
yum update
yum install mysql-server
```

权限设置：

```
chown -R mysql:mysql /var/lib/mysql/
```

初始化 MySQL：

```
mysqld --initialize
```

启动 MySQL：

```
systemctl start mysqld
```

查看 MySQL 运行状态：

```
systemctl status mysqld
```



查看初始密码：

```
grep password /var/log/mysql/mysqld.log 
2024-05-14T11:48:54.670531Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: 3d:Aw9u?-Tp!
```



登录后修改密码：

```bash
mysql -u root -p 3d:Aw9u?-Tp!
#修改密码
alter user 'root'@'localhost' identified by 'xxxxxx密码'
```



# 01 | 基础架构：一条SQL查询语句是如何执行的？

> https://juejin.cn/post/7130258357184626724 这篇文章更详细

下面是MySQL的基本架构示意图，从中你可以清楚地看到SQL语句在MySQL的各个功能模块中的执行过程。

<img src="MYSQL实战45讲.assets/image-20240428175938023.png" alt="image-20240428175938023" style="zoom:50%;" />

<center>MySQL的逻辑架构图</center>

​		大体来说，MySQL可以分为Server层和存储引擎层两部分。

-  Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。
- 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从**MySQL 5.5.5**版本开始成为了默认存储引擎。

 以1条SQL语句，带你走一遍整个执行流程，依次看下每个组件的作用。       

```sql
mysql> select * from T where ID=10；
```

![未命名绘图.png](MYSQL实战45讲.assets/73b4a36d35c3439694d0a29c20b880d9tplv-k3u1fbpfcp-zoom-in-crop-mark1512000-17143044539905.webp)

## 连接器

第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：

```sql
mysql -h $ip -P $port -u $user -p
```

连接命令中的mysql是客户端工具，用来跟服务端建立连接。在完成经典的TCP握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。

- 如果用户名或密码不对，你就会收到一个"Access denied for user"的错误，然后客户端程序结束执行。
- 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限

这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。**修改完成后，只有再新建的连接才会使用新的权限设置**。  

连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在**showprocesslist命令**中看到它。文本中这个图是showprocesslist的结果，其中的Command列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。

​                                                       <img src="MYSQL实战45讲.assets/image-20240428193357383.png" alt="image-20240428193357383" style="zoom:50%;" />

> 空闲连接会一直占用着吗？

客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数**wait_timeout（等待超时）**控制的，默认值是8小时。

```sql
mysql> show variables like 'wait_timeout';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| wait_timeout  | 28800 |
+---------------+-------+
1 row in set (0.00 sec)
```

当然，我们自己也可以手动断开空闲的连接，使用的是 kill connection + id 的命令。

```sql
mysql> kill connection +6;
Query OK, 0 rows affected (0.00 sec)
```

如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connectionto MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。

数据库里面，**长连接**是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。**短连接**则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。

建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是**尽量使用长连接**。但是全部使用长连接后，你可能会发现，有些时候MySQL占用**内存涨得特别快**，这是因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（**OOM**），从现象看就是MySQL异常重启了。

> 怎么解决长连接长期占用内存这个问题呢？你可以考虑以下两种方案。

1. 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。

2. **客户端主动重置连接**。MySQL 5.7 版本实现了 `mysql_reset_connection()` 函数的接口，注意这是接口函数不是命令，那么当客户端执行了一个很大的操作后，在代码里调用 mysql_reset_connection 函数来重置连接，达到释放内存的效果。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

   

> MySQL 的连接数有限制吗？

MySQL 服务支持的最大连接数由 **max_connections** 参数控制，比如我的 MySQL 服务默认是 151 个,超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。

```sql
mysql> show variables like 'max_connections';
+-----------------+-------+
| Variable_name   | Value |
+-----------------+-------+
| max_connections | 151   |
+-----------------+-------+
1 row in set (0.00 sec)
```



## 查询缓存

连接建立完成后，你就可以执行select语句了。执行逻辑就会来到第二步：查询缓存。

MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。

如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。

> 但是大多数情况下我会建议你不要使用查询缓存，为什么呢？

因为查询缓存往往弊大于利。**查询缓存的失效非常频繁**，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。

好在MySQL也提供了这种“按需使用”的方式。你可以将参数**query_cache_type设置成DEMAND**，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样：

```sql
mysql> select SQL_CACHE * from T where ID=10;
```

需要注意的是，MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有这个功能了。



## 分析器

如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL需要知道你要做什么，因此需要对SQL语句做解析。

1、词法分析

分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。

MySQL从你输入的"select"这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名T”，把字符串“ID”识别成“列ID”。



2、语法分析

做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。

如果你的语句不对，就会收到**“You have an error in your SQL syntax”的错误提醒**，比如下面这个语句select少打了开头的字母“s”。

```sql
mysql> elect * from t where ID=1;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'elect * from t where ID=1' at line 1
```

一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。



## 优化器 

经过了分析器，MySQL就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。

**优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序**。

比如你执行下面这样的语句，这个语句是执行两个表的join：

```sql
mysql> select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20;
```

- 既可以先从表t1里面取出c=10的记录的ID值，再根据ID值关联到表t2，再判断t2里面d的值是否等于20。
- 也可以先从表t2里面取出d=20的记录的ID值，再根据ID值关联到t1，再判断t1里面c的值是否等于10。

这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。

优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。如果你还有一些疑问，比如优化器是怎么选择索引的，有没有可能选择错等等，没关系，在后面的文章中单独展开说明优化器的内容。

（小表驱动大表会好点？）



## 执行器

MySQL通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。

开始执行的时候，要先判断一下你对这个表T有没有执行查询的**权限**，如果没有，就会返回没有权限的错误，如下所示：

```sql
mysql> select * from T where ID=10;
ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T'
```

如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。

比如我们这个例子中的表T中，ID字段没有索引，那么执行器的执行流程是这样的（全表查询）：

1. 调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则

将这行存在结果集中；

2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。

3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

至此，这个语句就执行完成了，

对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。



你会在数据库的慢查询日志中看到一个**rows_examined的字段，表示这个语句执行过程中扫描了多少行**。这个值就是在执行器每次调用引擎获取数据行的时候累加的。

在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟rows_examined 并不是完全相同的。 后面会专门有一篇文章来讲存储引擎的内部机制，里面会有详细的说明。



## 思考题

如果表T中没有字段k，而你执行了这个语句 select *fromTwhere k=1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢？

（分析器？

查询解析阶段：在这个阶段，MySQL 解析查询语句的语法和语义，检查语句的正确性，并生成相应的查询执行计划。在这个阶段，MySQL 会检查表和列是否存在，但不会检查列的值）



# 02 | 日志系统：一条SQL更新语句是如何执行的？

一条更新语句的执行流程又是怎样的呢？

**之前你可能经常听DBA同事说，MySQL可以恢复到半个月内任意一秒的状态**，惊叹的同时，你是不是心中也会不免会好奇，这是怎样做到的呢？

我们还是从一个表的一条更新语句说起，下面是这个表的创建语句，这个表有一个主键ID和一个整型字段c：

```sql
mysql> create table T(ID int primary key, c int);
```

如果要将ID=2这一行的值加1，SQL语句就会这么写：

```sql
mysql> update T set c=c+1 where ID=2;
```

<img src="MYSQL实战45讲.assets/image-20240429180703572.png" alt="image-20240429180703572" style="zoom:50%;" />

还是看这个架构图。

你执行语句前要先连接数据库，这是连接器的工作。

前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表T上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。

接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用ID这个索引。然后，执行器负责具体执行，找到这一行，然后更新。

**与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）**。



## **redo log**（重做日志）

在MySQL里，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。

为了解决这个问题，MySQL的设计者就用了类似酒店掌柜粉板（临时记录，最后再一起核算）的思路来提升更新效率。

而粉板和账本配合的整个过程，其实就是MySQL里经常说到的WAL技术，WAL的全称是**Write Ahead Logging（写前日志）**，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。



具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里面，**并更新内存**，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。



InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么这块“粉板”总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。

<img src="MYSQL实战45讲.assets/image-20240429190635968.png" alt="image-20240429190635968" style="zoom:50%;" />

**write pos**是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。**checkpoint**是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。

write pos和checkpoint之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。（双指针设计得很合理）

有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为**crash-safe**（奔溃安全）。

**innodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。**这个参数我建议你设置成1，这样可以保证MySQL异常重启之后数据不丢失。



这个Redo Log 是在磁盘上存储的，而不是在内存中。

Redo Log 记录了对数据库进行的修改操作，例如插入、更新和删除操作。当事务提交时，相关的 Redo Log 记录会被写入磁盘，以确保在数据库崩溃或故障恢复时，可以通过重放 Redo Log 来恢复未完成的事务。

尽管 Redo Log 是在磁盘上存储的，但在实际操作中，MySQL 会**使用缓冲区（Buffer）来管理 Redo Log 的写入**。这是为了提高性能，避免每次修改都直接写入磁盘。MySQL 会将 Redo Log 记录先写入到内存的缓冲区中，然后根据一定的策略将缓冲区中的内容刷新到磁盘上的 Redo Log 文件中。





## **binlog（归档日志）**

MySQL整体来看，其实就有两块：一块是Server层，它主要做的是MySQL功能

层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板redo log是InnoDB引擎特有的日志，而**Server层也有自己的日志，称为binlog（归档日志）**

你肯定会问，为什么会有两份日志呢？

因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统— — 也就是redo log来实现crash-safe能力。

**sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。**这个参数我也建议你设置成1，这样可以保证MySQL异常重启之后binlog不丢失



这两种日志有以下三点不同。

1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。

2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。

3. redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。



有了对这两个日志的概念性理解，我们再来看执行器和InnoDB引擎在执行这个简单的update语句时的内部流程：

1. 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。

2. 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。

3. 引擎将这行新数据更新到内存中，同时将这个更新操作**记录到redo log里面**，此时redo log处于prepare状态。然后**告知执行器执行完成了**，随时可以提交事务。

4. 执行器生成这个操作的binlog，并把binlog写入磁盘.(异步的？)

5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。

下面是update语句的执行流程图，图中浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的。

<img src="MYSQL实战45讲.assets/image-20240429192224620.png" alt="image-20240429192224620" style="zoom:50%;" />

<center>update语句执行流程</center>

你可能注意到了，最后三步看上去有点“绕”，将redo log的写入拆成了两个步骤：prepare和commit，这就是"两阶段提交"。



## 两阶段提交

为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得

从文章开头的那个问题说起：怎么让数据库恢复到半个月内任意一秒的状态？

前面我们说过了，**binlog会记录所有的逻辑操作**，并且是采用“追加写”的形式。如**果你的DBA承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有binlog，同时系统会定期做整库备份。**这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备

当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数

据，那你可以这么做：

- 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；
- 然后，从备份的时间点开始，将备份的binlog依次取出来，重放到中午误删表之前的那个时刻。

这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。

好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。

由于redo log和binlog是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redo log再写binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。仍然用前面的update语句来做例子。假设当前ID=2的行，字段c的值是0，再假设执行update语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？

（数据以redo log为准，这个WAL写了就表示操作成功了）

**1、先写redo log在写binlog 会造成恢复数据时的数据缺失。**假设在redo log写完，binlog还没有写完的时候，MySQL进程异常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1。但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份日志的时候，存起来的binlog里面就没有这条语句。然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同

2、先写binlog在写redo log 。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是1，与原库的值不同



可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。

你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀？

其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要**扩容的时候**，也就是需要再多搭建一些备库来增加系统的读能力的时候，**现在常见的做法也是用全量备份加上应用binlog来实现的**，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。

简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。



**两阶段提交是==跨系统==维持数据逻辑一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到**。



## 思考题

定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？

> chatGPT回答

一天一备的优势：

1. 数据恢复点更近：一天一备意味着在发生故障或数据丢失时，可以恢复到最近一天的备份。**这可以减少数据丢失的程度，尤其是对于频繁发生数据变更的系统**。
2. 数据恢复时间较短：由于备份频率较高，恢复数据所需的时间相对较短。这对于需要快速恢复数据库的系统非常重要，以减少业务中断时间。

一周一备的优势：

1. 节省存储空间：一周一备相对于一天一备可以减少备份数据的存储需求。如果数据量庞大，每天进行全量备份可能会占用大量的存储空间。一周一备可以在一定程度上减少存储成本。
2. 减少备份过程对系统性能的影响：进行全量备份可能会对数据库系统的性能产生一定的影响。一周一备相对于一天一备可以减少备份过程对系统性能的影响，尤其是对于大型数据库系统。

备份策略的选择会影响数据库系统的可恢复性和可用性指标。一天一备可以提供更近的数据恢复点和较短的恢复时间，从而减少数据丢失和业务中断时间。而一周一备可以节省存储空间和减少备份过程对系统性能的影响。

需要根据具体的业务需求、数据变更频率和系统重要性来评估并选择适合的备份策略。有些系统可能需要更频繁的备份以确保数据的安全性和可恢复性，而对于其他系统，一周一备可能已经足够满足需求。

> 作者回答

好处是“最长恢复时间”更短。

在一天一备的模式里，最坏情况下需要应用一天的binlog。比如，你每天0点做一次全量备份，

而要恢复出一个到昨天晚上23点的备份。

一周一备最坏情况就要应用一周的binlog了。

系统的对应指标就是 RTO（**Recovery Time Objective**，恢复时间目标），具体见[这里](https://www.dell.com/community/zh/conversations/%E6%95%B0%E6%8D%AE%E4%BF%9D%E6%8A%A4%E8%AE%A8%E8%AE%BA%E5%8C%BA/%E6%81%A2%E5%A4%8D%E6%97%B6%E9%97%B4%E7%9B%AE%E6%A0%87rto%E5%92%8C%E6%81%A2%E5%A4%8D%E7%82%B9%E7%9B%AE%E6%A0%87rpo/647f86edf4ccf8a8de60d31e)。

当然这个是有成本的，因为更频繁全量备份需要消耗更多存储空间，所以这个RTO是成本换来

的，就需要你根据业务重要性来评估了





# 03 | 事务隔离：为什么你改了我还看不见？

事务就是要保证一组数据库操作，要么全部成功，要么全部失败（原子性）。在MySQL中，**事务支持是在引擎层实现的**。

道，MySQL是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如M**ySQL原生的MyISAM引擎就不支持事务，这也是MyISAM被InnoDB取代**

**的重要原因之一**。

今天的文章以InnoDB为例，剖析MySQL在事务支持方面的特定实现，并基于原理给出相应的实践建议，希望这些案例能加深你对MySQL事务原理的理解



## 隔离性与隔离级别

提到事务，你肯定会想到ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中I，也就是“隔离性”。

当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、**幻读（phantomread）**的问题，为了解决这些问题，就有了“隔离级别”的概念。



隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。SQL标准的事务隔离级别包括：读未提交（read uncommitted）、

读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。下面逐一为你解释：

- 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。
- 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。
- 可重复读是指，**一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的**。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
- 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。



其中“读提交”和“可重复读”比较难理解，下面一个例子说明这几种隔离级别。假设数据表T中只有一列，其中一行的值为1，下面是按照时间顺序执行两个事务的行为。

```bash
mysql> create table T(c int) engine=InnoDB;
insert into T(c) values(1);
```

<img src="MYSQL实战45讲.assets/image-20240511174838564.png" alt="image-20240511174838564" style="zoom:50%;" />



在不同的隔离级别下，事务A会有哪些不同的返回结果，也就是图里面V1、V2、V3的返回值分别是什么。

- 若隔离级别是“读未提交”， 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被A看到了。因此，V2、V3也都是2。
- 若隔离级别是“读提交”，则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以，V3的值也是2。
- 若隔离级别是“可重复读”，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。
- **若隔离级别是“串行化”，则在事务B执行“将1改成2”的时候，会被锁住**。直到事务A提交后，事务B才可以继续执行。所以从A的角度看， V1、V2值是1，V3的值是2。

**在实现上，数据库里面会创建一个视图**，访问的时候以视图的逻辑结果为准。

- 在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。
- 在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的
- “读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；
- 而“串行化”隔离级别下直接用加锁的方式来避免并行访问

在不同的隔离级别下，数据库行为是有所不同的。Oracle数据库的默认隔离级别其实就是“读提交”，因此对于一些从Oracle迁移到MySQL的应用，为保证数据库隔离级别的一致，你一定要记得将MySQL的隔离级别设置为“读提交”。

配置的方式是，将启动参数transaction-isolation的值设置成READ-COMMITTED。你可以用showvariables来查看当前的值

```bash
mysql> show variables like 'transaction_isolation';
+-----------------------+----------------+
| Variable_name | Value |
+-----------------------+----------------+
| transaction_isolation | READ-COMMITTED |
+-----------------------+----------------+
```



总结来说，存在即合理，哪个隔离级别都有它自己的使用场景，你要根据自己的业务情况来定。

什么情况下会用到“可重复读“的场景？

假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。这时候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。



## 事务隔离的实现

理解了事务的隔离级别，我们再来看看事务隔离具体是怎么实现的。这里我们展开说明“可重复读”。

在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

假设一个值从1被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录。

<img src="MYSQL实战45讲.assets/image-20240511175531780.png" alt="image-20240511175531780" style="zoom:50%;" />

当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图中看到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于read-viewA，要得到1，就必须将当前值依次执行图中所有的回滚操作得到。



同时你会发现，即使现在有另外一个事务正在将4改成5，这个事务跟read-viewA、B、C对应的事务是不会冲突的。

你一定会问，回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，**系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除**。

什么时候才不需要了呢？**就是当系统里没有比这个回滚日志更早的read-view的时候**。



基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。

1、**长事务意味着系统里面会存在很老的事务视图**。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。

**在MySQL 5.5及以前的版本，回滚日志是跟数据字典一起放在ibdata文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小**。我见过数据只有20GB，而回滚段有200GB的库。最终只好为了清理回滚段，重建整个库。

2、除了对回滚段的影响，**长事务还占用锁资源**，也可能拖垮整个库，这个我们会在后面讲锁的时候展开。



## 事务的启动方式

如前面所述，长事务有这些潜在风险，我当然是建议你尽量避免。其实很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。MySQL的事务启动方式有以下几种：

1. **显式启动事务语句**， begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback。

2. **set autocommit=0**，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行commit 或 rollback 语句，或者断开连接。

有些客户端连接框架会默认连接成功后先执行一个set autocommit=0的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。

因此，建议你总是使用set autocommit=1, 并通过显式语句的方式来启动事务。

PS: 客户端请求时总会新建一个事务， autocommit只是设置了是否自动提交事务。且当begin显式开启事务时，只有commit才会提交事务（无关autocommit设置）



但是有的开发同学会**纠结“多一次交互”的问题**。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾虑，**建议你使用commit work and chain语法**。

在autocommit为1的情况下，用begin显式启动的事务，如果执行commit则提交事务。**如果执行commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语句的开销**。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。



你可以在information_schema库的innodb_trx这个表中查询长事务，比如下面这个语句，用于查找持续时间超过60s的事务。

```mysql
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60;
```

PS：事务表表结构

```mysql
mysql> describe information_schema.innodb_trx;
+----------------------------+---------------------+------+-----+---------------------+-------+
| Field                      | Type                | Null | Key | Default             | Extra |
+----------------------------+---------------------+------+-----+---------------------+-------+
| trx_id                     | varchar(18)         | NO   |     |                     |       |
| trx_state                  | varchar(13)         | NO   |     |                     |       |
| trx_started                | datetime            | NO   |     | 0000-00-00 00:00:00 |       |
| trx_requested_lock_id      | varchar(81)         | YES  |     | NULL                |       |
| trx_wait_started           | datetime            | YES  |     | NULL                |       |
| trx_weight                 | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_mysql_thread_id        | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_query                  | varchar(1024)       | YES  |     | NULL                |       |
| trx_operation_state        | varchar(64)         | YES  |     | NULL                |       |
| trx_tables_in_use          | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_tables_locked          | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_lock_structs           | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_lock_memory_bytes      | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_rows_locked            | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_rows_modified          | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_concurrency_tickets    | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_isolation_level        | varchar(16)         | NO   |     |                     |       |
| trx_unique_checks          | int(1)              | NO   |     | 0                   |       |
| trx_foreign_key_checks     | int(1)              | NO   |     | 0                   |       |
| trx_last_foreign_key_error | varchar(256)        | YES  |     | NULL                |       |
| trx_adaptive_hash_latched  | int(1)              | NO   |     | 0                   |       |
| trx_adaptive_hash_timeout  | bigint(21) unsigned | NO   |     | 0                   |       |
| trx_is_read_only           | int(1)              | NO   |     | 0                   |       |
| trx_autocommit_non_locking | int(1)              | NO   |     | 0                   |       |
+----------------------------+---------------------+------+-----+---------------------+-------+
24 rows in set (0.00 sec)
```

## 思考题

你现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？

> chatGPT回答

作为业务开发负责人和数据库负责人，以下是一些方案来避免或处理长事务的情况：

1. 优化查询和事务逻辑：通过优化查询语句和事务逻辑，可以减少事务的执行时间。这包括使用合适的索引、避免全表扫描、减少不必要的查询和锁定等。定期进行性能优化和代码审查，确保查询和事务逻辑的效率和合理性。
2. 分批处理和批量提交：对于大批量的数据操作，可以将其分批处理，避免一次性处理过多的数据。同时，可以使用批量提交的方式，将多个操作合并为一个事务，减少事务的数量和执行时间。
3. 合理设置事务超时时间：设置适当的事务超时时间，避免长时间占用数据库资源。如果事务执行时间超过设定的超时时间，可以主动回滚事务并记录相关日志，以便后续分析和处理。
4. 监控和报警：建立监控系统，实时监测数据库的事务执行情况和性能指标。设置合适的阈值和报警规则，及时发现长事务的存在，并采取相应的措施进行处理。
5. 分布式事务管理：对于分布式系统中的事务，可以采用分布式事务管理方案，如使用消息队列、分布式事务协调器等，确保事务的一致性和可靠性。
6. 定期维护和优化数据库：定期进行数据库维护和优化工作，包括索引重建、统计信息更新、碎片整理等，以提高数据库的性能和稳定性。
7. 数据库复制和备份：使用数据库复制和备份机制，确保数据的可靠性和可恢复性。在出现长事务或其他故障时，可以通过备份数据进行恢复，避免数据丢失和业务中断。

总之，通过优化查询和事务逻辑、分批处理和批量提交、合理设置事务超时时间、监控和报警、分布式事务管理、定期维护和优化数据库以及数据库复制和备份等方案，可以有效地避免或处理长事务的情况，提高数据库的性能和可靠性。

> 作者回答

首先，从应用开发端来看：

1. 确认是sdk否使用了set autocommit=0。这个确认工作可以在测试环境中开展，把MySQL的**general_log**开起来，然后随便跑一个业务逻辑，通过general_log的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成1。

2. 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用begin/commit框起来。我见过有些是业务并没有这个需要，但是也把好几个select语句放到了事务中。这种只读事务可以去掉。

3. 业务连接数据库的时候，根据业务本身的预估，通过**SETMAX_EXECUTION_TIME**命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）

其次，从数据库端来看：

1. 监控 information_schema.Innodb_trx表，设置长事务阈值，超过就报警/或者kill；

2. Percona的pt-kill这个工具不错，推荐使用；

3. 在业务功能测试阶段要求输出所有的general_log，分析日志行为提前发现问题；

4. 如果使用的是MySQL 5.6或者更新版本，把innodb_undo_tablespaces设置成2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。



## 补充

### 事务相关指令

1. 显示当前事务状态：

   ```
   SHOW ENGINE INNODB STATUS;
   ```

   这个命令将显示InnoDB存储引擎的状态信息，其中包含了当前事务的相关信息，如事务ID、事务状态等。

2. 开启事务：

   ```
   START TRANSACTION;
   ```

   这个命令用于显式地开启一个事务。在事务中执行的SQL语句将被视为一个原子操作，要么全部执行成功，要么全部回滚。

3. 提交事务：

   ```
   COMMIT;
   ```

   这个命令用于提交当前事务，将之前在事务中执行的SQL语句永久保存到数据库中。

4. 回滚事务：

   ```
   ROLLBACK;
   ```

   这个命令用于回滚当前事务，撤销之前在事务中执行的SQL语句对数据库的修改。

需要注意的是，MySQL默认情况下处于自动提交模式（autocommit），即每个SQL语句都被视为一个独立的事务并自动提交。如果要使用显式事务管理，可以通过设置`autocommit`参数为0来禁用自动提交：

```
SET autocommit = 0;
```

这样，在执行SQL语句时，~~需要显式地使用`START TRANSACTION`来开启事务，并~~使用`COMMIT`或`ROLLBACK`来结束事务。



### 开启general_log

MySQL的general_log是一种日志记录功能，它可以记录MySQL服务器上执行的所有SQL语句，包括查询、更新、事务等操作。通过开启general_log，可以详细了解MySQL服务器上的数据库操作情况，方便进行故障排查、性能分析和安全审计等工作。

要开启MySQL的general_log，可以按照以下步骤进行：

0、查看开启情况

```sql
mysql> show variables like '%general_log%';
+------------------+-------------------------------------+
| Variable_name    | Value                               |
+------------------+-------------------------------------+
| general_log      | OFF                                 |
| general_log_file | /var/lib/mysql/VM-32-165-centos.log |
+------------------+-------------------------------------+

## 临时生效
mysql> set global general_log=on;
```

1、编辑MySQL的配置文件（my.cnf或my.ini），找到并修改以下参数：

```
general_log = on
general_log_file = /data/mysql/log/general.log
```

将general_log设置为1表示开启general_log功能，将general_log_file设置为指定的日志文件路径。

2、重启MySQL服务器，使配置生效。

```bash
systemctl restart mysqld.service 
```

3、执行业务逻辑或操作数据库，MySQL将会将所有的SQL语句记录到指定的general_log文件中。

```bash
cat  /data/mysql/log/general.log 
/usr/libexec/mysqld, Version: 8.0.36 (Source distribution). started with:
Tcp port: 3306  Unix socket: /var/lib/mysql/mysql.sock
Time                 Id Command    Argument
2024-05-14T12:23:10.904093Z         8 Query     show variables like '%general_log%'
2024-05-14T12:24:10.710862Z         8 Quit
```



请注意，开启general_log会产生大量的日志记录，因此在生产环境中应谨慎使用，以免对性能产生负面影响。在测试环境中使用general_log进行确认工作是一种常见的做法，但在生产环境中，应该根据具体需求和安全考虑来决定是否开启general_log。

另外，为了避免general_log文件过大，可以定期清理或限制其大小，以免占用过多的磁盘空间。



# 04 | 深入浅出索引（上）

提到数据库索引，我想你并不陌生，在日常工作中会经常接触到。比如某一个SQL查询比较慢， 分析完原因之后，你可能就会说“给某个字段加个索引吧”之类的解决方案。但到底什么是索引， 索引又是如何工作的呢？今天就让我们一起来聊聊这个话题吧。



数据库索引的内容比较多，我分成了上下两篇文章。索引是数据库系统里面最重要的概念之一， 所以我希望你能够耐心看完。在后面的实战文章中，我也会经常引用这两篇文章中提到的知识 点，加深你对数据库索引的理解。



一句话简单来说，**索引的出现其实就是为了提高数据查询的效率，就像书的目录index一样**。一本500 页的书，如果你想快速找到其中的某一个知识点，在不借助目录的情况下，那我估计你可得找一 会儿。同样，对于数据库的表而言，索引其实就是它的“目录”。



## 索引的常见模型 

索引的出现是为了提高查询效率，但是实现索引的方式却有很多种，所以这里也就引入了**索引模型**的概念。

可以用于提高读写效率的数据结构很多，这里我先给你介绍三种常见、也比较简单的数据结构，它们分别是**哈希表、有序数组和搜索树**。



### 哈希表

下面我主要从使用的角度，为你简单分析一下这三种模型的区别。 

**哈希表**是一种以键-值（key-value）存储数据的结构，我们只要输入待查找的值即key，就可以找 到其对应的值即Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置。

不可避免地，多个key值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方 法是，拉出一个链表。 假设，你现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时对应 的哈希索引的示意图如下所示：

<img src="MYSQL实战45讲.assets/image-20241227122512264.png" alt="image-20241227122512264" style="zoom:50%;" />



图中，User2和User4根据身份证号算出来的值都是N，但没关系，后面还跟了一个链表。假设， 这时候你要查ID_card_n2对应的名字是什么，处理步骤就是：首先，将ID_card_n2通过哈希函 数算出N；然后，按顺序遍历，找到User2。

需要注意的是，图中四个ID_card_n的值并不是递增的，这样做的好处是增加新的User时速度会 很快，只需要往后追加。但缺点是，因为不是有序的，所以**哈希索引做区间查询的速度是很慢的**。

你可以设想下，如果你现在要找身份证号在[ID_card_X, ID_card_Y]这个区间的所有用户，就必须全部扫描一遍了。 

所以，**哈希表这种结构适用于只有等值查询的场景**，比如Memcached及其他一些NoSQL引擎。



### 有序数组

而有序数组在等值查询和范围查询场景中的性能就都非常优秀。还是上面这个根据身份证号 查名字的例子，如果我们使用有序数组来实现的话，示意图如下所示：

<img src="MYSQL实战45讲.assets/image-20241227122829283.png" alt="image-20241227122829283" style="zoom: 50%;" />



这里我们假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候如果你要查ID_card_n2对应的名字，用二分法就可以快速得到，这个时间复杂度是O(log(N))。

同时很显然，这个索引结构支持范围查询。你要查身份证号在[ID_card_X, ID_card_Y]区间的 User，可以先用二分法找到ID_card_X（如果不存在ID_card_X，就找到大于ID_card_X的第一 个User），然后向右遍历，直到查到第一个大于ID_card_Y的身份证号，退出循环。 如果**仅仅看查询效率，有序数组就是最好的数据结构**了。但是，在需要**更新数据的时候就麻烦**了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。



所以，**有序数组索引只适用于静态存储引擎**，比如你要保存的是2017年某个城市的所有人口 信息，这类不会再修改的数据。



### 二叉搜索树

二叉搜索树也是课本里的经典数据结构了。还是上面根据**身份证号查名字**的例子，如果我们用二叉搜索树来实现的话，示意图如下所示：

<img src="MYSQL实战45讲.assets/image-20241227123154069.png" alt="image-20241227123154069" style="zoom:50%;" />

二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。这样如果你要查 ID_card_n2的话，按照图中的搜索顺序就是按照UserA ->UserC->UserF ->User2这个路径得 到。这个时间复杂度是O(log(N))。

当然为了维持O(log(N))的查询复杂度，你就需要**保持这棵树是平衡二叉树**。为了做这个保证，更新的时间复杂度也是O(log(N))。



树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右 递增。

二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。



你可以想象一下一棵100万节点的平衡二叉树，树高20。一次查询可能需要访问20个数据块。在 机械硬盘时代，从磁盘随机读一个数据块需要10 ms左右的寻址时间。也就是说，对于一个100 万行的表，如果使用二叉树来存储，单独访问一个行可能需要20个10 ms的时间，这个查询可真够慢的。



为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该 使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于**数据块**的大小。



以InnoDB的一个整数字段索引为例，这个N差不多是1200。这棵树高是4的时候，就可以存 1200的3次方个值，这已经17亿了。考虑到树根的数据块总是在内存中的，一个10亿行的表上一 个整数字段的索引，查找一个值最多只需要访问3次磁盘。其实，树的第二层也有很大概率在内 存中，那么访问磁盘的平均次数就更少了。



N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中 了。 不管是哈希还是有序数组，或者N叉树，它们都是不断迭代、不断优化的产物或者解决方案。数 据库技术发展到今天，跳表、LSM树等数据结构也被用于引擎设计中，这里我就不再一一展开 了。



你心里要有个概念，数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据库，我 们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。 截止到这里，我用了半篇文章的篇幅和你介绍了不同的数据结构，以及它们的适用场景，你可能 会觉得有些枯燥。但是，我建议你还是要多花一些时间来理解这部分内容，毕竟这是数据库处理 数据的核心概念之一，在分析问题的时候会经常用到。当你理解了索引的模型后，就会发现在分 析问题的时候会有一个更清晰的视角，体会到引擎设计的精妙之处。 



## InnoDB 的索引模型

现在，我们一起进入相对偏实战的内容吧。在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索 引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不 同。由于InnoDB存储引擎在MySQL数据库中使用最为广泛，所以下面我就以InnoDB为例，和 你分析一下其中的索引模型。



在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。 又因为前面我们提到的，InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。 每一个索引在InnoDB里面对应一棵B+树。 假设，我们有一个主键列为ID的表，表中有字段k，并且在k上有索引。



这个表的建表语句是：

```mysql
mysql> create table T (
    id int primary key,
    k int not null,
    name varchar(16),
    index (k))engine=InnoDB;
```



表中R1~R5的(id,k)值分别为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)，两棵树的示例示意 图如下。

<img src="MYSQL实战45讲.assets/image-20241227174542135.png" alt="image-20241227174542135" style="zoom:50%;" />

从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存的是整行数据。

在InnoDB里，主键索引也被称为聚簇索引（clustered index）。 

非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引 （secondary index）。



根据上面的索引结构说明，我们来讨论一个问题：基于主键索引和普通索引的查询有什么区别？

- 如果语句是select *fromTwhere ID=500，即主键查询方式，则只需要搜索ID这棵B+树； 
- 如果语句是select *fromTwhere k=5，即普通索引查询方式，则需要先搜索k索引树，得到ID 的值为500，再到ID索引树搜索一次。这个过程称为**回表**。 

也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主 键查询。



## 索引维护(更新)

B+树为了维护**索引有序性**，在插入新值的时候需要做必要的维护。



以上面这个图为例，如果插 入新的行ID值为700，则只需要在R5的记录后面插入一个新记录。

如果新插入的ID值为400，就 相对麻烦了，需要逻辑上挪动后面的数据，空出位置。

而更糟的情况是，如果R5所在的数据页已经满了，根据B+树的算法，这时候需要申请一个新的 数据页，然后挪动部分数据过去。这个过程称为**页分裂**。在这种情况下，性能自然会受影响。



除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中， 整体空间利用率降低大约50%。



当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做**合并**。合并的过程，可以认为是分裂过程的逆过程。 基于上面的索引维护过程说明，我们来讨论一个案例：

> 你可能在一些建表规范里面见到过类似的描述，要求建表语句里一定要有自增主键。当然事无 绝对，我们来分析一下哪些场景下应该使用自增主键，而哪些场景下不应该。

自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。

插入新记录的时候可以不指定ID的值，系统会获取当前ID最大值加1作为下一条记录的ID值。 也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条 新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。



而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。

除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如 字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？ 

由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的 叶子节点占用约20个字节，而如果用整型做主键，则只要4个字节，如果是长整型（bigint）则是8个字节。 显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 所以，**从性能和存储空间方面考量，自增主键往往是更合理的选择**。



有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样 的： 1. 只有一个索引； 2. 该索引必须是唯一索引。 你一定看出来了，这就是典型的KV场景。 由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。 这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键， 可以避免每次查询需要搜索两棵树。



## 小结

今天，我跟你分析了数据库引擎可用的数据结构，介绍了InnoDB采用的B+树结构，以及为什么 InnoDB要这么选择。

B+树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。 

由于InnoDB是索引组织表，一般情况下我会建议你创建一个自增主键，这样非主键索引占用的 空间最小。

但事无绝对，我也跟你讨论了使用业务逻辑字段做主键的应用场景。 



## 问题

最后，我给你留下一个问题吧。对于上面例子中的InnoDB表T，如果你要重建索引 k，你的两个 SQL语句可以这么写：

```
alter table T drop index k;
alter table T add index(k);
```

如果你要重建主键索引，也可以这么写：

```
alter table T drop primary key;
alter table T add primary key(id);
```

我的问题是，对于上面这两个重建索引的作法，说出你的理解。如果有不合适的，为什么，更好 的方法是什么？

你可以把你的思考和观点写在留言区里，我会在下一篇文章的末尾给出我的参考答案。感谢你的 收听，也欢迎你把这篇文章分享给更多的朋友一起阅读。



### 回答

在评论区，有同学问到为什么要重建索引。我们文章里面有提到，索引可能因为删除，或者页分 裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样 页面的利用率最高，也就是索引更紧凑、更省空间。 

这道题目，我给你的“参考答案”是： 重建索引k的做法是合理的，可以达到省空间的目的。但是，重建主键的过程不合理。不论是删 除主键还是创建主键，都会将整个表重建。所以连着执行这两个语句的话，第一个语句就白做 了。这两个语句，你可以用这个语句代替 ： `alter table T engine=InnoDB`





### chatGPT

不合适的地方

1. **性能问题**：
   - 在删除索引和重新添加索引的过程中，表会被锁定，可能会导致长时间的写入延迟，影响应用程序的性能。
   - 对于大型表，删除和重新创建索引可能会消耗大量的时间和资源。
2. **数据完整性**：
   - 在删除主键索引期间，表的完整性可能会受到影响，尤其是如果有外键依赖于该主键。
3. **事务安全性**：
   - 如果在删除索引和添加索引之间发生错误，可能会导致表处于不一致状态。

更好的方法

1. **使用 `ALTER TABLE ... DROP INDEX ... ADD INDEX ...` 一次性重建索引**：

   - MySQL 允许在同一个 `ALTER TABLE` 语句中同时删除和添加索引，这样可以减少锁定时间和资源消耗。

   ```sql
   ALTER TABLE T DROP INDEX k, ADD INDEX  k_name(k);
   ```

   ![image-20241227181041450](MYSQL实战45讲.assets/image-20241227181041450.png)

2. **使用 `ALTER TABLE ... CHANGE` 或 `MODIFY`**：

   - 对于主键索引，可以使用 `CHANGE` 或 `MODIFY` 来重建主键，而不需要显式地删除它。

   ```sql
   ALTER TABLE employees CHANGE id id INT NOT NULL PRIMARY KEY;
   ALTER TABLE employees MODIFY id INT NOT NULL PRIMARY KEY;
   
   ```

3. **使用 `ONLINE` 选项**（如果使用的是 MySQL 5.6 及以上版本）：

   - 在某些情况下，可以使用 `ONLINE` 选项来重建索引而不锁定表。

   ```
   ALTER TABLE T ADD INDEX k (column_name) ALGORITHM=INPLACE, LOCK=NONE;
   ```





# 05 | 深入浅出索引（下）

今天我们再继续聊聊跟MySQL索 引有关的概念。 

在开始这篇文章之前，我们先来看一下这个问题： 在下面这个表T中，如果我执行 `select * from T where k between 3 and 5`，需要执行几次树的搜索操作，会扫描多少行？ 下面是这个表的初始化语句。

```sql
mysql> create table T (
ID int primary key,
k int NOT NULL DEFAULT 0,
s varchar(16) NOT NULL DEFAULT '',
index k(k))
engine=InnoDB;


insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg')
```

<img src="MYSQL实战45讲.assets/image-20241227200431625.png" alt="image-20241227200431625" style="zoom:50%;" />



现在，我们一起来看看这条SQL查询语句的执行流程： 

1. 在k索引树上找到k=3的记录，取得 ID = 300； 
2.  再到ID索引树查到ID=300对应的R3； 
3. 在k索引树取下一个值k=5，取得ID=500； 
4. 再回到ID索引树查到ID=500对应的R4； 
5. 在k索引树取下一个值k=6，不满足条件，循环结束。



在这个过程中，回到主键索引树搜索的过程，我们称为回表。

可以看到，这个查询过程读了k 索引树的3条记录（步骤1、3和5），回表了两次（步骤2和4）。 在这个例子中，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。那么，有没有可能经过索引优化，避免回表过程呢？



## 覆盖索引

如果执行的语句是select ID fromTwhere k between 3 and 5，这时只需要查ID的值，而ID的值 已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，**在这个查询里面， 索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引**。



由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。 

需要注意的是，在引擎内部使用覆盖索引在索引k上其实读了三个记录，R3~R5（对应的索引k 上的记录项），但**是对于MySQL的Server层来说，它就是找引擎拿到了两条记录，因此MySQL 认为扫描行数是2(两次回表）**。

> 备注：关于如何查看扫描行数的问题，我将会在第16文章《如何正确地显示随机消息？》 中，和你详细讨论。



基于上面覆盖索引的说明，我们来讨论一个问题：在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？ 假设这个市民表的定义是这样的：

```sql
CREATE TABLE `tuser` (
    `id` int(11) NOT NULL,
    `id_card` varchar(32) DEFAULT NULL,
    `name` varchar(32) DEFAULT NULL,
    `age` int(11) DEFAULT NULL,
    `ismale` tinyint(1) DEFAULT NULL,
    PRIMARY KEY (`id`),
    KEY `id_card` (`id_card`),
    KEY `name_age` (`name`,`age`)
) ENGINE=InnoDB

```



我们知道，身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求， 我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是 不是浪费空间？

如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它 可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。

当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑 了。这正是业务DBA，或者称为业务数据架构师的工作。



## 最左前缀原则

看到这里你一定有一个疑问，如果为每一种查询都设计一个索引，索引是不是太多了。如果我现 在要按照市民的身份证号去查他的家庭地址呢？虽然这个查询需求在业务中出现的概率不高，但 总不能让它走全表扫描吧？反过来说，单独为一个不频繁的请求创建一个（身份证号，地址）的 索引又感觉有点浪费。应该怎么做呢？ 

这里，我先和你说结论吧。**B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录。** 为了直观地说明这个概念，我们用（name，age）这个联合索引来分析。

<img src="MYSQL实战45讲.assets/image-20250102122626442.png" alt="image-20250102122626442" style="zoom: 50%;" />



可以看到，索引项是按照索引定义里面出现的字段顺序排序的。 当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到ID4，然后向后遍历得到所有 需要的结果。

如果你要查的是所有名字第一个字是“张”的人，你的SQL语句的条件是"where name like ‘张%’"。这时，你也能够用上这个索引，查找到第一个符合条件的记录是ID3，然后向后遍历， 直到不满足条件为止。 

可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。**这个最左 前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符**。



基于上面对最左前缀索引的说明，我们来讨论一个问题：在建立联合索引的时候，**如何安排索 引内的字段顺序**。 

这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了(a,b)这个联 合索引后，一般就不需要单独在a上建立索引了。

因此，**第一原则是，如果通过调整顺序，可 以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的**。



所以现在你知道了，这段开头的问题里，我们要为高频请求创建(身份证号，姓名）这个联合索 引，并用这个索引支持“根据身份证号查询地址”的需求。 

那么，如果既有联合查询，又有基于a、b各自的查询呢？查询条件里面只有b的语句，是无法使 用(a,b)这个联合索引的，这时候你不得不维护另外一个索引，也就是说你**需要同时维护(a,b)、 (b)** 这两个索引。

这时候，我们要考虑的原则就是空间了。比如上面这个市民表的情况，name字段是比age字段 大的 ，那我就建议你创建一个（name,age)的联合索引和一个(age)的单字段索引。



## 索引下推

上一段我们说到满足最左前缀原则的时候，最左前缀可以用于在索引中定位记录。这时，你可能 要问，那些不符合最左前缀的部分，会怎么样呢？ 

我们还是以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一 个字是张，而且年龄是10岁的所有男孩”。那么，SQL语句是这么写的：

```sql
mysql> select * from tuser where name like '张%' and age=10 and ismale=1;
```



你已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足 条件的记录ID3。当然，这还不错，总比全表扫描要好。



然后呢？ 当然是判断其他条件是否满足。 在MySQL 5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。



而MySQL 5.6 引入的**索引下推优化（index condition pushdown)**， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。



图3和图4，是这两个过程的执行流程图。

<img src="MYSQL实战45讲.assets/image-20250103121301961.png" alt="image-20250103121301961" style="zoom: 50%;" />

<img src="MYSQL实战45讲.assets/image-20250103121414858.png" alt="image-20250103121414858" style="zoom:50%;" />



在图3和4这两个图里面，每一个虚线箭头表示回表一次。

图3中，在(name,age)索引里面我特意去掉了age的值，这个过程InnoDB并不会去看age的值， 只是按顺序把“name第一个字是’张’”的记录一条条取出来回表。因此，需要回表4次。 

图4跟图3的区别是，InnoDB在(name,age)索引内部就判断了age是否等于10，对于不等于10的 记录，直接判断并跳过。

在我们的这个例子中，只需要对ID4、ID5这两条记录回表取数据判 断，就只需要回表2次。



## 小结 

今天这篇文章，我和你继续讨论了数据库索引的概念，包括了**覆盖索引、前缀索引、索引下推**。 你可以看到，在满足语句需求的情况下， 尽量少地访问资源是数据库设计的重要原则之一。我 们在使用数据库的时候，尤其是在设计表结构时，也要以减少资源消耗作为目标。



### 问题

接下来我给你留下一个问题吧。 实际上主键索引也是可以使用多个字段的。DBA小吕在入职新公司的时候，就发现自己接手维 护的库里面，有这么一个表，表结构定义类似这样的：

```sql
CREATE TABLE `geek` (
    `a` int(11) NOT NULL,
    `b` int(11) NOT NULL,
    `c` int(11) NOT NULL,
    `d` int(11) NOT NULL,
    PRIMARY KEY (`a`,`b`),
    KEY `c` (`c`),
    KEY `ca` (`c`,`a`),
    KEY `cb` (`c`,`b`)
) ENGINE=InnoDB;

```

公司的同事告诉他说，由于历史原因，这个表需要a、b做联合主键，这个小吕理解了。 但是，学过本章内容的小吕又纳闷了，既然主键包含了a、b这两个字段，那意味着单独在字段c 上创建一个索引，就已经包含了三个字段了呀，为什么要创建“ca”“cb”这两个索引？ 同事告诉他，是因为他们的业务里面有这样的两种语句：

```
select * from geek where c=N order by a limit 1;
select * from geek where c=N order by b limit 1;
```

我给你的问题是，这位同事的解释对吗，为了这两个查询模式，这两个索引是否都是必须的？为 什么呢？



### 回答

上期的问题是关于对联合主键索引和InnoDB索引组织表的理解。

我直接贴@老杨同志 的回复略作修改如下（我修改的部分用橙色标出）： 

表记录

```
–a--|–b--|–c--|–d--
1 2 3 d
1 3 2 d
1 4 3 d
2 1 3 d
2 2 2 d
2 3 4 d
```

主键 a，b的聚簇索引组织顺序相当于 order bya,b ，也就是先按a排序，再按b排序，c无序。



索引 ca 的组织是先按c排序，再按a排序，同时记录主键

```
–c--|–a--|–主键部分b-- （注意，这里不是ab，而是只有b）
2 1 3
2 2 2
3 1 2
3 1 4
3 2 1
4 2 3
```

这个跟索引c的数据是一模一样的。



索引 cb 的组织是先按c排序，在按b排序，同时记录主键

```
–c--|–b--|–主键部分a-- （同上）
2 2 2
2 3 1
3 1 2
3 2 1
3 4 1
4 3 2
```

所以，结论是ca可以去掉，cb需要保留。

> PS：索引可以加快排序速度

如果您在MySQL中创建了一个索引，例如`KEY('a', 'b')`，并且您的SQL语句中包含了`ORDER BY b`子句，那么这个索引可以提高查询语句的性能。

索引的作用是加快数据库的查询速度，通过创建索引，MySQL可以更快地定位到满足查询条件的数据行。当您在索引中包含了`b`列，并且查询语句中使用了`ORDER BY b`进行排序时，MySQL可以直接利用索引中的排序信息，避免对所有数据进行排序操作。

具体来说，当查询语句包含`ORDER BY b`时，MySQL可以使用索引中的排序顺序来直接返回按照`b`列排序的结果，而无需额外的排序操作。这样可以大大减少查询的时间和资源消耗，提高查询性能。









# 06 | 全局锁和表锁 ：给表加个字段怎么有这么多阻碍？

数据库锁设计的初衷是处理并发问题。

作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。

根据加锁的范围，MySQL中的锁大致可以分为**全局锁、表级锁和行锁**。

这篇文章分享全局锁和表级锁。

需要说明的是，锁的设计比较复杂，这两篇文章不会涉及锁的具体实现细节，主要介绍的是碰到锁时的现象和其背后的原理。



## 全局锁

顾名思义，全局锁就是对整个数据库实例加锁。

### FTWRL

MySQL提供了一个加全局读锁的方法，命令是**Flush tables with read lock (FTWRL)**。

当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句

**全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本。**

以前有一种做法，是通过FTWRL确保不会有其他线程对数据库做更新，然后对整个库做备份。

注意，在备份过程中整个库完全处于只读状态。但是让整库都只读，听上去就很危险（有以下问题）：

- 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；
- 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟。

看来加全局锁不太好。但是细想一下，备份为什么要加锁呢？我们来看一下不加锁会有什么问题。

假设你现在要维护“极客时间”的购买系统，关注的是用户账户余额表和用户课程表。

 现在发起一个逻辑备份。假设备份期间，有一个用户，他购买了一门课程，业务逻辑里就要扣掉 他的余额，然后往已购课程里面加上一门课。 如果时间顺序上是先备份账户余额表(u_account)，然后用户购买，然后备份用户课程表 (u_course)，会怎么样呢？你可以看一下这个图：

<img src="MYSQL实战45讲.assets/image-20250103193922935.png" alt="image-20250103193922935" style="zoom:50%;" />



可以看到，这个备份结果里，用户A的数据状态是“账户余额没扣，但是用户课程表里面已经多了一门课”。如果后面用这个备份来恢复数据的话，用户A就发现，自己赚了。

作为用户可别觉得这样可真好啊，你可以试想一下：如果备份表的顺序反过来，先备份用户课程表再备份账户余额表，又可能会出现什么结果？ 



也就是说，不加锁的话，备份系统备份得到的库（不同的表）不是一个逻辑时间点，这个视图是逻辑不一致的。

说到视图你肯定想起来了，我们在前面讲事务隔离的时候，其实是有一个方法能够拿到一致性视图的，对吧？是的，就是**在可重复读隔离级别下开启一个事务**。



### mysqldump工具

官方自带的逻辑备份工具是mysqldump。

**当mysqldump使用参数–single-transaction的时候**，导数据之前就会启动一个事务，来确保拿到一致性视图。

而由于MVCC的支持，这个过程中数据是可以正常更新的。

为什么还需要FTWRL呢？

一致性读是好，但前提是引擎要支持这个隔离级别。 

比如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。

这时，我们就需要使用FTWRL命令了。如果有的表使用了不支持事务的引擎，那么备份就只能通过FTWRL方法。这往往是DBA要求业务开发人员使用InnoDB替代MyISAM的原因之一。



你也许会问，既然要全库只读，为什么不使用 既 set global readonly=true 的方式呢 的 ？确实readonly方式也可以让全库进入只读状态，但还是会建议你用FTWRL方式，主要有两个原因：

- 一是，在有些系统中，**readonly的值会被用来做其他逻辑**，比如用来判断一个库是主库还是备库。因此，修改global变量的方式影响面更大，我不建议你使用。
- 二是，**在异常处理机制上有差异**。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。

**业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。**不论是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操作，都是会被锁住的。但是，即使没有被全局锁住，加字段也不是就能一帆风顺的，因为你还会碰到接下来我们要介绍的表级锁



## 表级锁

MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。

### 表锁

表锁的语法是 `lock tables ... read/write`.

与FTWRL类似，可以用`unlock tables`主动释放锁，也可以在客户端断开的时候自动释放。

需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。

举个例子, 如果在某个线程A中执行**lock tables t1 read, t2 write;** 这个语句，则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许，自然也不能访问其他表。

在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。

而对于InnoDB这种支持**行锁**的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。



### 元数据锁（meta data lock，MDL)

MDL不需要显式使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。

因此，**在MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁**。

- 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。(互斥的意思是只有一个锁)
- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行

虽然MDL锁是系统默认会加的，但却是你不能忽略的一个机制。比如下面这个例子，**经常看到有人掉到这个坑里：给一个小表加个字段，导致整个库挂了**。

**给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据**。

在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，即使是小表，操作不慎也会出问题。我们来看一下下面的操作序列，假设表t是一个小表。

<img src="MYSQL实战45讲.assets/image-20240604194259700.png" alt="image-20240604194259700" style="zoom:50%;" />

我们可以看到session A先启动，这时候会对表t加一个MDL读锁。由于session B需要的也是MDL读锁，因此可以正常执行。

之后session C会被blocked，是因为session A的MDL读锁还没有释放，而session C需要MDL写锁，因此只能被阻塞。



如果只有session C自己被阻塞还没什么关系，但是之后所有要在表t上新申请MDL读锁的请求也会被session C阻塞。前面我们说了，所有对表的增删改查操作都需要先申请MDL读锁，就都被锁住，等于这个表现在完全不可读写了。

如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新session

再请求的话，这个库的线程很快就会爆满。

你现在应该知道了，事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。



基于上面的分析，我们来讨论一个问题，

**首先我们要解决掉长事务，事务不提交，就会一直占着MDL锁。**在MySQL的information_schema库的 innodb_trx表中，你可以查到当前执行中的事务。如果你要做DDL变更的表刚好有长事务在执行，要考虑先暂停DDL，或者kill掉这个长事务。



但考虑一下这个场景。如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？

这时候kill可能未必管用，因为新的请求马上就来了。

比较理想的机制是，在alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。

MariaDB已经合并了AliSQL的这个功能，所以这两个开源分支目前都支持DDL NOWAIT/WAITn这个语法

```
ALTER TABLE tbl_name NOWAIT add column ...
ALTER TABLE tbl_name WAIT N add column ...
```

## 小结

全局锁主要用在逻辑备份过程中。对于全部是InnoDB引擎的库，我建议你选择使用–singletransaction参数，对应用会更友好

表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有lock tables这样的语句，你需要追查一下，比较可能的情况是： 要么是你的系统现在还在用MyISAM这类不支持事务的引擎，那要安排升级换引擎； 要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。

MDL会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和 更新。

### 问题

最后，我给你留一个问题吧。备份一般都会在备库上执行，你在用–single-transaction方法做逻 辑备份的过程中，如果主库上的一个小表做了一个DDL，比如给一个表上加了一列。这时候，从 备库上会看到什么现象呢？

> 回答

假设这个DDL是针对表t1的， 这里我把备份过程中几个关键的语句列出来：

```
Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;
Q2:START TRANSACTION WITH CONSISTENT SNAPSHOT；
/* other tables */
Q3:SAVEPOINT sp;
/* 时刻 1 */
Q4:show create table `t1`;
/* 时刻 2 */
Q5:SELECT * FROM `t1`;
/* 时刻 3 */
Q6:ROLLBACK TO SAVEPOINT sp;
/* 时刻 4 */
/* other tables */

```

1、在备份开始的时候，为了确保RR（可重复读）隔离级别，再设置一次RR隔离级别(Q1); 

2、启动事务，这里用 WITH CONSISTENTSNAPSHOT确保这个语句执行完就可以得到一个一致性 视图（Q2)；

3、设置一个保存点，这个很重要（Q3）；

4、showcreate 是为了拿到表结构(Q4)，然后正式导数据 （Q5），回滚到SAVEPOINTsp，在这里的作用是释放 t1的MDL锁 （Q6）。当然这部分属于“超纲”，上文正文里面都没提到。



DDL从主库传过来的时间按照效果不同，我打了四个时刻。题目设定为小表，我们假定到达后， 如果开始执行，则很快能够执行完成。

参考答案如下： 

1. 如果在Q4语句执行之前到达，现象：没有影响，备份拿到的是DDL后的表结构。 
2. 如果在“时刻 2”到达，则表结构被改过，Q5执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump终止； 
3. 如果在“时刻2”和“时刻3”之间到达，mysqldump占着t1的MDL读锁，binlog被阻塞，**现象： 主从延迟**，直到Q6执行完成。 
4. 从“时刻4”开始，mysqldump释放了MDL读锁，现象：没有影响，备份拿到的是DDL前的表结构。





# 07 | 行锁功过：怎么减少行锁对性能的影响？



在上一篇文章中，我跟你介绍了MySQL的全局锁和表级锁，今天我们就来讲讲MySQL的**行锁**。

MySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 **MyISAM引擎就不支持行锁**。

不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。

InnoDB是支持行锁的， 这也是MyISAM被InnoDB替代的重要原因之一。



我们今天就主要来聊聊InnoDB的行锁，以及如何通过减少锁冲突来提升业务并发度。

顾名思义，行锁就是针对数据表中行记录的锁。这很好理解，比如事务A更新了一行，而这时候 事务B也要更新同一行，则必须等事务A的操作完成后才能进行更新。



当然，数据库中还有一些没那么一目了然的概念和设计，这些概念如果理解和使用不当，容易导 致程序出现非预期行为，比如两阶段锁。



## 从两阶段锁说起

我先给你举个例子。在下面的操作序列中，事务B的update语句执行时会是什么现象呢？假设字 段id是表t的主键。

<img src="MYSQL实战45讲.assets/image-20250108181942677.png" alt="image-20250108181942677" style="zoom: 50%;" />

这个问题的结论取决于事务A在执行完两条update语句后，持有哪些锁，以及在什么时候释放。 你可以验证一下：实际上事务B的update语句会被阻塞，直到事务A执行commit之后，事务B才 能继续执行。



知道了这个答案，你一定知道了**事务A持有的两个记录的行锁，都是在commit的时候才释放的**。



也就是说，**在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放**。这个就是**两阶段锁协议**。



知道了这个设定，对我们使用事务有什么帮助呢？

那就是，如果你的事务中需要锁多个行，要**把最可能造成锁冲突、最可能影响并发度的锁尽量往后放**。我给你举个例子。



假设你负责实现一个电影票在线交易业务，顾客A要在影院B购买电影票。我们简化一点，这个 业务需要涉及到以下操作：

1. 从顾客A账户余额中扣除电影票价； 
2. 给影院B的账户余额增加这张电影票价；
3. 记录一条交易日志。

也就是说，要完成这个交易，我们需要update两条记录，并insert一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。那么，你会怎样安排这三个语句在事务中的 顺序呢？

试想如果同时有另外一个顾客C要在影院B买票，那么这两个事务冲突的部分就是语句2了。因为 它们要更新同一个影院账户的余额，需要修改同一行数据。



根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你**把语句2安排在最后**，比如按照3、1、2这样的顺序，那么影院账户余额 这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。



好了，现在由于你的正确设计，影院余额这一行的行锁在一个事务中不会停留很长时间。但是， 这并没有完全解决你的困扰。



如果这个影院做活动，可以低价预售一年内所有的电影票，而且这个活动只做一天。于是在活动 时间开始的时候，你的MySQL就挂了。你登上服务器一看，CPU消耗接近100%，但整个数据库每秒就执行不到100个事务。这是什么原因呢？



这里，我就要说到死锁和死锁检测了。



## 死锁和死锁检测

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致 这几个线程都进入无限等待的状态，称为死锁。这里我用数据库中的行锁举个例子。

<img src="MYSQL实战45讲.assets/image-20250108182637092.png" alt="image-20250108182637092" style="zoom:50%;" />

这时候，事务A在等待事务B释放id=2的行锁，而事务B在等待事务A释放id=1的行锁。 事务A和 事务B在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：

- 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 **innodb_lock_wait_timeout**来设置。 
- 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数**innodb_deadlock_detect**设置为on，表示开启这个逻辑。

在InnoDB中，innodb_lock_wait_timeout的默认值是50s，意味着如果采用第一个策略，当出现 死锁以后，第一个被锁住的线程要过50s才会超时退出，然后其他线程才有可能继续执行。对于 在线服务来说，这个等待时间往往是无法接受的。

但是，我们又不可能直接把这个时间设置成一个很小的值，比如1s。这样当出现死锁的时候，确 实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会 出现很多误伤。



所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect的默认值本身就是on。主动死锁检测在发生死锁的时候，是能够快速发 现并进行处理的，但是它也是有额外负担的。



你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。

那如果是我们上面说到的所有事务都要更新同一行的场景呢？



每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级 的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，你就会看到 CPU利用率很高，但是每秒却执行不了几个事务。



根据上面的分析，我们来讨论一下，**怎么解决由这种热点行更新导致的性能问题呢**？问题的**症结在于，死锁检测要耗费大量的CPU资源**。



**一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉**。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严 重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关 掉死锁检测意味着可能会出现大量的超时，这是业务有损的。



**另一个思路是控制并发度**。根据上面的分析，你会发现如果并发能够控制住，比如同一行同时最多只有10个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有600个客户端，这样即使每个客户端控制到只有5个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到3000。

因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的 团队有能修改MySQL源码的人，也可以做在MySQL里面。**基本思路就是，对于相同行的更新， 在进入引擎之前排队。**这样在InnoDB内部就不会有大量的死锁检测工作了。



可能你会问，如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设 计上优化这个问题呢？

**你可以考虑通过将一行改成逻辑上的多行来减少锁冲突**。还是以影院账户为例，可以考虑放在多 条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账 户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的1/10，可以减少锁等 待个数，也就减少了死锁检测的CPU消耗。

这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会 减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成0的时候，代码要有特殊处理。



## 小结

今天，我和你介绍了MySQL的行锁，涉及了两阶段锁协议、死锁和死锁检测这两大部分内容。

其中，我以两阶段协议为起点，和你一起讨论了在开发的时候如何安排正确的事务语句。这里的 原则/我给你的建议是：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并 发度的锁的申请时机尽量往后放。

但是，调整语句顺序并不能完全避免死锁。所以我们引入了死锁和死锁检测的概念，以及提供了 三个方案，来减少死锁对数据库的影响。减少死锁的主要方向，就是控制访问相同资源的并发事 务量。



### 问题

最后，我给你留下一个问题吧。如果你要删除一个表里面的前10000行数据，有以下三种方法可 以做到：

- 第一种，直接执行delete fromTlimit 10000; 
- 第二种，在一个连接中循环执行20次 delete fromTlimit 500; 
- 第三种，在20个连接中同时执行delete fromTlimit 500。

你会选择哪一种方法呢？为什么呢？



### 回答

比较多的留言都选择了第二 种方式，即：在一个连接中循环执行20次 delete fromTlimit 500。 确实是这样的，第二种方式是相对较好的。

第一种方式（即：直接执行delete fromTlimit 10000）里面，单个语句占用时间长，锁的时间也 比较长；而且大事务还会导致主从延迟。

第三种方式（即：在20个连接中同时执行delete fromTlimit 500），会人为造成锁冲突（并发大）。



# 08 | 事务到底是隔离的还是不隔离的？



我在第3篇文章和你讲事务隔离级别的时候提到过，如果是可重复读隔离级别，事务T启动的时 候会创建一个视图read-view，之后事务T执行期间，即使有其他事务修改了数据，事务T看到的 仍然跟在启动时看到的一样。也就是说，一个在可重复读隔离级别下执行的事务，好像与世无争，不受外界影响。



但是，我在上一篇文章中，和你分享行锁的时候又提到，一个事务要更新一行，如果刚好有另外 一个事务拥有这一行的行锁，它又不能这么超然了，会被锁住，进入等待状态。问题是，既然进 入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢？



我给你举一个例子吧。下面是一个只有两行的表的初始化语句。

```sql
mysql> CREATE TABLE `t` (
`id` int(11) NOT NULL,
`k` int(11) DEFAULT NULL,
PRIMARY KEY (`id`)
) ENGINE=InnoDB;

insert into t(id, k) values(1,1),(2,2);
```



<img src="MYSQL实战45讲.assets/image-20250108192740451.png" alt="image-20250108192740451" style="zoom: 50%;" />

这里，我们需要注意的是**事务的启动时机**。

**begin/start transaction 命令**并不是一个事务的起点，**在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动（包括select语句）**。如果你想要马上启动一个事务，可以使用`start transaction with consistent snapshot` 这个命令。

还需要注意的是，在整个专栏里面，我们的例子中如果没有特别说明，都是默认 autocommit=1。

```mysql
mysql> SHOW VARIABLES like 'autocommit';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| autocommit    | ON    |
+---------------+-------+
```



在这个例子中，事务C没有显式地使用begin/commit，表示这个update语句本身就是一个事务， 语句完成的时候会自动提交。

事务B在更新了行之后查询; 

事务A在一个只读事务中查询，并且时间顺序上是在事务B的查询之后。



这时，如果我告诉你**事务B查到的k的值是3，而事务A查到的k的值是1**，你是不是感觉有点晕呢？



所以，今天这篇文章，我其实就是想和你说明白这个问题，希望借由把这个疑惑解开的过程，能 够帮助你对InnoDB的事务和锁有更进一步的理解。 在MySQL里，有两个“视图”的概念：

- 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。 创建视图的语法是create view…，而它的查询方法与表一样(一旦视图创建成功，您可以像查询表一样使用它)。

```mysql
mysql> CREATE VIEW v1 AS select * from T where id = 1;
Query OK, 0 rows affected (0.00 sec)


mysql> select * from v1 where id = 1;
+----+---+------+
| id | k | name |
+----+---+------+
|  1 | 1 | NULL |
+----+---+------+
1 row in set (0.00 sec)

mysql> SELECT TABLE_NAME
    -> FROM INFORMATION_SCHEMA.VIEWS
    -> WHERE TABLE_SCHEMA = 'test'
    -> ;
+------------+
| TABLE_NAME |
+------------+
| v1         |
+------------+

```



- 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持 RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”。

在第3篇文章《事务隔离：为什么你改了我还看不见？》中，我跟你解释过一遍MVCC的实现逻 辑。今天为了说明查询和更新的区别，我换一个方式来说明，把read view拆开。你可以结合这 两篇文章的说明来更深一步地理解MVCC。



## “快照”在MVCC里是怎么工作的？

在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。



这时，你会说这看上去不太现实啊。如果一个库有100G，那么我启动一个事务，MySQL就要拷 贝100G的数据出来，这个过程得多慢啊。可是，我平时的事务执行起来很快啊。 

实际上，我们并不需要拷贝出这100G的数据。我们先来看看这个快照是怎么实现的。

InnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向 InnoDB的事务系统申请的，是按申请顺序严格递增的。



而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且 把t**ransaction id**赋值给这个数据版本的事务ID，记为**row trx_id**。同时，旧的数据版本要保留， 并且在新的数据版本中，能够有信息可以直接拿到它。



也就是说，数据表中的一行记录，其实可能有多个版本(row)，每个版本有自己的rowtrx_id。



如图2所示，就是一个记录被多个事务连续更新后的状态。

<img src="MYSQL实战45讲.assets/image-20250108193748598.png" alt="image-20250108193748598" style="zoom:50%;" />



图中虚线框里是同一行数据的4个版本，当前最新版本是V4，k的值是22，它是被transaction id 为25的事务更新的，因此它的rowtrx_id也是25。

你可能会问，前面的文章不是说，语句更新会生成undo log（回滚日志）吗？那么，undo log 在哪呢？

实际上，图2中的三个虚线箭头，就是undo log；而V1、V2、V3并不是物理上真实存在的，而 是每次需要的时候根据当前版本和undo log计算出来的。比如，需要V2的时候，就是通过V4依 次执行U3、U2算出来。



明白了多版本和rowtrx_id的概念后，我们再来想一下，InnoDB是怎么定义那个“100G”的快照 的。

按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这 个事务执行期间，其他事务的更新对它不可见。



因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；

如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。

当然，如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。



在实现上， **InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交**。

**数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位**。

这个视图数组和高水位，就组成了**当前事务的一致性视图（read-view）**。

而数据版本的可见性规则，就是基于数据的rowtrx_id和这个一致性视图的对比结果得到的。



这个视图数组把所有的rowtrx_id 分成了几种不同的情况。

<img src="MYSQL实战45讲.assets/image-20250108194420594.png" alt="image-20250108194420594" style="zoom:50%;" />



这样，对于当前事务的启动瞬间来说，一个数据版本的row trx_id，有以下几种可能：

1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；

3. 如果落在黄色部分，那就包括两种情况
   1. 若 rowtrx_id在数组中，表示这个版本是由还没提交的事务生成的，不可见；
   2. 若 rowtrx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见。

比如，对于图2中的数据来说，如果有一个事务，它的低水位是18，那么当它访问这一行数据 时，就会从V4通过U3计算出V3，所以在它看来，这一行的值是11。



你看，有了这个声明后，系统里面随后发生的更新，是不是就跟这个事务看到的内容无关了呢？ 因为之后的更新，生成的版本一定属于上面的2或者3(a)的情况，而对它来说，这些新的数据版本是不存在的，所以这个事务的快照，就是“静态”的了。



所以你现在知道了，**InnoDB利用了“所有数据都有多个版本”(版本链)的这个特性，实现了“秒级创建快照”的能力**。



接下来，我们继续看一下图1中的三个事务，**分析下事务A的语句返回的结果，为什么是k=1**。

<img src="MYSQL实战45讲.assets/image-20250108192740451.png" alt="image-20250108192740451" style="zoom: 50%;" />



```mysql
start transaction with consistent snapshot;
                                             update T set k=k+1 where id=1;
update T set k=k+1 where id=1;
select * from T where id =1;

commit;
```



这里，我们不妨做如下假设：

1. 事务A开始前，系统里面只有一个活跃事务ID是99；

2. 事务A、B、C的版本号分别是100、101、102，且当前系统里只有这四个事务；

3. 三个事务开始前，(1,1）这一行数据的rowtrx_id是90。

这样，事务A的视图数组就是[99,100], 事务B的视图数组是[99,100,101], 事务C的视图数组是 [99,100,101,102]。



为了简化分析，我先把其他干扰语句去掉，只画出跟事务A查询逻辑有关的操作：

<img src="MYSQL实战45讲.assets/image-20250108195034604.png" alt="image-20250108195034604" style="zoom:50%;" />

从图中可以看到，第一个有效更新是事务C，把数据从(1,1)改成了(1,2)。这时候，这个数据的最 新版本的rowtrx_id是102，而90这个版本已经成为了历史版本。

第二个有效更新是事务B，把数据从(1,2)改成了(1,3)。这时候，这个数据的最新版本（即row trx_id）是101，而102又成为了历史版本。

你可能注意到了，在事务A查询的时候，其实事务B还没有提交，但是它生成的(1,3)这个版本已经变成当前版本了。但这个版本对事务A必须是不可见的，否则就变成脏读了。

好，现在事务A要来读数据了，它的视图数组是[99,100]。当然了，**读数据都是从当前版本读起的**。所以，事务A查询语句的读数据流程是这样的： 找到(1,3)的时候，判断出rowtrx_id=101，比高水位大，处于红色区域，不可见； 接着，找到上一个历史版本，一看rowtrx_id=102，比高水位大，处于红色区域，不可见； 再往前找，终于找到了（1,1)，它的rowtrx_id=90，比低水位小，处于绿色区域，可见。



这样执行下来，虽然期间这一行数据被修改过，但是事务A不论在什么时候查询，看到这行数据的结果都是一致的，所以我们称之为一致性读。



这个判断规则是从代码逻辑直接转译过来的，但是正如你所见，用于人肉分析可见性很麻烦。 所以，我来给你翻译一下。

一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况： 

1. 版本未提交，不可见； 
2. 版本已提交，但是是在视图创建后提交的，不可见； 
3. 版本已提交，而且是在视图创建前提交的，可见。

现在，我们用这个规则来判断图4中的查询结果，事务A的查询语句的视图数组是在事务A启动的 时候生成的，这时候：

-  (1,3)还没提交，属于情况1，不可见；
-  (1,2)虽然提交了，但是是在视图数组创建之后提交的，属于情况2，不可见； 
- (1,1)是在视图数组创建之前提交的，可见。 

你看，去掉数字对比后，只用时间先后顺序来判断，分析起来是不是轻松多了。所以，后面我们 就都用这个规则来分析。



## 更新逻辑

细心的同学可能有疑问了：事务B的update语句，如果按照一致性读，好像结果不对哦？ 你看图5中，事务B的视图数组是先生成的，之后事务C才提交，不是应该看不见(1,2)吗，怎么能 算出(1,3)来?

<img src="MYSQL实战45讲.assets/image-20250205182207775.png" alt="image-20250205182207775" style="zoom:50%;" />



是的，如果事务B在更新之前查询一次数据，这个查询返回的k的值确实是1。

但是，当它要去更新数据的时候，就不能再在历史版本上更新了，否则事务C的更新就丢失了。 因此，事务B此时的set k=k+1是在（1,2）的基础上进行的操作。



所以，这里就用到了这样一条规则：更新数据都是先读后写的，而这个读，只能读当前的 值，称为**“当前读”（current read）**。



因此，在更新的时候，当前读拿到的数据是(1,2)，更新后生成了新版本的数据(1,3)，这个新版本 的rowtrx_id是101。 所以，在执行事务B查询语句的时候，一看自己的版本号是101，最新数据的版本号也是101，是 自己的更新，可以直接使用，所以查询得到的k的值是3。 

这里我们提到了一个概念，叫作当前读。

其实，**除了update语句外，select语句如果加锁，也是 当前读**。



所以，如果把事务A的查询语句`select * fromt where id=1`修改一下，加上`lock in share mode` 或 `for update`，也都可以读到版本号是101的数据，返回的k的值是3。

下面这两个select语句，就是 分别加了读锁（S锁，共享锁）和写锁（X锁，排他锁）。

```sql
mysql> select k from t where id=1 lock in share mode;
mysql> select k from t where id=1 for update;
```

再往前一步，假设事务C不是马上提交的，而是变成了下面的事务C’，会怎么样呢？ (A-1 B-3)

<img src="MYSQL实战45讲.assets/image-20250205194557380.png" alt="image-20250205194557380" style="zoom:50%;" />

事务C’的不同是，更新后并没有马上提交，在它提交前，事务B的更新语句先发起了。

前面说过 了，虽然事务C’还没提交，但是(1,2)这个版本也已经生成了，并且是当前的最新版本。

那么，事 务B的更新语句会怎么处理呢？ 

这时候，我们在上一篇文章中提到的**“两阶段锁协议”**就要上场了。

事务C’没提交，也就是说(1,2) 这个版本上的写锁还没释放。而事务B是当前读，必须要读最新版本，而且必须加锁，因此就被 锁住了，必须等到事务C’释放这个锁，才能继续它的当前读。

<img src="MYSQL实战45讲.assets/image-20250205194902332.png" alt="image-20250205194902332" style="zoom:50%;" />



到这里，我们把一致性读、当前读和行锁就串起来了。 

现在，我们再回到文章开头的问题：事务的可重复读的能力是怎么实现的？ 

可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。

**如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待**。 

而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：

- **在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图**，之后事务里的其他查询都共用这个一致性视图；
- 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。



那么，我们再看一下，在读提交隔离级别下，事务A和事务B的查询语句查到的k，分别应该是多 少呢？ 

这里需要说明一下，“start transaction with consistent snapshot; ”的意思是从这个语句开始，创 建一个持续整个事务的一致性快照。所以，在读提交隔离级别下，这个用法就没意义了，等效于 普通的start transaction。

下面是读提交时的状态图，可以看到这两个查询语句的创建视图数组的时机发生了变化，就是图 中的read view框。（注意：这里，我们用的还是事务C的逻辑直接提交，而不是事务C’）

<img src="MYSQL实战45讲.assets/image-20250205195536661.png" alt="image-20250205195536661" style="zoom:50%;" />



<img src="MYSQL实战45讲.assets/image-20250205195554358.png" alt="image-20250205195554358" style="zoom: 50%;" />



这时，事务A的查询语句的视图数组是在执行这个语句的时候创建的，时序上(1,2)、(1,3)的生成 时间都在创建这个视图数组的时刻之前。但是，在这个时刻：

-  (1,3)还没提交，属于情况1，不可见；
-  (1,2)提交了，属于情况3，可见。



所以，这时候事务A查询语句返回的是k=2。 显然地，事务B查询结果k=3。





## 小结

InnoDB的行数据有多个版本，每个数据版本有自己的rowtrx_id，每个事务或者语句有自己的一 致性视图。

普通查询语句是一致性读，一致性读会根据rowtrx_id和一致性视图确定数据版本的 可见性。

- 对于可重复读，查询只承认在事务启动前就已经提交完成的数据； 
- 对于读提交，查询只承认在语句启动前就已经提交完成的数据；

你也可以想一下，为什么表结构不支持“可重复读”？这是因为表结构没有对应的行数据，也没有 rowtrx_id，因此只能遵循当前读的逻辑。

当然，MySQL 8.0已经可以把表结构放在InnoDB字典里了，也许以后会支持表结构的可重复 读。



### 思考题

又到思考题时间了。我用下面的表结构和初始化语句作为试验环境，事务隔离级别是**可重复读**。 现在，我要把所有“字段c和id值相等的行”的c值清零，但是却发现了一个“诡异”的、改不掉的情 况。请你构造出这种情况，并说明其原理。

```sql
mysql> CREATE TABLE `t` (
`id` int(11) NOT NULL,
`c` int(11) DEFAULT NULL,
PRIMARY KEY (`id`)
) ENGINE=InnoDB;
insert into t(id, c) values(1,1),(2,2),(3,3),(4,4);

```

<img src="MYSQL实战45讲.assets/image-20250205200514535.png" alt="image-20250205200514535" style="zoom:50%;" />

复现出来以后，请你再思考一下，在实际的业务开发中有没有可能碰到这种情况？你的应用代码会不会掉进这个“坑”里，你又是怎么解决的呢？



回答：

在单前读情况下数据改变了，满足不了==的条件了。

<img src="MYSQL实战45讲.assets/image-20250205200539952.png" alt="image-20250205200539952" style="zoom:50%;" />

- 第二种是事务B早开启并提交了结果（不commit的话，A的update会等待锁）。

这个操作序列跑出来，session A看的内容也是能够复现我截图的效果的。这个session B’启动的 事务比A要早，其实是上期我们描述事务版本的可见性规则时留的彩蛋，因为规则里还有一个“活 跃事务的判断”，我是准备留到这里再补充的。



**用新的方式来分析session B’的更新为什么对session A不可见就是：在session A视图数组创建 的瞬间，session B’是活跃的，属于“版本未提交，不可见”这种情况**。（beigin/start transaction时，在第一个操作表的语句（包括select）就生成视图了）



业务中如果要绕过这类问题，@约书亚提供了一个“乐观锁”的解法，大家可以去上一篇的留言区看一下。



# 09 | 普通索引和唯一索引，应该怎么选择？

在前面的基础篇文章中，我给你介绍过索引的基本概念，相信你已经了解了唯一索引和普通索引 的区别。今天我们就继续来谈谈，在不同的业务场景下，应该选择普通索引，还是唯一索引？

假设你在维护一个市民系统，每个人都有一个唯一的身份证号，而且业务代码已经保证了不会写 入两个重复的身份证号。如果市民系统需要按照身份证号查姓名，就会执行类似这样的SQL语 句：

```sql
select name from CUser where id_card = 'xxxxxxxyyyyyyzzzzz';
```

所以，你一定会考虑在id_card字段上建索引。

由于身份证号字段比较大，我不建议你把身份证号当做主键，那么现在你有两个选择，要么给 id_card字段创建唯一索引，要么创建一个普通索引。如果业务代码已经保证了不会写入重复的 身份证号，那么这两个选择逻辑上都是正确的。



现在我要问你的是，从性能的角度考虑，你选择唯一索引还是普通索引呢？选择的依据是什么呢？



简单起见，我们还是用第4篇文章《深入浅出索引（上）》中的例子来说明，假设字段 k 上的值 都不重复。

<img src="MYSQL实战45讲.assets/image-20250206115204121.png" alt="image-20250206115204121" style="zoom:50%;" />

接下来，我们就从这两种索引对查询语句和更新语句的性能影响来进行分析。



## 查询过程

假设，执行查询的语句是 select id fromTwhere k=5。这个查询语句在索引树上查找的过程，先 是通过B+树从树根开始，按层搜索到叶子节点，也就是图中右下角的这个数据页，然后可以认 为数据页内部通过二分法来定位记录。

- 对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰 到第一个不满足k=5条件的记录。 
- 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继 续检索。

那么，这个不同带来的性能差距会有多少呢？

答案是，微乎其微。 你知道的，**InnoDB的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候， 并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。**在InnoDB中，每 个数据页的大小默认是16KB。



因为引擎是按页读写的，所以说，当找到k=5的记录的时候，它所在的数据页就都在内存里了。 那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针 寻找和一次计算。



当然，如果k=5这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下 一个数据页，这个操作会稍微复杂一些。 但是，我们之前计算过，对于整型字段，一个数据页可以放近千个key，因此出现这种情况的概 率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的CPU来说可以 忽略不计。



## 更新过程

为了说明普通索引和唯一索引对更新语句性能的影响这个问题，我需要先跟你介绍一下**change buffer**。



当需要更新一个数据页时，**如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话**，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样 就不需要从磁盘中读入这个数据页了。

在下次查询需要访问这个数据页的时候，将数据页读入内 存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。



需要说明的是，虽然名字叫作change buffer，实际上它是可以**持久化的数据**。也就是 说，change buffer在内存中有拷贝，也会被写入到磁盘上。



**将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。**除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。



显然，如果能够将更新操作先记录在change buffer，减少读磁盘，语句的执行速度会得到明显 的提升。而且，数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率。



那么，什么条件下可以使用change buffer呢？

对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400)这个记录，就要先判断现在表中是否已经存在k=4的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用change buffer 了。



因此，**唯一索引的更新就不能使用change buffer，实际上也只有普通索引可以使用**。



change buffer用的是buffer pool里的内存，因此不能无限增大。change buffer的大小，可以通 过参数`innodb_change_buffer_max_size`来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。



现在，你已经理解了change buffer的机制，那么我们再一起来看看如果要在这张表中插入一个 新记录(4,400)的话，InnoDB的处理流程是怎样的。



第一种情况是，这个记录要更新的目标页在内存中。

这时，InnoDB的处理流程如下： 

- 对于唯一索引来说，找到3和5之间的位置，判断到没有冲突，插入这个值，语句执行结束； 
- 对于普通索引来说，找到3和5之间的位置，插入这个值，语句执行结束。

 这样看来，普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU时间。



但这不是我们关注的重点。 第二种情况是，这个记录要更新的目标页不在内存中。这时，InnoDB的处理流程如下：

- 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
- 对于普通索引来说，则是将更新记录在change buffer，语句执行就结束了。

将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一。

**change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的**。



之前我就碰到过一件事儿，有个DBA的同学跟我反馈说，他负责的某个业务的库内存命中率突 然从99%降低到了75%，整个系统处于阻塞状态，更新语句全部堵住。

而探究其原因后，我发现 这个业务有大量插入数据的操作，而他在前一天把其中的某个普通索引改成了唯一索引。



## change buffer的使用场景

通过上面的分析，你已经清楚了使用change buffer对更新过程的加速作用，也清楚了change buffer只限于用在普通索引的场景下，而不适用于唯一索引。那么，现在有一个问题就是：普通 索引的所有场景，使用change buffer都可以起到加速作用吗？



因为merge的时候是真正进行数据更新的时刻，而change buffer的主要目的就是将记录的变更动作缓存下来，

所以**在一个数据页做merge之前，change buffer记录的变更越多（也就是这个页面 上要更新的次数越多），收益就越大**。



因此，**对于写多读少的业务来说**，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。

这种业务模型常见的就是**账单类、日志类**的系统。



反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记 录在change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问 IO的次数不会减少，反而增加了change buffer的维护代价。所以，对于这种业务模式来说，change buffer反而起到了副作用。



## 索引选择和实践

回到我们文章开头的问题，普通索引和唯一索引应该怎么选择。

其实，这两类索引在查询能力上 是没差别的，主要考虑的是对更新性能的影响。所以，我**建议你尽量选择普通索引**。

**如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭change buffer。**而在 其他情况下，change buffer都能提升更新性能。

在实际使用中，你会发现，普通索引和change buffer的配合使用，对于数据量大的表的更新优 化还是很明显的。



特别地，在使用机械硬盘时，change buffer这个机制的收效是非常显著的。所以，当你有一个 类似“历史数据”的库，并且出于成本考虑用的是机械硬盘时，那你应该特别关注这些表里的索 引，尽量使用普通索引，然后把change buffer 尽量开大，以确保这个“历史数据”表的数据写入 速度。



## change buffer 和 redo log

理解了change buffer的原理，你可能会联想到我在前面文章中和你介绍过的redo log和WAL。

在前面文章的评论中，我发现有同学混淆了redo log和change buffer。WAL 提升性能的核心机 制，也的确是尽量减少随机读写，这两个概念确实容易混淆。所以，这里我把它们放到了同一个 流程里来说明，便于你区分这两个概念

> 备注：这里，你可以再回顾下第2篇文章《日志系统：一条SQL更新语句是如何执行的？》中 的相关内容。



现在，我们要在表上执行这个插入语句：

```
mysql> insert into t(id,k) values(id1,k1),(id2,k2);
```

这里，我们假设当前k索引树的状态，查找到位置后，k1所在的数据页在内存(InnoDB buffer pool)中，k2所在的数据页不在内存中。如图2所示是带change buffer的更新状态图。

<img src="MYSQL实战45讲.assets/image-20250206120816268.png" alt="image-20250206120816268" style="zoom:67%;" />



分析这条更新语句，你会发现它涉及了四个部分：内存、redo log（ib_log_fileX）、 数据表空间 （t.ibd）、系统表空间（ibdata1）。

这条更新语句做了如下的操作（按照图中的数字顺序）：

1. Page 1在内存中，直接更新内存；
2. Page 2没有在内存中，就在内存的change buffer区域，记录下“我要往Page 2插入一行”这个 信息
3. 将上述两个动作记入redo log中（图中3和4）。

做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘（写redo log，两次操作合在一起写了一次磁盘），而且还是顺序写的。



同时，图中的两个虚线箭头，是后台操作，不影响更新的响应时间。



那在这之后的读请求，要怎么处理呢？ 

比如，我们现在要执行 select *fromt where k in (k1, k2)。这里，我画了这两个读请求的流程 图。

如果读语句发生在更新语句后不久，内存中的数据都还在，那么此时的这两个读操作就与系统表 空间（ibdata1）和 redo log（ib_log_fileX）无关了。所以，我在图中就没画出这两部分。



<img src="MYSQL实战45讲.assets/image-20250206121123774.png" alt="image-20250206121123774" style="zoom:50%;" />



从图中可以看到：

1. 读Page 1的时候，直接从内存返回。有几位同学在前面文章的评论中问到，WAL之后如果 读数据，是不是一定要读盘，是不是一定要从redo log里面把数据更新以后才可以返回？其 实是不用的。你可以看一下图3的这个状态，虽然磁盘上还是之前的数据，但是这里直接从 内存返回结果，结果是正确的。
2. 要读Page 2的时候，需要把Page 2从磁盘读入内存中，然后应用change buffer里面的操作日志，生成一个正确的版本并返回结果。



可以看到，直到需要读Page 2的时候，这个数据页才会被读入内存。

 所以，如果要简单地对比这两个机制在提升更新性能上的收益的话，

- redo log 主要节省的是随机写磁盘的IO消耗（转成顺序写）
- 而change buffer主要节省的则是随机读磁盘的IO消耗。（减少读次数）



## 小结

今天，我从普通索引和唯一索引的选择开始，和你分享了数据的查询和更新过程，然后说明了 change buffer的机制以及应用场景，最后讲到了索引选择的实践。

由于唯一索引用不上change buffer的优化机制，因此如果业务可以接受，从性能角度出发我建 议你优先考虑非唯一索引。



### 思考题

最后，又到了思考题时间。 通过图2你可以看到，change buffer一开始是写内存的，那么如果这个时候机器掉电重启，会不 会导致change buffer丢失呢？change buffer丢失可不是小事儿，再从磁盘读入数据可就没有了 merge过程，就等于是数据丢失了。会不会出现这种情况呢？



你可以把你的思考和观点写在留言区里，我会在下一篇文章的末尾和你讨论这个问题。



回答：

我在上一篇文章最后留给你的问题是，如果某次写入使用了change buffer机制，之后主机异常 重启，是否会丢失change buffer和数据。 

这个问题的答案是**不会丢失**，留言区的很多同学都回答对了。虽然是只更新内存，但是在事务提 交的时候，我们把change buffer的操作也记录到redo log里了，所以崩溃恢复的时候，change buffer也能找回来。 

在评论区有同学问到，merge的过程是否会把数据直接写回磁盘，这是个好问题。这里，我再为你分析一下。

merge的执行流程是这样的： 

1. 从磁盘读入数据页到内存（老版本的数据页）； 
2. 从change buffer里找出这个数据页的change buffer 记录(可能有多个），依次应用，得到新版数据页； 
3. **写redo log**。这个redo log包含了数据的变更和change buffer的变更。

到这里merge过程就结束了。这时候，数据页和内存中change buffer对应的磁盘位置都还没有修 改，属于脏页，之后各自刷回自己的物理数据，就是另外一个过程了。



### 补充

评论区大家对“是否使用唯一索引”有比较多的讨论，主要是纠结在“业务可能无法确保”的情况。 这里，我再说明一下：

- 首先，业务正确性优先。咱们这篇文章的前提是“业务代码已经保证不会写入重复数据”的情况 下，讨论性能问题。如果业务不能保证，或者业务就是要求数据库来做约束，那么没得选， 必须创建唯一索引。这种情况下，本篇文章的意义在于，如果碰上了大量插入数据慢、内存 命中率低的时候，可以给你多提供一个排查思路。 
- 然后，在一些“归档库”的场景，你是可以考虑使用唯一索引的。比如，线上数据只需要保留半 年，然后历史数据保存在归档库。这时候，归档数据已经是确保没有唯一键冲突了。要提高归档效率，可以考虑把表里面的唯一索引改成普通索引。



# 10 | MySQL为什么有时候会选错索引？



前面我们介绍过索引，你已经知道了在MySQL中一张表其实是可以支持多个索引的。但是，你 写SQL语句的时候，并没有主动指定使用哪个索引。也就是说，使用哪个索引是由MySQL来确定的。 

不知道你有没有碰到过这种情况，一条本来可以执行得很快的语句，却由于MySQL选错了索 引，而导致执行速度变得很慢？



我们一起来看一个例子吧。 我们先建一个简单的表，表里有a、b两个字段，并分别建上索引：

```sql
CREATE TABLE `t` (
`id` int(11) NOT NULL,
`a` int(11) DEFAULT NULL,
`b` int(11) DEFAULT NULL,
PRIMARY KEY (`id`),
KEY `a` (`a`),
KEY `b` (`b`)
) ENGINE=InnoDB;

```

然后，我们往表t中插入10万行记录，取值按整数递增，即：(1,1,1)，(2,2,2)，(3,3,3) 直到 (100000,100000,100000)。 我是用存储过程来插入数据的，这里我贴出来方便你复现：

```sql
delimiter ;;
create procedure idata()
begin
    declare i int;
    set i=1;
    while(i<=100000) do
        insert into t values(i, i, i);
        set i=i+1;
    end while;
end;;
delimiter ;
call idata();

```



接下来，我们分析一条SQL语句：

```
mysql> select * from t where a between 10000 and 20000;
```

你一定会说，这个语句还用分析吗，很简单呀，a上有索引，肯定是要使用索引a的。 你说得没错，图1显示的就是使用explain命令看到的这条语句的执行情况。

<img src="MYSQL实战45讲.assets/image-20250207194130079.png" alt="image-20250207194130079" style="zoom: 67%;" />



从图1看上去，这条查询语句的执行也确实符合预期，**key这个字段值是’a’**，表示优化器选择了索引a。

 不过别急，这个案例不会这么简单。在我们已经准备好的包含了10万行数据的表上，我们再做 如下操作。

<img src="MYSQL实战45讲.assets/image-20250207194253237.png" alt="image-20250207194253237" style="zoom:50%;" />



```
start transaction with consistent snapshot;
   -- delete from t;
   -- call idata();
   -- explain select * from t where a between 10000 and 20000;
   
commit;
```



这里，session A的操作你已经很熟悉了，它就是开启了一个事务。随后，session B把数据都删 除后，又调用了 idata这个存储过程，插入了10万行数据。



这时候，session B的查询语句select * fromt where a between 10000 and 20000就不会再选择索引a了。我们可以通过慢查询日志（slowlog）来查看一下具体的执行情况。

```
SELECT * FROM mysql.slow_log;
```

为了说明优化器选择的结果是否正确，我增加了一个对照，即：使用`force index(a)`来让优化器强 制使用索引a（这部分内容，我还会在这篇文章的后半部分中提到）。

下面的三条SQL语句，就是这个实验过程。

```
set long_query_time=0;
select * from t where a between 10000 and 20000; /*Q1*/
select * from t force index(a) where a between 10000 and 20000;/*Q2*/
```

- 第一句，是将慢查询日志的阈值设置为0，表示这个线程接下来的语句都会被记录入慢查询日 志中； 
- 第二句，Q1是session B原来的查询； 
- 第三句，Q2是加了force index(a)来和session B原来的查询语句执行情况对比。

如图3所示是这三条SQL语句执行完成后的慢查询日志。

<img src="MYSQL实战45讲.assets/image-20250207195823573.png" alt="image-20250207195823573" style="zoom: 67%;" />

可以看到，Q1扫描了10万行，显然是走了全表扫描，执行时间是40毫秒。Q2扫描了10001行， 执行了21毫秒。也就是说，我们在没有使用force index的时候，MySQL用错了索引，导致了更长的执行时间。

**(mysql 8.0 没有复现，可能是新版本给优化了)**

![image-20250207201651164](MYSQL实战45讲.assets/image-20250207201651164.png)

这个例子对应的是我们平常不断地删除历史数据和新增数据的场景。这时，MySQL竟然会选错 索引，是不是有点奇怪呢？今天，我们就从这个奇怪的结果说起吧。

## 优化器的逻辑

在第一篇文章中，我们就提到过，选择索引是优化器的工作。

而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库 里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越 少，消耗的CPU资源越少。

当然，扫描行数并不是唯一的判断标准，优化器还会结合是否使用**临时表**、**是否排序**等因素进行 综合判断。

我们这个简单的查询语句并没有涉及到临时表和排序，所以MySQL选错索引肯定是在判断扫描 行数的时候出问题了。

那么，问题就是：扫描行数是怎么判断的？

MySQL在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而**只能根据统计信息来估算记录数。**



这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越 好。而一个索引上不同的值的个数，我们称之为**“基数”（cardinality）**。也就是说，这个基数越 大，索引的区分度越好。（索引选择性）



我们可以使用show index方法，看到一个索引的基数。如图4所示，就是表t的show index的结果 。虽然这个表的每一行的三个字段值都是一样的，但是在统计信息中，这三个索引的基数值并不 同，而且其实都不准确。

```
show index from t;
```

![image-20250207201950352](MYSQL实战45讲.assets/image-20250207201950352.png)

那么，MySQL是怎样得到索引的基数的呢？这里，我给你简单介绍一下MySQL**采样统计**的方法。



为什么要采样统计呢？因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太 高了，所以只能选择“采样统计”。



采样统计的时候，**InnoDB默认会选择N个数据页，统计这些页面上的不同值**，得到一个平均 值，然后乘以这个索引的页面数，就得到了这个索引的基数。

而数据表是会持续更新的，索引统计信息也不会固定不变。所以，**当变更的数据行数超过1/M的时候**，会自动触发重新做一次索引统计。



在MySQL中，有两种存储索引统计的方式，可以通过设置参数`innodb_stats_persistent`的值来选 择：

- 设置为on的时候，表示统计信息会持久化存储。这时，默认的N是20，M是10。 
- 设置为off的时候，表示统计信息只存储在内存中。这时，默认的N是8，M是16。



由于是采样统计，所以不管N是20还是8，这个基数都是很容易不准的。 但，这还不是全部。



你可以从图4中看到，这次的索引统计值（cardinality列）虽然不够精确，但大体上还是差不多 的，选错索引一定还有别的原因。



其实索引统计只是一个输入，对于一个具体的语句来说，**优化器还要判断，执行这个语句本身要扫描多少行**。



接下来，我们再一起看看优化器预估的，这两个语句的扫描行数是多少。**rows**这个字段表示的是预计扫描行数。



![image-20250207202318317](MYSQL实战45讲.assets/image-20250207202318317.png)





其中，Q1的结果还是符合预期的，rows的值是104620；但是Q2的rows值是37116，偏差就大 了。而图1中我们用explain命令看到的rows是只有10001行，是这个偏差误导了优化器的判断。



到这里，可能你的第一个疑问不是为什么不准，而是优化器为什么放着扫描37000行的执行计划 不用，却选择了扫描行数是100000的执行计划呢？



这是因为，如果使用索引a，每次从索引a上拿到一个值，都要回到主键索引上查出整行数据， 这个代价优化器也要算进去的。而如果选择扫描10万行，是直接在主键索引上扫描的，没有额外的代价。

优化器会估算这两个选择的代价，从结果看来，优化器认为直接扫描主键索引更快。当然，从执 行时间看来，这个选择并不是最优的。



使用普通索引需要把回表的代价算进去，在图1执行explain的时候，也考虑了这个策略的代价 ， 但图1的选择是对的。也就是说，这个策略并没有问题。



所以冤有头债有主，MySQL选错索引，这件事儿还得归咎到没能准确地判断出扫描行数。至于 为什么会得到错误的扫描行数，这个原因就作为课后问题，留给你去分析了。



既然是统计信息不对，那就修正。`analyze table t` 命令，可以用来重新统计索引信息。我们来看 一下执行效果。

<img src="MYSQL实战45讲.assets/image-20250207202606343.png" alt="image-20250207202606343" style="zoom:50%;" />

这回对了。

所以在实践中，如果你发现explain的结果预估的rows值跟实际情况差距比较大，可以采用这个 方法来处理。



其实，如果只是索引统计不准确，通过analyze命令可以解决很多问题，但是前面我们说了，优 化器可不止是看扫描行数。

依然是基于这个表t，我们看看另外一个语句：

```
mysql> select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1;
```

从条件上看，这个查询没有符合条件的记录，因此会返回空集合。 在开始执行这条语句之前，你可以先设想一下，如果你来选择索引，会选择哪一个呢？



为了便于分析，我们先来看一下a、b这两个索引的结构图。

<img src="MYSQL实战45讲.assets/image-20250207202829883.png" alt="image-20250207202829883" style="zoom:50%;" />

如果使用索引a进行查询，那么就是扫描索引a的前1000个值，然后取到对应的id，再到主键索 引上去查出每一行，然后根据字段b来过滤。显然这样需要扫描1000行。



如果使用索引b进行查询，那么就是扫描索引b的最后50001个值，与上面的执行过程相同，也是 需要回到主键索引上取值再判断，所以需要扫描50001行。



所以你一定会想，如果使用索引a的话，执行速度明显会快很多。那么，下面我们就来看看到底 是不是这么一回事儿。

图8是执行explain的结果。

![image-20250207202912863](MYSQL实战45讲.assets/image-20250207202912863.png)



可以看到，返回结果中key字段显示，这次优化器选择了索引b，而rows字段显示需要扫描的行 数是50198。

从这个结果中，你可以得到两个结论： 1. 扫描行数的估计值依然不准确； 2. 这个例子里MySQL又选错了索引。



## 索引选择异常和处理

一种方法是，像我们第一个例子一样，采用force index强行选择一个索引。MySQL会根据 词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需 要扫描多少行。如果force index指定的索引在候选索引列表中，就直接选择这个索引，不再评估 其他索引的执行代价。



我们来看看第二个例子。刚开始分析时，我们认为选择索引a会更好。现在，我们就来看看执行 效果：

![image-20250208192653031](MYSQL实战45讲.assets/image-20250208192653031.png)



可以看到，原本语句需要执行2.23秒，而当你使用force index(a)的时候，只用了0.05秒，比优化 器的选择快了40多倍。



也就是说，优化器没有选择正确的索引，force index起到了“矫正”的作用。



不过很多程序员不喜欢使用force index，一来这么写不优美，二来如果索引改了名字，这个语句 也得改，显得很麻烦。而且如果以后迁移到别的数据库的话，这个语法还可能会不兼容。

但其实使用force index最主要的问题还是变更的及时性。因为选错索引的情况还是比较少出现 的，所以开发的时候通常不会先写上force index。而是等到线上出现问题的时候，你才会再去修 改SQL语句、加上force index。但是修改之后还要测试和发布，对于生产系统来说，这个过程不 够敏捷。

所以，数据库的问题最好还是在数据库内部来解决。那么，在数据库里面该怎样解决呢？



既然优化器放弃了使用索引a，说明a还不够合适，所以第二种方法就是，我们可以考虑修改 语句，引导MySQL使用我们期望的索引。比如，在这个例子里，显然把“order byb limit 1” 改 成 “order byb,a limit 1” ，语义的逻辑是相同的。

我们来看看改之后的效果：

![image-20250208192837078](MYSQL实战45讲.assets/image-20250208192837078.png)



之前优化器选择使用索引b，是因为它认为使用索引b可以避免排序（b本身是索引，已经是有序 的了，如果选择索引b的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为 代价更小。 

现在order byb,a 这种写法，要求按照b,a排序，就意味着使用这两个索引都需要排序。因此，扫 描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描1000行的索引a。



当然，这种修改并不是通用的优化手段，只是刚好在这个语句里面有limit 1，因此如果有满足条 件的记录， order byb limit 1和order byb,a limit 1 都会返回b是最小的那一行，逻辑上一致，才 可以这么做。

如果你觉得修改语义这件事儿不太好，这里还有一种改法，图11是执行效果。



```sql
mysql> select * from (select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 100) alias limit 1;
```

![image-20250208192950571](MYSQL实战45讲.assets/image-20250208192950571.png)

在这个例子里，我们用limit 100让优化器意识到，使用b索引代价是很高的。其实是我们根据数 mysql> select * from (select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 100)alias limit 1; 据特征诱导了一下优化器，也不具备通用性。



第三种方法是，在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选 择，或删掉误用的索引。



不过，在这个例子中，我没有找到通过新增索引来改变优化器行为的方法。这种情况其实比较 少，尤其是经过DBA索引优化过的库，再碰到这个bug，找到一个更合适的索引一般比较难。

如果我说还有一个方法是删掉索引b，你可能会觉得好笑。但实际上我碰到过两次这样的例子， 最终是DBA跟业务开发沟通后，发现这个优化器错误选择的索引其实根本没有必要存在，于是 就删掉了这个索引，优化器也就重新选择到了正确的索引。



## 小结

今天我们一起聊了聊索引统计的更新机制，并提到了优化器存在选错索引的可能性。

对于由于索引统计信息不准确导致的问题，你可以用analyze table来解决。

而对于其他优化器误判的情况，你可以在应用端用force index来强行指定索引，也可以通过修改 语句来引导优化器，还可以通过增加或者删除索引来绕过这个问题。



你可能会说，今天这篇文章后面的几个例子，怎么都没有展开说明其原理。我要告诉你的是，今 天的话题，我们面对的是MySQL的bug，每一个展开都必须深入到一行行代码去量化，实在不是 我们在这里应该做的事情。



所以，我把我用过的解决方法跟你分享，希望你在碰到类似情况的时候，能够有一些思路。



你平时在处理MySQL优化器bug的时候有什么别的方法，也发到评论区分享一下吧。



### 思考题

最后，我给你留下一个思考题。前面我们在构造第一个例子的过程中，通过session A的配合， 让session B删除数据后又重新插入了一遍数据，然后就发现explain结果中，rows字段从10001 变成37000多。

而如果没有session A的配合，只是单独执行delete fromt 、call idata()、explain这三句话，会看 到rows字段其实还是10000左右。你可以自己验证一下这个结果。 这是什么原因呢？也请你分析一下吧。



回答：

上篇文章中的第一个例子，评论区有几位同学说没有复现，大家要检查一下隔离级别是不是 RR（Repeatable Read，可重复读），创建的表t是不是InnoDB引擎。我把复现过程做成了一 个视频，供你参考。



在上一篇文章最后，我给你留的问题是，为什么经过这个操作序列，explain的结果就不对了？这 里，我来为你分析一下原因。

delete 语句删掉了所有的数据，然后再通过call idata()插入了10万行数据，看上去是覆盖了原来 的10万行。

但是，session A开启了事务并没有提交，所以之前插入的10万行数据是不能删除的。这样，之 前的数据每一行数据都有两个版本，旧版本是delete之前的数据，新版本是标记为deleted的数 据。

这样，索引a上的数据其实就有两份。



然后你会说，不对啊，主键上的数据也不能删，那没有使用force index的语句，使用explain命令 看到的扫描行数为什么还是100000左右？（潜台词，如果这个也翻倍，也许优化器还会认为选 字段a作为索引更合适）



是的，不过这个是主键，主键是直接按照表的行数来估计的。而表的行数，优化器直接用的是 show table status的值。

这个值的计算方法，我会在后面有文章为你详细讲解

![image-20250208193839291](MYSQL实战45讲.assets/image-20250208193839291.png)



# 11 | 怎么给字符串字段加索引？

现在，几乎所有的系统都支持邮箱登录，如何在邮箱这样的字段上建立合理的索引，是我们今天要讨论的问题。

假设，你现在维护一个支持邮箱登录的系统，用户表是这么定义的：

```sql
mysql> create table SUser(
ID bigint unsigned primary key,
email varchar(64),
...
)engine=innodb;

```

由于要使用邮箱登录，所以业务代码中一定会出现类似于这样的语句：

```
mysql> select f1, f2 from SUser where email='xxx';
```

如果email这个字段上没有索引，那么这个语句就只能做全表扫描。



## 前缀索引

同时，**MySQL是支持前缀索引的，也就是说，你可以定义字符串的一部分作为索引**。

默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串.

比如，这两个在email字段上创建索引的语句：

```mysql
mysql> alter table SUser add index index1(email);
或
mysql> alter table SUser add index index2(email(6));
```

第一个语句创建的index1索引里面，包含了每个记录的整个字符串；而第二个语句创建的index2索引里面，对于每个记录都是只取前6个字节。

那么，这两种不同的定义在数据结构和存储上有什么区别呢？如图2和3所示，就是这两个索引的示意图。

<img src="MYSQL实战45讲.assets/image-20240612200303046.png" alt="image-20240612200303046" style="zoom: 33%;" />

<img src="MYSQL实战45讲.assets/image-20240612200316004.png" alt="image-20240612200316004" style="zoom:50%;" />



从图中你可以看到，由于email(6)这个索引结构中每个邮箱字段都只取前6个字节（即：zhangs），所以**占用的空间会更小**，这就是使用前缀索引的优势。

但，这同时带来的损失是，**可能会增加额外的记录扫描次数**。

接下来，我们再看看下面这个语句，在这两个索引定义下分别是怎么执行的。

```sql
select id,name,email from SUser where email='zhangssxyz@xxx.com';
```

如果使用index1全字符索引，，执行顺序是这样的：

1. 从index1索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得ID2的值；

2. 到主键上查到主键值是ID2的行，判断email的值是正确的，将这行记录加入结果集；

3. 取index1索引树上刚刚查到的位置的下一条记录，发现已经不满足email='zhangssxyz@xxx.com’的条件了，循环结束。

这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。



如果使用index2前缀索引，，执行顺序是这样的：

1. 从index2索引树找到满足索引值是’zhangs’的记录，找到的第一个是ID1；

2. 到主键上查到主键值是ID1的行，判断出email的值不是’zhangssxyz@xxx.com’，这行记录丢

弃；

3. 取index2上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出ID2，再到ID索引上取

整行然后判断，这次值对了，将这行记录加入结果集；

4. 重复上一步，直到在idxe2上取到的值不是’zhangs’时，循环结束。

在这个过程中，要回主键索引取4次数据，也就是扫描了4行。通过这个对比，你很容易就可以发现，使用前缀索引后，可能会导致查询语句读数据的次数变

多。

但是，对于这个查询语句来说，如果你定义的index2不是email(6)而是email(7），也就是说取email字段的前7个字节来构建索引的话，即满足前缀’zhangss’的记录只有一个，也能够直接查到ID2，只扫描一行就结束了。



也就是说使用前缀索引，**定义好长度，就可以做到既节省空间，又不用额外增加太多查询成本（增加查询次数）。**



于是，你就有个问题：**当要给字符串创建前缀索引时，有什么方法能够确定我应该使用多长的前缀呢？**

实际上，我们在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的 键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。



首先，你可以使用下面这个语句，算出这个列上有多少个不同的值：

```sql
mysql> select count(distinct email) as L from SUser;
```



然后，依次选取不同长度的前缀来看这个值，比如我们要看一下4~7个字节的前缀索引，可以用left函数，用下面这个语句：

```sql
mysql> select
    count(distinct left(email,4)) as L4,
    count(distinct left(email,5))as L5,
    count(distinct left(email,6))as L6,
    count(distinct left(email,7))as L7
from SUser;  
```

当然，使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如 5%。然后，在返回的L4~L7中，找出不小于 L * 95%的值，假设这里L6、L7都满足，你就可以 选择前缀长度为6。



## 前缀索引对覆盖索引的影响

> 可能用不了覆盖索引

前面我们说了使用前缀索引可能会增加扫描行数，这会影响到性能。其实，前缀索引的影响不止如此，我们再看一下另外一个场景。

你先来看看这个SQL语句：

```sql
select id,email from SUser where email='zhangssxyz@xxx.com';
```

与前面例子中的SQL语句

```
select id,name,email from SUser where email='zhangssxyz@xxx.com';
```

相比，这个语句只要求返回id和email字段。



所以，如果使用index1（即email整个字符串的索引结构）的话，可以利用覆盖索引，从index1查 到结果后直接就返回了，不需要回到ID索引再去查一次。而如果使用index2（即email(6)索引结 构）的话，就不得不回到ID索引再去判断email字段的值。

即使你将index2的定义修改为email(18)的前缀索引，这时候虽然index2已经包含了所有的信息， 但InnoDB还是要回到id索引再查一下，因为系统并不确定前缀索引的定义是否截断了完整信息。



也就是说，使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀 索引时需要考虑的一个因素。



## 其他方式

对于类似于邮箱这样的字段来说，使用前缀索引的效果可能还不错。但是，遇到前缀的区分度不够好的情况时，我们要怎么办呢？

比如，我们国家的身份证号，一共18位，其中前6位是地址码，所以同一个县的人的身份证号前 6位一般会是相同的。

假设你维护的数据库是一个市的公民信息系统，这时候如果对身份证号做长度为6的前缀索引的 话，这个索引的区分度就非常低了。

按照我们前面说的方法，可能你需要创建长度为12以上的前缀索引，才能够满足区分度要求。 但是，索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的 效率也就会越低。



那么，**如果我们能够确定业务需求里面只有按照身份证进行等值查询的需求，还有没有别的处理方法呢？**这种方法，既可以占用更小的空间，也能达到相同的查询效率。

答案是，有的。

### 倒序存储和hash字段

> 倒序存储

第一种方式是使用倒序存储。如果你存储身份证号的时候把它倒过来存，每次查询的时候，你 可以这么写：

```
mysql> select field_list from t where id_card = reverse('input_id_card_string');
```

由于身份证号的最后6位没有地址码这样的重复逻辑，所以最后这6位很可能就提供了足够的区 分度。当然了，实践中你不要忘记使用count(distinct)方法去做个验证。



> hash字段

第二种方式是使用hash字段。你可以在表上再创建一个整数字段，来保存身份证的校验码， 同时在这个字段上创建索引。

```sql
mysql> alter table t add id_card_crc int unsigned, add index(id_card_crc);
```

然后每次插入新记录的时候，都同时用crc32()这个函数得到校验码填到这个新字段。由于校验码 可能存在冲突，也就是说两个不同的身份证号通过crc32()函数得到的结果可能是相同的，所以你 的查询语句where部分要判断id_card的值是否精确相同。

```sql
mysql> select field_list from t where id_card_crc=crc32('input_id_card_string') and id_card='input_id_card_string'
```



这样，索引的长度变成了4个字节，比原来小了很多。

接下来，我们再一起看看使用倒序存储和使用hash字段这两种方法的异同点。



首先，它们的相同点是，都不支持范围查询。倒序存储的字段上创建的索引是按照倒序字符串的 方式排序的，已经没有办法利用索引方式查出身份证号码在[ID_X, ID_Y]的所有市民了。同样 地，hash字段的方式也只能支持等值查询。



它们的区别，主要体现在以下三个方面：



1. 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而hash字 段方法需要增加一个字段。当然，倒序存储方式使用4个字节的前缀长度应该是不够的，如 果再长一点，这个消耗跟额外这个hash字段也差不多抵消了。
2. 在CPU消耗方面，倒序方式每次写和读的时候，都需要额外调用一次reverse函数，而hash 字段的方式需要额外调用一次crc32()函数。如果只从这两个函数的计算复杂度来看的 话，reverse函数额外消耗的CPU资源会更小些。
3. 从查询效率上看，使用hash字段方式的查询性能相对更稳定一些。因为crc32算出来的值虽 然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近1。而倒序存储 方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。



## 小结

在今天这篇文章中，我跟你聊了聊字符串字段创建索引的场景。我们来回顾一下，你可以使用的方式有： 

1. 直接创建完整索引，这样可能比较占用空间； 
2. 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引； 
3. 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；
4. 创建hash字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。

在实际应用中，你要根据业务字段的特点选择使用哪种方式。

### 思考题

 好了，又到了最后的问题时间。

如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是”学号@gmail.com", 而学号 的规则是：十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、**第七位到第十位是入学年份、最后五位是顺序编号**。



系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验 证这个行为的话，你会怎么设计这个登录名的索引呢



回答：

由于这个学号的规则，无论是正向还是反向的前缀索引，重复度都比较高。

因为维护的只是一个 学校的，因此前面6位（其中，前三位是所在城市编号、第四到第六位是学校编号）其实是固定 的，邮箱后缀都是@gamil.com，因此可以只存入学年份加顺序编号，它们的长度是9位。 而其实在此基础上，可以用数字类型来存这9位数字。比如201100001，这样只需要占4个字 节。

其实这个就是一种hash，只是它用了最简单的转换规则：字符串转数字的规则，而刚好我 们设定的这个背景，可以保证这个转换后结果的唯一性。



评论区中，也有其他一些很不错的见解。 评论用户@封建的风 说，一个学校的总人数这种数据量，50年才100万学生，这个表肯定是小 表。为了业务简单，直接存原来的字符串。这个答复里面包含了“优化成本和收益”的思想，我觉 得值得at出来。 

@小潘 同学提了另外一个极致的方向。如果碰到表数据量特别大的场景，通过这种方式的收益 很不错的。



# 12 | 为什么我的MySQL会“抖”一下？

平时的工作中，不知道你有没有遇到过这样的场景，一条SQL语句，正常执行的时候特别快，但 是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持 续时间还很短。

看上去，这就像是数据库“抖”了一下。今天，我们就一起来看一看这是什么原因。



## 你的SQL语句为什么变“慢”了



在前面第2篇文章《日志系统：一条SQL更新语句是如何执行的？》中，我为你介绍了WAL机 制。

> MySQL 中的 WAL（Write-Ahead Logging）机制是一种用于事务日志的持久化和恢复的技术。它是一种在写入数据之前先写入日志的方法，以确保数据的持久性和一致性。
>
> WAL 机制的工作原理如下：
>
> 1. 在执行事务操作之前，MySQL 将事务的修改操作写入一个称为 WAL 日志的特殊文件中，而不是直接写入磁盘上的数据文件。
> 2. WAL 日志记录了事务的每个修改操作，包括插入、更新和删除等。
> 3. 当事务提交时，MySQL 将修改操作应用到磁盘上的数据文件，以确保数据的持久性。
> 4. 如果系统崩溃或发生故障，MySQL 可以使用 WAL 日志来恢复数据的一致性。通过重新执行 WAL 日志中的操作，可以将数据文件恢复到崩溃之前的状态。
>
> WAL 机制的优点包括：
>
> - 提高了数据库的写入性能，因为数据的写入是顺序写入 WAL 日志文件，而不是随机写入数据文件。
> - 提供了更好的事务持久性和一致性，即使在系统崩溃或故障的情况下也能够恢复数据。
> - 减少了数据文件的随机写入，从而减少了磁盘 I/O 操作，提高了数据库的整体性能。



现在你知道了，InnoDB在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志 叫作redo log（重做日志），也就是《孔乙己》里咸亨酒店掌柜用来记账的粉板，在更新内存写 完redo log后，就返回给客户端，本次更新成功。



**做下类比的话，掌柜记账的账本是数据文件，记账用的粉板是日志文件（redo log），掌柜的记 忆就是内存**。

掌柜总要找时间把账本更新一下，这对应的就是把内存里的数据写入磁盘的过程，术语就是 flush。在这个flush操作执行之前，孔乙己的赊账总额，其实跟掌柜手中账本里面的记录是不一 致的。因为孔乙己今天的赊账金额还只在粉板上，而账本里的记录是老的，还没把今天的赊账算 进去。



当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写 入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。



不论是脏页还是干净页，都在内存中。在这个例子里，内存对应的就是掌柜的记忆。

接下来，我们用一个示意图来展示一下“孔乙己赊账”的整个操作过程。假设原来孔乙己欠账10 文，这次又要赊9文。

<img src="MYSQL实战45讲.assets/image-20250211182129265.png" alt="image-20250211182129265" style="zoom:50%;" />



回到文章开头的问题，你不难想象，平时执行很快的更新操作，其实就是在写内存和日志，而 **MySQL偶尔“抖”一下的那个瞬间，可能就是在刷脏页（flush）**。

那么，什么情况会引发数据库的flush过程呢？



我们还是继续用咸亨酒店掌柜的这个例子，想一想：掌柜在什么情况下会把粉板上的赊账记录改 到账本上？

1、第一种场景是，粉板满了，记不下了。这时候如果再有人来赊账，掌柜就只得放下手里的活 儿，将粉板上的记录擦掉一些，留出空位以便继续记账。当然在擦掉之前，他必须先将正确 的账目记录到账本中才行。

这个场景，对应的就是**InnoDB的redo log写满了**。这时候系统会停止所有更新操作，把 checkpoint往前推进，redo log留出空间可以继续写。我在第二讲画了一个redo log的示意 图，这里我改成环形，便于大家理解。

<img src="MYSQL实战45讲.assets/image-20250211182250951.png" alt="image-20250211182250951" style="zoom:50%;" />

checkpoint（更新点）可不是随便往前修改一下位置就可以的。比如图2中，把checkpoint位置从CP推进到 CP’，就需要将两个点之间的日志（浅绿色部分），对应的所有脏页都flush到磁盘上。之后，图 中从write pos到CP’之间就是可以再写入的redo log的区域。



2、第二种场景是，这一天生意太好，要记住的事情太多，掌柜发现自己快记不住了，赶紧找出 账本把孔乙己这笔账先加进去。

这种场景，对应的就是系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰 一些数据页，空出内存给别的数据页使用。

**如果淘汰的是“脏页”，就要先将脏页写到磁盘**。 你一定会说，这时候难道不能直接把内存淘汰掉，下次需要请求的时候，从磁盘读入数据 页，然后拿redo log出来应用不就行了？这里其实是从性能考虑的。如果刷脏页一定会写盘， 就保证了每个数据页有两种状态：

- 一种是内存里存在，内存里就肯定是正确的结果，直接返回； 
- 另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。 这样的效率最高。



3、第三种场景是，生意不忙的时候，或者打烊之后。这时候柜台没事，掌柜闲着也是闲着，不 如更新账本。

这种场景，对应的就是**MySQL认为系统“空闲”的时候**。当然，MySQL“这家酒店”的生意好起 来可是会很快就能把粉板记满的，所以“掌柜”要合理地安排时间，即使是“生意好”的时候，也 要见缝插针地找时间，只要有机会就刷一点“脏页”。



4、第四种场景是，年底了咸亨酒店要关门几天，需要把账结清一下。这时候掌柜要把所有账都 记到账本上，这样过完年重新开张的时候，就能就着账本明确账目情况了

这种场景，对应的就是**MySQL正常关闭的情况**。这时候，MySQL会把内存的脏页都flush到磁 盘上，这样下次MySQL启动的时候，就可以直接从磁盘上读数据，启动速度会很快。



接下来，你可以分析一下上面四种场景对性能的影响。 其中，第三种情况是属于MySQL空闲时的操作，这时系统没什么压力，而第四种场景是数据库 本来就要关闭了。这两种情况下，你不会太关注“性能”问题。

所以这里，我们主要来分析一下前 两种场景下的**性能问题**。



第一种是“redo log写满了，要flush脏页”，这种情况是InnoDB要尽量避免的。因为出现这种情况 的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更 新数会跌为0。



第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。InnoDB用缓冲池 （buffer pool）管理内存，缓冲池中的内存页有三种状态：

- 第一种是，还没有使用的； 
- 第二种是，使用了并且是干净页；
- 第三种是，使用了并且是脏页。



InnoDB的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。

而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久 不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是 脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。



所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：

1. 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；

2. 日志写满，更新全部堵住，写性能跌为0，这种情况对敏感业务来说，是不能接受的。

所以，InnoDB需要有控制脏页比例的机制，来尽量避免上面的这两种情况。



## InnoDB刷脏页的控制策略

接下来，我就来和你说说InnoDB脏页的控制策略，以及和这些策略相关的参数。

首先，你要正确地告诉InnoDB所在主机的IO能力，这样InnoDB才能知道需要全力刷脏页的时候，可以刷多快。

这就要用到innodb_io_capacity这个参数了，它会告诉InnoDB你的磁盘能力。这个值我建议你设置成磁盘的**IOPS**。磁盘的IOPS可以通过fio这个工具来测试，下面的语句是我用来测试磁盘随机读写的命令：

```bash
fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M 
```

其实，因为没能正确地设置innodb_io_capacity参数，而导致的性能问题也比比皆是。之前，就 曾有其他公司的开发负责人找我看一个库的性能问题，说MySQL的写入速度很慢，TPS很低， 但是数据库主机的IO压力并不大。经过一番排查，发现罪魁祸首就是这个参数的设置出了问题。



他的主机磁盘用的是SSD，但是innodb_io_capacity的值设置的是300。于是，InnoDB认为这个 系统的能力就这么差，所以刷脏页刷得特别慢，甚至比脏页生成的速度还慢，这样就造成了脏页 累积，影响了查询和更新性能。



虽然我们现在已经定义了“全力刷脏页”的行为，但平时总不能一直是全力刷吧？毕竟磁盘能力不 能只用来刷脏页，还需要服务用户请求。所以接下来，我们就一起看看InnoDB怎么控制引擎按 照“全力”的百分比来刷脏页。



根据我前面提到的知识点，试想一下，如果你来设计策略控制刷脏页的速度，会参考哪些因 素呢？



这个问题可以这么想，如果刷太慢，会出现什么情况？首先是内存脏页太多，其次是redo log写 满。



所以，**InnoDB的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是redo log写盘速度。**



InnoDB会根据这两个因素先单独算出两个数字。

参数`innodb_max_dirty_pages_pct`是脏页比例上限，默认值是75%。InnoDB会根据当前的脏页 比例（假设为M），算出一个范围在0到100之间的数字，计算这个数字的伪代码类似这样：

```
F1(M)
{
    if M>=innodb_max_dirty_pages_pct then
        return 100;
    return 100*M/innodb_max_dirty_pages_pct;
}

```

InnoDB每次写入的日志都有一个序号，当前写入的序号跟checkpoint对应的序号之间的差值， 我们假设为N。InnoDB会根据这个N算出一个范围在0到100之间的数字，这个计算公式可以记为 F2(N)。F2(N)算法比较复杂，你只要知道N越大，算出来的值越大就好了。

然后，根据上述算得的F1(M)和F2(N)两个值，取其中较大的值记为R，之后引擎就可以按 照innodb_io_capacity定义的能力乘以R%来控制刷脏页的速度。



上述的计算流程比较抽象，不容易理解，所以我画了一个简单的流程图。图中的F1、F2就是上 面我们通过脏页比例和redo log写入速度算出来的两个值。

<img src="MYSQL实战45讲.assets/image-20250211195002030.png" alt="image-20250211195002030" style="zoom:50%;" />



现在你知道了，InnoDB会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。所以，无论 是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用IO资源 并可能影响到了你的更新语句，都可能是造成你从业务端感知到MySQL“抖”了一下的原因。



要尽量避免这种情况，你就要合理地设置innodb_io_capacity的值，并且平时要多关注脏页比 例，不要让它经常接近75%

其中，脏页比例是通过Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total得到 的，具体的命令参考下面的代码：

```
mysql> select VARIABLE_VALUE into @a from information_schema.global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty';
select VARIABLE_VALUE into @b from information_schema.global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total';
select @a/@b;

```

==> mysql 8.0 好像存在status中了

```mysql
mysql> SHOW GLOBAL STATUS LIKE 'Innodb_buffer_pool_pages_dirty';
+--------------------------------+-------+
| Variable_name                  | Value |
+--------------------------------+-------+
| Innodb_buffer_pool_pages_dirty | 0     |
+--------------------------------+-------+
1 row in set (0.00 sec)

mysql> SHOW GLOBAL STATUS LIKE 'Innodb_buffer_pool_pages_total';
+--------------------------------+--------+
| Variable_name                  | Value  |
+--------------------------------+--------+
| Innodb_buffer_pool_pages_total | 196608 |
+--------------------------------+--------+
1 row in set (0.00 sec)
```



接下来，我们再看一个有趣的策略。



一旦一个查询请求需要在执行过程中先flush掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁 边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还 可以继续**蔓延**，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放 到一起刷。



在InnoDB中，`innodb_flush_neighbors` 参数就是用来控制这个行为的，值为1的时候会有上述 的“连坐”机制，值为0时表示不找邻居，自己刷自己的。

找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机IO。机械硬盘的随机IOPS 一般只有几百，相同的逻辑操作减少随机IO就意味着系统性能的大幅度提升。

而如果使用的是SSD这类IOPS比较高的设备的话，我就建议你把innodb_flush_neighbors的值 设置成0。因为这时候IOPS往往不是瓶颈，而“只刷自己”，就能更快地执行完必要的刷脏页操 作，减少SQL语句响应时间。

在MySQL 8.0中，innodb_flush_neighbors参数的默认值已经是0了。

```mysql
mysql> SHOW VARIABLES LIKE 'innodb_flush_neighbors';
+------------------------+-------+
| Variable_name          | Value |
+------------------------+-------+
| innodb_flush_neighbors | 0     |
+------------------------+-------+
1 row in set (0.00 sec)
```





## 小结 

今天这篇文章，我延续第2篇中介绍的WAL的概念，和你解释了这个机制后续需要的刷脏页操作和执行时机。

**利用WAL技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能**。 

但是，由此也带来了内存脏页的问题。脏页会被后台线程自动flush，也会由于数据页淘汰而触 发flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。 

在文章里，我也给你介绍了控制刷脏页的方法和对应的监控方式。





### 思考题

一个内存配置为128GB、innodb_io_capacity设置为20000的大规格实例，正常会建议你将redo log设置成4个1GB的文件。

但如果你在配置的时候不慎将redo log设置成了1个100M的文件，会发生什么情况呢？又为什么 会出现这样的情况呢？



回答：

如果一个高配的机器，redo log设置太小，会发生什么情况？

每次事务提交都要写redo log，如果设置太小，很快就会被写满，也就是下面这个图的状态，这 个“环”将很快被写满，write pos一直追着CP。

<img src="MYSQL实战45讲.assets/image-20250211200208457.png" alt="image-20250211200208457" style="zoom:50%;" />

这时候系统不得不停止所有更新，去推进checkpoint。 这时，**你看到的现象就是磁盘压力很小，但是数据库出现间歇性的性能下跌**。



评论区留言点赞板： 

@某、人 给了一个形象的描述，而且提到了，在这种情况下，连change buffer的优化也失效 了。因为checkpoint一直要往前推，这个操作就会触发merge操作，然后又进一步地触发刷脏 页操作；

有几个同学提到了内存淘汰脏页，对应的redo log的操作，这个我们会在后面的文章中展开， 大家可以先看一下 @melon 同学的描述了解一下； 

@算不出流源 提到了“动态平衡”，其实只要出现了这种“平衡”，意味着本应该后台的操作，就 已经影响了业务应用，属于有损失的平衡











































# 16 | “order by”是怎么工作的？

在你开发应用的时候，一定会经常碰到需要根据指定的字段排序来显示结果的需求。还是以我们前面举例用过的市民表为例，假设你要查询城市是“杭州”的所有人名字，并且按照姓名排序返回前1000个人的姓名、年龄。

假设这个表的部分定义是这样的：

```sql
CREATE TABLE `t` (
    `id` int(11) NOT NULL,
    `city` varchar(16) NOT NULL,
    `name` varchar(16) NOT NULL,
    `age` int(11) NOT NULL,
    `addr` varchar(128) DEFAULT NULL,
    PRIMARY KEY (`id`),
    KEY `city` (`city`)
) ENGINE=InnoDB;
```

这时，你的SQL语句可以这么写：

```SQL
select city,name,age from t where city='杭州' order by name limit 1000 ;
```

这个语句看上去逻辑很清晰，但是你了解它的执行流程吗？今天，我就和你聊聊这个语句是怎么执行的，以及有什么参数会影响执行的行为。

## 全字段排序

前面我们介绍过索引，所以你现在就很清楚了，为避免全表扫描，我们需要在city字段加上索引。

在city字段上创建索引之后，我们用explain命令来看看这个语句的执行情况。

```sql
mysql> explain select city,name,age from t where city='杭州' order by name limit 1000 ;
```

![image-20240426192519643](MYSQL实战45讲.assets/image-20240426192519643.png)

​                                                                                                  图1 使用explain命令查看语句的执行情况

这是一个 MySQL 的 `EXPLAIN` 查询结果，用于解释查询语句的执行计划和性能优化信息。下面是对每个列的解释：

- `id`: 查询中每个操作的标识符。在这个例子中，只有一个操作，因此为 1。
- `select_type`: 查询的类型。在这个例子中，是 `SIMPLE`，表示这是一个简单的 SELECT 查询。
- `table`: 查询涉及的表名。
- `partitions`: 表的分区信息。在这个例子中，为 `NULL`，表示没有使用分区。
- `type`: 访问类型，表示查询时使用的访问方法。在这个例子中，是 `ref`，表示使用索引进行查询。
- `possible_keys`: 可能使用的索引列表。
- `key`: 实际使用的索引。
- `key_len`: 使用的索引长度。
- `ref`: 表示索引的哪个列或常量与查询条件进行比较。
- `rows`: 估计的扫描行数。
- `filtered`: 表示在表中过滤的行的百分比。
- `Extra`: 额外的信息和优化提示。在这个例子中，表示使用了索引条件和文件排序。

Extra这个字段中的“Using filesort”表示的就是需要排序，**MySQL会给每个线程分配一块内存用于排序，称为sort_buffer。**

为了说明这个SQL查询语句的执行过程，我们先来看一下city这个索引的示意图。

<img src="MYSQL实战45讲.assets/image-20240426192103573.png" alt="image-20240426192103573" style="zoom:50%;" />

​                                                                                                      图2 city字段的索引示意图



从图中可以看到，满足city='杭州’条件的行，是从ID_X到ID_(X+N)的这些记录。

通常情况下，这个语句执行流程如下所示 ：（聚簇索引+非聚簇索引）

1. 初始化sort_buffer，确定放入name、city、age这三个字段；

2. 从索引city找到第一个满足city='杭州’条件的主键id，也就是图中的ID_X；

3. 到主键id索引取出整行，取name、city、age三个字段的值，存入sort_buffer中；

4. 从索引city取下一个记录的主键id；

5. 重复步骤3、4直到city的值不满足查询条件为止，对应的主键id也就是图中的ID_Y；

6. 对sort_buffer中的数据按照字段name做快速排序；

7. 按照排序结果取前1000行返回给客户端。

我们暂且把这个排序过程，称为**全字段排序**，执行流程的示意图如下所示，下一篇文章中我们还会用到这个排序。

<img src="MYSQL实战45讲.assets/image-20240426192338252.png" alt="image-20240426192338252" style="zoom:50%;" />

​                                                                                                                             图3 全字段排序

图中“按name排序”这个动作，**可能在内存中完成，也可能需要使用外部排序**，这取决于排序所需的**内存和参数sort_buffer_size**。

sort_buffer_size，就是MySQL为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序

你可以用下面介绍的方法，来确定一个排序语句是否使用了临时文件。

```sql
/* 打开optimizer_trace，只对本线程有效 */
SET optimizer_trace='enabled=on';
/* @a保存Innodb_rows_read的初始值 */
select VARIABLE_VALUE into @a from performance_schema.session_status where variable_name = 'Innodb_rows_read';
/* 执行语句 */
select city, name,age from t where city='杭州' order by name limit 1000;
/* 查看 OPTIMIZER_TRACE 输出 */
SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G
/* @b保存Innodb_rows_read的当前值 */
select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read';
/* 计算Innodb_rows_read差值 */
select @b-@a;
```

这个方法是通过查看 OPTIMIZER_TRACE 的结果来确认的，你可以从 **number_of_tmp_files**中看到是否使用了临时文件

![image-20240426192804478](MYSQL实战45讲.assets/image-20240426192804478.png)

​                                                                                    图4 全排序的OPTIMIZER_TRACE部分结果

number_of_tmp_files表示的是，排序过程中使用的临时文件数。你一定奇怪，为什么需要12个文件？内存放不下时，就需要使用外部排序，外部排序一般使用**归并排序算法**。

示例表中有4000条满足city='杭州’的记录，所以你可以看到 examined_rows=4000，表示参与排序的行数是4000行。

sort_mode 里面的packed_additional_fields的意思是，排序过程对字符串做了“紧凑”处理。即使name字段的定义是varchar(16)，在排序过程中还是要按照实际长度来分配空间的。

同时，最后一个查询语句select @b-@a 的返回结果是4000，表示整个执行过程只扫描了4000行。

这里需要注意的是，为了避免对结论造成干扰，我把internal_tmp_disk_storage_engine设置成MyISAM。否则，select @b-@a的结果会显示为4001。这是因为查询OPTIMIZER_TRACE这个表时，需要用到临时表，而**internal_tmp_disk_storage_engine**的默认值是InnoDB。如果使用的是InnoDB引擎的话，把数据从临时表取出来的时候，会让Innodb_rows_read的值加1。



## **rowId**排序排

在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在sort_buffer和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么sort_buffer里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。所以如果单行很大，这个方法效率不够好。

接下来，我来修改一个参数，让MySQL采用另外一种算法

```sql
SET max_length_for_sort_data = 16;
```

max_length_for_sort_data，是MySQL中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL就认为单行太大，要换一个算法。city、name、age 这三个字段的定义总长度是36，我把max_length_for_sort_data设置为16，我们再来看看计算过程有什么改变。

新的算法放入sort_buffer的字段，只有要排序的列（即name字段）和主键id

但这时，排序的结果就因为少了city和age字段的值，不能直接返回了，整个执行流程就变成如下所示的样子：

1. 初始化sort_buffer，确定放入两个字段，即name和id；

2. 从索引city找到第一个满足city='杭州’条件的主键id，也就是图中的ID_X；

3. 到主键id索引取出整行，取name、id这两个字段，存入sort_buffer中；

4. 从索引city取下一个记录的主键id；

5. 重复步骤3、4直到不满足city='杭州’条件为止，也就是图中的ID_Y；

6. 对sort_buffer中的数据按照字段name进行排序；

7. 遍历排序结果，取前1000行，并按照id的值回到原表中取出city、name和age三个字段返回给客户端。（**多了一次回表查询**）

这个执行流程的示意图如下，我把它称为rowid排序。

![image-20240426194011936](MYSQL实战45讲.assets/image-20240426194011936.png)

​                                                                                        图5 rowid排序

根据这个说明过程和图示，你可以想一下，这个时候执行select @b-@a，结果会是多少呢？

现在，我们就来看看结果有什么不同。

首先，图中的examined_rows的值还是4000，表示用于排序的数据是4000行。但是select @b-@a这个语句的值变成5000了。

因为这时候除了排序过程外，在排序完成后，还要根据id去原表取值。由于语句是limit 1000，因此会多读1000行。

<img src="MYSQL实战45讲.assets/image-20240426194738764.png" alt="image-20240426194738764" style="zoom:50%;" />

​                                                                                        图6 rowid排序的OPTIMIZER_TRACE部分输出

从OPTIMIZER_TRACE的结果中，你还能看到另外两个信息也变了。sort_mode变成了<sort_key, rowid>，表示参与排序的只有name和id这两个字段。

number_of_tmp_files变成10了，是因为这时候参与排序的行数虽然仍然是4000行，但是每一行都变小了，因此需要排序的总数据量就变小了，需要的临时文件也相应地变少了



## 全字段排序 VS rowid排序

对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。

这体现了**MySQL的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。**

看到这里，你就了解了，MySQL做排序是一个成本比较高的操作。那么你会问，是不是所有的order by都需要排序操作呢？如果不排序就能得到正确的结果，那对系统的消耗会小很多，语句的执行时间也会变得更短。

你可以设想下，如果能够保证从city这个索引上取出来的行，天然就是按照name递增排序的话，是不是就可以不用再排序了呢？确实是这样的。

所以，我们可以在这个市民表上创建一个city和name的联合索引，对应的SQL语句是：

```sql
alter table t add index city_user(city, name);
```

作为与city索引的对比，我们来看看这个索引的示意图。

<img src="MYSQL实战45讲.assets/image-20240426195330323.png" alt="image-20240426195330323" style="zoom:50%;" />

​                                                                                          图7 city和name联合索引示意图

在这个索引里面，我们依然可以用树搜索的方式定位到第一个满足city='杭州’的记录，并且额外确保了，接下来按顺序取“下一条记录”的遍历过程中，只要city的值是杭州，name的值就一定是有序的。

这样整个查询过程的流程就变成了：

1. 从索引(city,name)找到第一个满足city='杭州’条件的主键id；

2. 到主键id索引取出整行，取name、city、age三个字段的值，作为结果集的一部分直接返

回；

3. 从索引(city,name)取下一个记录主键id；
4.  重复步骤2、3，直到查到第1000条记录，或者是不满足city='杭州’条件时循环结束。

<img src="MYSQL实战45讲.assets/image-20240426195530556.png" alt="image-20240426195530556" style="zoom:50%;" />

​                                                                    图8 引入(city,name)联合索引后，查询语句的执行计划

可以看到，这个查询过程不需要临时表，也不需要排序。接下来，我们用explain的结果来印证一下。

![image-20240426195601594](MYSQL实战45讲.assets/image-20240426195601594.png)

​                                                         图9 引入(city,name)联合索引后，查询语句的执行计划

从图中可以看到，Extra字段中没有Using filesort了，也就是不需要排序了。而且由于(city,name)这个联合索引本身有序，所以这个查询也不用把4000行全都读一遍，只要找到满足条件的前1000条记录就可以退出了。也就是说，在我们这个例子里，只需要扫描1000次。

進一步簡化：覆盖索引。指索引上的信息足够满足查询请求，不需要再 覆回到主键索引上去取数据。

按照覆盖索引的概念，我们可以再优化一下这个查询语句的执行流程。

针对这个查询，我们可以创建一个city、name和age的联合索引，对应的SQL语句就是：

```sql
alter table t add index city_user_age(city, name, age);
```

这时，对于city字段的值相同的行来说，还是按照name字段的值递增排序的，此时的查询语句也

就不再需要排序了。这样整个查询语句的执行流程就变成了：

1. 从索引(city,name,age)找到第一个满足city='杭州’条件的记录，取出其中的city、name和age

这三个字段的值，作为结果集的一部分直接返回；

2. 从索引(city,name,age)取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接

返回；

3. 重复执行步骤2，直到查到第1000条记录，或者是不满足city='杭州’条件时循环结束。

<img src="MYSQL实战45讲.assets/image-20240426195944226.png" alt="image-20240426195944226" style="zoom:50%;" />

​                                                                           图10 引入(city,name,age)联合索引后，查询语句的执行流程

然后，我们再来看看explain的结果：

![image-20240426200021609](MYSQL实战45讲.assets/image-20240426200021609.png)

​                                                                                   图11 引入(city,name,age)联合索引后，查询语句的执行计划

可以看到，Extra字段里面多了“Using index”，表示的就是使用了覆盖索引，性能上会快很多。

当然，这里并不是说要为了每个查询能用上覆盖索引，就要把语句中涉及的字段都建上联合索引，毕竟索引还是有维护代价的。这是一个需要权衡的决定。



## 小结

今天这篇文章，我和你介绍了MySQL里面order by语句的几种算法流程。

在开发系统的时候，你总是不可避免地会使用到order by语句。你心里要清楚每个语句的排序逻辑是怎么实现的，还要能够分析出在最坏情况下，每个语句的执行对系统资源的消耗，这样才能做到下笔如有神，不犯低级错误。

## 思考题

假设你的表里面已经有了city_name(city, name)这个联合索引，然后你要查杭州和苏州两个城市中所有的市民的姓名，并且按名字排序，显示前100条记录。如果SQL查询语句是这么写的 ：

```
mysql> select * from t where city in ('杭州',"苏州") order by name limit 100;
```

那么，这个语句执行的时候会有排序过程吗，为什么？

如果业务端代码由你来开发，需要实现一个在数据库端不需要排序的方案，你会怎么实现呢？

进一步地，如果有分页需求，要显示第101页，也就是说语句最后要改成 “limit 10000,100”， 你的实现方法又会是什么呢？





# 22 | MySQL有哪些“饮鸩止渴”提高性能的方法？

不知道你在实际运维过程中有没有碰到这样的情景：业务高峰期，生产环境的MySQL压力太 大，没法正常响应，需要短期内、临时性地提升一些性能。 我以前做业务护航的时候，就偶尔会碰上这种场景。用户的开发负责人说，不管你用什么方案， 让业务先跑起来再说。 但，如果是无损方案的话，肯定不需要等到这个时候才上场。今天我们就来聊聊这些临时方案， 并着重说一说它们可能存在的风险。



## 短连接风暴

 正常的短连接模式就是连接到数据库后，执行很少的SQL语句就断开，下次需要的时候再重连。 如果使用的是短连接，在业务高峰期的时候，就可能出现连接数突然暴涨的情况。 



我在第1篇文章《基础架构：一条SQL查询语句是如何执行的？》中说过，

MySQL建立连接的过 程，成本是很高的。除了正常的网络连接三次握手外，还需要做登录权限判断和获得这个连接的 数据读写权限。 在数据库压力比较小的时候，这些额外的成本并不明显。 但是，短连接模型存在一个风险，就是一旦数据库处理得慢一些，连接数就会暴涨。

**max_connections参数**，用来控制一个MySQL实例同时存在的连接数的上限，超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。对于被拒绝连接的请求来 说，从业务角度看就是数据库不可用。



**在机器负载比较高的时候**，处理现有请求的时间变长，每个连接保持的时间也更长。这时，再有 新建连接的话，就可能会超过max_connections的限制。



碰到这种情况时，一个比较自然的想法，就是调高max_connections的值。但这样做是有风险 的。因为设计max_connections这个参数的目的是想保护MySQL，如果我们把它改得太大，让更 多的连接都可以进来，那么系统的负载可能会进一步加大，大量的资源耗费在权限验证等逻辑 上，结果可能是适得其反，已经连接的线程拿不到CPU资源去执行业务的SQL请求。



### 两种有损方法

 那么这种情况下，你还有没有别的建议呢？我这里还有两种方法，但要注意，这些方法都是**有损的**。



**第一种方法：先处理掉那些占着连接但是不工作的线程**。

 max_connections的计算，不是看谁在running，是只要连着就占用一个计数位置。对于那些不需 要保持的连接，我们可以通过kill connection主动踢掉。

这个行为跟事先设置wait_timeout的效果 是一样的。设置wait_timeout参数表示的是，一个线程空闲wait_timeout这么多秒之后，就会被 MySQL直接断开连接。



但是需要注意，在`show processlist`的结果里，踢掉显示为sleep的线程，可能是有损的。我们来 看下面这个例子。

![image-20241225203005539](MYSQL实战45讲.assets/image-20241225203005539.png)

在上面这个例子里，如果断开session A的连接，因为这时候session A还没有提交，所以MySQL 只能按照回滚事务来处理；而断开session B的连接，就没什么大影响。所以，如果按照优先级 来说，你应该优先断开像session B这样的事务外空闲的连接。(先断开读操作的sleep进程)



但是，怎么判断哪些是事务外空闲的呢？session C在T时刻之后的30秒执行showprocesslist， 看到的结果是这样的。

<img src="MYSQL实战45讲.assets/image-20241225203342065.png" alt="image-20241225203342065" style="zoom: 67%;" />



图中id=4和id=5的两个会话都是Sleep 状态。而要看事务具体状态的话，你可以查 information_schema库的innodb_trx表。

```sql
select * from information_schema.innodb_trx \G;
```

<img src="MYSQL实战45讲.assets/image-20241225203421367.png" alt="image-20241225203421367" style="zoom:50%;" />



这个结果里，trx_mysql_thread_id=4，表示id=4的线程还处在事务中。 因此，如果是连接数过多，你可以**优先断开事务外空闲太久的连接**；如果这样还不够，再考虑断 开事务内空闲太久的连接。



从服务端断开连接使用的是`KILL <connection_id>;` 的命令， 一个客户端处于sleep状态时，它的连接 被服务端主动断开后，这个客户端并不会马上知道。直到客户端在发起下一个请求的时候，才会 收到这样的报错“ERROR 2013 (HY000): Lost connection to MySQL server during query”。



从数据库端主动断开连接可能是有损的，尤其是有的应用端收到这个错误后，不重新连接，而是 直接用这个已经不能用的句柄重试查询。这会导致从应用端看上去，“MySQL一直没恢复”。 你可能觉得这是一个冷笑话，但实际上我碰到过不下10次。 所以，如果你是一个支持业务的DBA，不要假设所有的应用代码都会被正确地处理。即使只是 一个断开连接的操作，也要确保通知到业务开发团队。



**第二种方法：减少连接过程的消耗。** 



有的业务代码会在短时间内先大量申请数据库连接做备用，如果现在数据库确认是被连接行为打 挂了，那么一种可能的做法，是让数据库跳过权限验证阶段。

 跳过权限验证的方法是：重启数据库，并使用**–skip-grant-tables参数**启动。

这样，整个MySQL会 跳过所有的权限验证阶段，包括连接过程和语句执行过程在内。

 但是，这种方法特别符合我们标题里说的“饮鸩止渴”，风险极高，是我特别不建议使用的方案。

 尤其你的库外网可访问的话，就更不能这么做了。

 **在MySQL 8.0版本里，如果你启用–skip-grant-tables参数，MySQL会默认把 --skip-networking参 数打开，表示这时候数据库只能被本地的客户端连接**。

可见，MySQL官方对skip-grant-tables这 个参数的安全问题也很重视。 除了短连接数暴增可能会带来性能问题外，实际上，我们在线上碰到更多的是查询或者更新语句 导致的性能问题。其中，查询问题比较典型的有两类，一类是由新出现的慢查询导致的，一类是 由QPS（每秒查询数）突增导致的。而关于更新语句导致的性能问题，我会在下一篇文章和你展 开说明。



## 慢查询性能问题 



在MySQL中，会引发性能问题的慢查询，大体有以下三种可能： 1. 索引没有设计好； 2. SQL语句没写好； 3. MySQL选错了索引。

接下来，我们就具体分析一下这三种可能，以及对应的解决方案。



导致慢查询的第一种可能是，**索引没有设计好**。

这种场景一般就是通过紧急创建索引来解决。MySQL 5.6版本以后，创建索引都支持Online DDL 了，对于那种高峰期数据库已经被这个语句打挂了的情况，最高效的做法就是直接执行alter table 语句



比较理想的是能够在备库先执行。假设你现在的服务是一主一备，主库A、备库B，这个方案的 大致流程是这样的：

1. 在备库B上执行 `set sql_log_bin=off`，也就是不写binlog，然后执行alter table 语句加上索 引；
2. 执行主备切换； 
3. 这时候主库是B，备库是A。在A上执行 set sql_log_bin=off，然后执行alter table 语句加上 索引。



这是一个“古老”的DDL方案。平时在做变更的时候，你应该考虑类似gh-ost这样的方案，更加稳 妥。但是在需要紧急处理时，上面这个方案的效率是最高的。



导致慢查询的第二种可能是，**语句没写好**。 



比如，我们犯了在第18篇文章《为什么这些SQL语句逻辑相同，性能却差异巨大？》中提到的 那些错误，导致语句没有使用上索引。

这时，我们可以通过改写SQL语句来处理。

MySQL 5.7提供了**query_rewrite功能**，可以把输入的 一种语句改写成另外一种模式。 比如，语句被错误地写成了 `select *from t where id + 1 = 10000`，你可以通过下面的方式，增加 一个语句改写规则。

```sql
mysql> insert into query_rewrite.rewrite_rules(pattern, replacement, pattern_database) values ("select * from t where id + 1 = ?", "select * from t where id = ? - 1", "db1");

call query_rewrite.flush_rewrite_rules()
```

这里，call query_rewrite.flush_rewrite_rules()这个存储过程，是让插入的新规则生效，也就是我 们说的“查询重写”。你可以用图4中的方法来确认改写规则是否生效。



<img src="MYSQL实战45讲.assets/image-20241225204639614.png" alt="image-20241225204639614" style="zoom: 67%;" />



导致慢查询的第三种可能，就是碰上了我们在第10篇文章《MySQL为什么有时候会选错索 引？》中提到的情况，**MySQL选错了索引**。 

这时候，应急方案就是给这个语句加上force index。



这时候，应急方案就是给这个语句加上force index。 同样地，使用查询重写功能，给原来的语句加上force index，也可以解决这个问题。



上面我和你讨论的由慢查询导致性能问题的三种可能情况，实际上出现最多的是前两种，即：索 引没设计好和语句没写好。而这两种情况，恰恰是完全可以避免的。比如，通过下面这个过程， 我们就可以预先发现问题。

1. 上线前，在测试环境，把**慢查询日志（slowlog）**打开，并且把long_query_time设置成0， 确保每个语句都会被记录入慢查询日志；
2. 在测试表里插入模拟线上的数据，做一遍回归测试；

```yaml
#my.cnf
slow_query_log = 1
slow_query_log_file = /path/to/slow_query.log
long_query_time = 0 ## If a query takes longer than this many seconds, the server increments the Slow_queries status variable
```

3. 观察慢查询日志里每类语句的输出，特别留意Rows_examined字段是否与预期一致。（我 们在前面文章中已经多次用到过Rows_examined方法了，相信你已经动手尝试过了。如果 还有不明白的，欢迎给我留言，我们一起讨论）。



不要吝啬这段花在上线前的“额外”时间，因为这会帮你省下很多故障复盘的时间。 如果新增的SQL语句不多，手动跑一下就可以。而如果是新项目的话，或者是修改了原有项目的 表结构设计，**全量回归测试**都是必要的。这时候，你需要工具帮你检查所有的SQL语句的返回结 果。比如，你可以使用开源工具pt-query-digest(https://www.percona.com/doc/perconatoolkit/3.0/pt-query-digest.html)。



## QPS突增问题

有时候由于业务突然出现高峰，或者应用程序bug，导致某个语句的QPS突然暴涨，也可能导致 MySQL压力过大，影响服务。

我之前碰到过一类情况，是由一个新功能的bug导致的。当然，最理想的情况是让业务把这个功能下掉，服务自然就会恢复。

而下掉一个功能，如果从数据库端处理的话，对应于不同的背景，有不同的方法可用。我这里再 和你展开说明一下。

1. 一种是由全新业务的bug导致的。假设你的DB运维是比较规范的，也就是说白名单是一个个 加的。这种情况下，如果你能够确定业务方会下掉这个功能，只是时间上没那么快，那么就 可以从数据库端直接把白名单去掉。
2. 如果这个新功能使用的是单独的数据库用户，可以用管理员账号把这个用户删掉，然后断开 现有连接。这样，这个新功能的连接不成功，由它引发的QPS就会变成0。

3. 如果这个新增的功能跟主体功能是部署在一起的，那么我们只能通过处理语句来限制。这 时，我们可以使用上面提到的查询重写功能，**把压力最大的SQL语句直接重写成"select 1"返回**。 当然，这个操作的风险很高，需要你特别细致。它可能存在两个副作用：
   1. 如果别的功能里面也用到了这个SQL语句模板，会有误伤； 
   2. 很多业务并不是靠这一个语句就能完成逻辑的，所以如果单独把这一个语句以select 1的结 果返回的话，可能会导致后面的业务逻辑一起失败。



所以，方案3是用于止血的，跟前面提到的去掉权限验证一样，应该是你所有选项里优先级最低 的一个方案。 同时你会发现，其实方案1和2都要依赖于规范的运维体系：**虚拟化、白名单机制、业务账号分离**。由此可见，更多的准备，往往意味着更稳定的系统。



## 小结

今天这篇文章，我以业务高峰期的性能问题为背景，和你介绍了一些紧急处理的手段。 

这些处理手段中，既包括了粗暴地拒绝连接和断开连接，也有通过重写语句来绕过一些坑的方法；

既有临时的高危方案，也有未雨绸缪的、相对安全的预案。 

在实际开发中，我们也要尽量避免一些低效的方法，比如避免大量地使用短连接。

同时，如果你做业务开发的话，要知道，**连接异常断开是常有的事，你的代码里要有正确地重连并重试的机制**。 

DBA虽然可以通过语句重写来暂时处理问题，但是这本身是一个风险高的操作，做好SQL审计 可以减少需要这类操作的机会。 

其实，你可以看得出来，在这篇文章中我提到的解决方法主要集中在server层。

在下一篇文章 中，我会继续和你讨论一些跟InnoDB有关的处理方法。 

最后，又到了我们的思考题时间了。

今天，我留给你的课后问题是，你是否碰到过，在业务高峰期需要临时救火的场景？你又是怎么 处理的呢？



# 其他

### 精选评论







